{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Really Counts: A Cash Recognization System\n",
    "\n",
    "---\n",
    "\n",
    "**Group Members**:\n",
    "- Ek Chin Hui ()\n",
    "- Lee Hyung Woon (A0218475X)\n",
    "- Toh Hai Jie Joey (A0205141Y)\n",
    "\n",
    "---\n",
    "\n",
    "Our project, named **What Really Counts**, is a Cash Recognization System for the Visually Impaired in Singapore. In Singapore, the disabled community face many challenges in their daily lives, and this is especially so for those who are hampered by visual impairments. One such challenge they face is cash payment, as they need to identify the correct combinations of bills and coins. Hence, our aim was to contruct a system that can help them overcome these challenges by employing a deep learning-based Object Detection model using Convolutional Neural Networks (CNN) - in particular, the Faster R-CNN model. \n",
    "\n",
    "**What Really Counts** is an architecture that detects and analyzes given images of Singapore Currencies (bills and/or coins), and is primarily designed to assist the visually impaired in identifying the correct combinations of bills and coins. The model uses CNNs to perform image classification on the objects detected in a given input image, through which we can ascertain the exact number and type of bills / coins present in the image, allowing us to calculate and return the sum of the currency to the user.\n",
    "\n",
    "For this project, we will gather and pre-process our own dataset, and then move onto training and testing of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used\n",
    "\n",
    "The following are modules that we used for this project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection & Preprocessing\n",
    "\n",
    "For this project, we collected data by taking pictures...\n",
    "\n",
    "As an initial proof of concept, we decided to focus on the 7 most common classes of currencies in Singapore: $10, $5, $2, $1, 50c, 20c, 10c...\n",
    "\n",
    "After we gathered the data, we did data cleaning, augmentation, labelling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10c': 1, '20c': 2, '50c': 3, '$1': 4, '$2': 5, '$5': 6, '$10': 7, 'background': 0}\n",
      "{1: '10c', 2: '20c', 3: '50c', 4: '$1', 5: '$2', 6: '$5', 7: '$10', 0: 'background'}\n"
     ]
    }
   ],
   "source": [
    "# Set class labels\n",
    "\n",
    "coin_labels = ('10c', '20c', '50c', '$1', '$2', '$5', '$10')\n",
    "\n",
    "label_index_dict = {k:v+1 for v, k in enumerate(coin_labels)}\n",
    "label_index_dict['background'] = 0\n",
    "print(label_index_dict)\n",
    "\n",
    "index_label_dict = {v+1:k for v, k in enumerate(coin_labels)}\n",
    "index_label_dict[0] = 'background'\n",
    "print(index_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"\n",
    "    Function to convert XML data of a single image into an object.\n",
    "    The object contains Bbox parameters and the corresponding label for each Bbox.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the XML file into a tree structure\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Set initial lists\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    # difficulties = list()\n",
    "\n",
    "    # Loop over each Bbox found in the XML file\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        # Convert Bbox co-ordinates\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1 # TODO: Why minus 1?\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert Bbox Label\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_index_dict:\n",
    "            continue\n",
    "        labels.append(label_index_dict[label])\n",
    "\n",
    "        # Convert Bbox Difficulty\n",
    "        # difficult = int(object.find('difficult').text == '1')\n",
    "        # difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "def xml_to_json(files):\n",
    "    \"\"\"\n",
    "    Function to convert image and XML data into two separate JSON objects.\n",
    "    One object for images, and another object for Bbox co-ordinate values and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise lists\n",
    "    images_list = [] \n",
    "    objects_list = []\n",
    "    \n",
    "    # Set up two JSON files to be written\n",
    "    images_file = open(\"TRAIN_images.json\", 'w')\n",
    "    objects_file = open(\"TRAIN_objects.json\", 'w')\n",
    "    \n",
    "    # Iterate through each XML-Image pair\n",
    "    for file in files:\n",
    "    \n",
    "        # Add each image file path into the images list\n",
    "        file_path = os.path.splitext(file)[0]   \n",
    "        images_list.append(file_path + \".jpg\")\n",
    "        \n",
    "        # Add each XML object into the objects list\n",
    "        xml_dict = parse_annotation(file)\n",
    "        objects_list.append(xml_dict)\n",
    "    \n",
    "    # Write each list into the corresponding JSON files\n",
    "    json.dump(images_list, images_file)\n",
    "    json.dump(objects_list, objects_file)\n",
    "\n",
    "CH_FILES = glob(r'dataset/ch dataset/*.xml')\n",
    "# JY_FILES = glob(r'dataset/jy dataset/*.xml')\n",
    "# HW_FILES = glob(r'dataset/hw dataset/*.xml')\n",
    "\n",
    "xml_to_json(CH_FILES)\n",
    "# xml_to_json(JY_FILES)\n",
    "# xml_to_json(HW_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, boxes, dims = (320, 320), return_percent_coords = False):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Resize image. For the SSD300, resize to (300, 300).\n",
    "    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n",
    "    you may choose to retain them.\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resize image\n",
    "    new_image = FT.resize(image, dims)\n",
    "\n",
    "    # Resize Bboxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims  # percent coordinates\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, boxes, labels):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Apply the transformations above.\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
    "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
    "    \"\"\"\n",
    "\n",
    "    # Mean and Standard deviation used for the base VGG from torchvision\n",
    "    # See: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "\n",
    "    # Resize image - this also converts absolute boundary coordinates to their fractional form\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims = (320, 320))\n",
    "\n",
    "    # Convert PIL image to Torch tensor\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "\n",
    "    # Normalize by mean and standard deviation of ImageNet data that the base torchvision VGG was trained on\n",
    "    new_image = FT.normalize(new_image, mean = mean, std = std)\n",
    "\n",
    "    return new_image, new_boxes, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader.\n",
    "    This class is primarily used to create batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_folder, split):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param split: split, one of 'TRAIN' or 'TEST'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {'TRAIN', 'TEST'}\n",
    "\n",
    "        # Read from the JSON files\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        # Number of images must match the number of objects containing the Bboxes for each image\n",
    "        assert len(self.images) == len(self.objects)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # Read image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensor = transforms.Resize(size = (320, 320))(image_tensor)\n",
    "\n",
    "        # Read objects in this image (Bboxes, labels)\n",
    "        objects = self.objects[i]\n",
    "        box = torch.FloatTensor(objects['boxes']) # (n_objects, 4xy values and 1 )\n",
    "        label = torch.LongTensor(objects['labels']) # (n_objects)\n",
    "\n",
    "        # Apply transformations\n",
    "        image, box, label = transform(image, box, label)\n",
    "        box_and_label = torch.cat([box, label.unsqueeze(1)], 1)\n",
    "        return image_tensor, box_and_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "\n",
    "        images = torch.stack(images, dim = 0)\n",
    "        return images, boxes  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "color_maps = ['r', 'g', 'b', 'y', 'm', 'w', 'k', 'c']\n",
    "bs = 10\n",
    "dataset = PascalVOCDataset(\".\", \"TRAIN\")\n",
    "train_dataloader = DataLoader(dataset, batch_size = bs, shuffle = True, collate_fn = dataset.collate_fn)\n",
    "\n",
    "# TODO: Currently, the object itself is being resized. \n",
    "# Need to instead use the resize() method to resize the bounding boxes instead?\n",
    "# This should ideally be done in the Faster R-CNN network itself\n",
    "# We'll need to ROI-Pool both the Bounding box Coordinates and the ROI itself\n",
    "\n",
    "# Visualise data\n",
    "# for data in train_dataloader:\n",
    "#     count = 1\n",
    "#     # if count == 1:\n",
    "#     #     print(data[0])\n",
    "#     #     print(data[0].size())\n",
    "#     #     print(data[0].type())\n",
    "#     count = count + 1\n",
    "#     for batch in range(bs):\n",
    "#         img = data[0][batch]\n",
    "#         boxes = data[1][batch]\n",
    "# #         labels = data[2][batch].tolist()\n",
    "# #         named_labels = [index_label_dict[label] for label in labels]\n",
    "#         plt.imshow(transforms.ToPILImage()(img))\n",
    "#         ax = plt.gca()\n",
    "#         labelled = set()\n",
    "#         for i, box in enumerate(boxes):\n",
    "#             w,h = box[2] - box[0], box[3] - box[1]\n",
    "#             x,y = box[0].item(), box[1].item()\n",
    "#             x = [x, x + w, x + w, x, x]\n",
    "#             y = [y, y, y + h, y + h, y]\n",
    "#             label = int(box[4].item())\n",
    "#             if label not in labelled:\n",
    "#                 plt.plot(x,y, color = color_maps[label], label = index_label_dict[label])\n",
    "#                 labelled.add(label)\n",
    "#             else:\n",
    "#                 plt.plot(x,y, color = color_maps[label])\n",
    "#             plt.legend(loc = 'best')\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n",
    "BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n",
    "BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "def pascal2yolo(anchor):\n",
    "    \"\"\"\n",
    "    Transforms anchor coordinates of the form [xmin, ymin, xmax, ymax] to [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # TODO: +1? for width and height\n",
    "    width = anchor[2] - anchor[0] + 1\n",
    "    height = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + (width-1)/2 \n",
    "    y_ctr = anchor[1] + (height-1)/2\n",
    "    return width, height, x_ctr, y_ctr\n",
    "\n",
    "def yolo2pascal(width, height, x_ctr, y_ctr):\n",
    "    \"\"\"\n",
    "    Transforms anchor coordinates of the form [x_center, y_center, width, height] to [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    width = width[:, np.newaxis]\n",
    "    height = height[:, np.newaxis]\n",
    "    anchors = np.hstack((x_ctr - 0.5 * (width - 1), y_ctr - 0.5 * (height - 1),\n",
    "                         x_ctr + 0.5 * (width - 1), y_ctr + 0.5 * (height - 1)))\n",
    "    return anchors\n",
    "\n",
    "def generate_ratio_anchors(anchor, ratios=(0.5,1,2)):\n",
    "    \"\"\"\n",
    "    Generate anchors based on given width:height ratio\n",
    "    \"\"\"\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor)\n",
    "    size = w*h\n",
    "    size_ratios = size / ratios\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    hs = np.round(ws * ratios)\n",
    "    anchors = yolo2pascal(ws, hs, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_scale_anchors(anchor, scales=np.array((8,16,32))):\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor) \n",
    "    scaled_w = w * scales\n",
    "    scaled_h = h * scales\n",
    "    anchors = yolo2pascal(scaled_w, scaled_h, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_anchors(height, width, aspect_ratio=np.array((0.5,1,2)), stride_length=16, scales=np.array((8,16,32))):\n",
    "    # Generate anchors of differing scale and aspect ratios\n",
    "    # return anchors and anchor length\n",
    "    base_anchor = pascal2yolo([0,0,15,15]) # 16, 16, 7.5, 7.5\n",
    "    ratio_anchors = generate_ratio_anchors(base_anchor, ratios=aspect_ratio)\n",
    "    anchors = np.vstack([\n",
    "        generate_scale_anchors(ratio_anchors[i, :], scales)\n",
    "        for i in range(ratio_anchors.shape[0])\n",
    "    ])\n",
    "    A = anchors.shape[0]\n",
    "    shift_x = np.arange(0, width) * stride_length\n",
    "    shift_y = np.arange(0, height) * stride_length\n",
    "    # Shift each ratio and \n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n",
    "                        shift_y.ravel())).transpose()\n",
    "    K = shifts.shape[0]\n",
    "    # width changes faster, so here it is H, W, C\n",
    "    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n",
    "    length = np.int32(anchors.shape[0])\n",
    "\n",
    "    return anchors, length\n",
    "\n",
    "def bbox_overlaps(boxes, query_boxes):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: (N, 4) ndarray or tensor or variable\n",
    "    query_boxes: (K, 4) ndarray or tensor or variable\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, np.ndarray):\n",
    "        boxes = torch.from_numpy(boxes)\n",
    "        query_boxes = torch.from_numpy(query_boxes)\n",
    "        out_fn = lambda x: x.numpy()  # If input is ndarray, turn the overlaps back to ndarray when return\n",
    "    else:\n",
    "        out_fn = lambda x: x\n",
    "\n",
    "    box_areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    query_areas = (query_boxes[:, 2] - query_boxes[:, 0] + 1) * (query_boxes[:, 3] - query_boxes[:, 1] + 1)\n",
    "\n",
    "    iw = (torch.min(boxes[:, 2:3], query_boxes[:, 2:3].t()) - torch.max(boxes[:, 0:1], query_boxes[:, 0:1].t()) + 1).clamp(min=0)\n",
    "    ih = (torch.min(boxes[:, 3:4], query_boxes[:, 3:4].t()) - torch.max(boxes[:, 1:2], query_boxes[:, 1:2].t()) + 1).clamp(min=0)\n",
    "    ua = box_areas.view(-1, 1) + query_areas.view(1, -1) - iw * ih\n",
    "    overlaps = iw * ih / ua\n",
    "    return out_fn(overlaps)\n",
    "\n",
    "def bbox_overlaps_batch(anchors, gt_boxes):\n",
    "    \"\"\"\n",
    "    anchors: (N, 4) ndarray of float\n",
    "    gt_boxes: (b, K, 5) ndarray of float\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    batch_size = gt_boxes.size(0)\n",
    "\n",
    "\n",
    "    if anchors.dim() == 2:\n",
    "\n",
    "        N = anchors.size(0)\n",
    "        K = gt_boxes.size(1)\n",
    "\n",
    "        anchors = anchors.view(1, N, 4).expand(batch_size, N, 4).contiguous()\n",
    "        gt_boxes = gt_boxes[:,:,:4].contiguous()\n",
    "\n",
    "\n",
    "        gt_boxes_x = (gt_boxes[:,:,2] - gt_boxes[:,:,0] + 1)\n",
    "        gt_boxes_y = (gt_boxes[:,:,3] - gt_boxes[:,:,1] + 1)\n",
    "        gt_boxes_area = (gt_boxes_x * gt_boxes_y).view(batch_size, 1, K)\n",
    "\n",
    "        anchors_boxes_x = (anchors[:,:,2] - anchors[:,:,0] + 1)\n",
    "        anchors_boxes_y = (anchors[:,:,3] - anchors[:,:,1] + 1)\n",
    "        anchors_area = (anchors_boxes_x * anchors_boxes_y).view(batch_size, N, 1)\n",
    "\n",
    "        gt_area_zero = (gt_boxes_x == 1) & (gt_boxes_y == 1)\n",
    "        anchors_area_zero = (anchors_boxes_x == 1) & (anchors_boxes_y == 1)\n",
    "\n",
    "        boxes = anchors.view(batch_size, N, 1, 4).expand(batch_size, N, K, 4)\n",
    "        query_boxes = gt_boxes.view(batch_size, 1, K, 4).expand(batch_size, N, K, 4)\n",
    "\n",
    "        iw = (torch.min(boxes[:,:,:,2], query_boxes[:,:,:,2]) -\n",
    "            torch.max(boxes[:,:,:,0], query_boxes[:,:,:,0]) + 1)\n",
    "        iw[iw < 0] = 0\n",
    "\n",
    "        ih = (torch.min(boxes[:,:,:,3], query_boxes[:,:,:,3]) -\n",
    "            torch.max(boxes[:,:,:,1], query_boxes[:,:,:,1]) + 1)\n",
    "        ih[ih < 0] = 0\n",
    "        ua = anchors_area + gt_boxes_area - (iw * ih)\n",
    "        overlaps = iw * ih / ua\n",
    "\n",
    "        # mask the overlap here.\n",
    "        overlaps.masked_fill_(gt_area_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)\n",
    "        overlaps.masked_fill_(anchors_area_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)\n",
    "\n",
    "    elif anchors.dim() == 3:\n",
    "        N = anchors.size(1)\n",
    "        K = gt_boxes.size(1)\n",
    "\n",
    "        if anchors.size(2) == 4:\n",
    "            anchors = anchors[:,:,:4].contiguous()\n",
    "        else:\n",
    "            anchors = anchors[:,:,1:5].contiguous()\n",
    "\n",
    "        gt_boxes = gt_boxes[:,:,:4].contiguous()\n",
    "\n",
    "        gt_boxes_x = (gt_boxes[:,:,2] - gt_boxes[:,:,0] + 1)\n",
    "        gt_boxes_y = (gt_boxes[:,:,3] - gt_boxes[:,:,1] + 1)\n",
    "        gt_boxes_area = (gt_boxes_x * gt_boxes_y).view(batch_size, 1, K)\n",
    "\n",
    "        anchors_boxes_x = (anchors[:,:,2] - anchors[:,:,0] + 1)\n",
    "        anchors_boxes_y = (anchors[:,:,3] - anchors[:,:,1] + 1)\n",
    "        anchors_area = (anchors_boxes_x * anchors_boxes_y).view(batch_size, N, 1)\n",
    "\n",
    "        gt_area_zero = (gt_boxes_x == 1) & (gt_boxes_y == 1)\n",
    "        anchors_area_zero = (anchors_boxes_x == 1) & (anchors_boxes_y == 1)\n",
    "\n",
    "        boxes = anchors.view(batch_size, N, 1, 4).expand(batch_size, N, K, 4)\n",
    "        query_boxes = gt_boxes.view(batch_size, 1, K, 4).expand(batch_size, N, K, 4)\n",
    "\n",
    "        iw = (torch.min(boxes[:,:,:,2], query_boxes[:,:,:,2]) -\n",
    "            torch.max(boxes[:,:,:,0], query_boxes[:,:,:,0]) + 1)\n",
    "        iw[iw < 0] = 0\n",
    "\n",
    "        ih = (torch.min(boxes[:,:,:,3], query_boxes[:,:,:,3]) -\n",
    "            torch.max(boxes[:,:,:,1], query_boxes[:,:,:,1]) + 1)\n",
    "        ih[ih < 0] = 0\n",
    "        ua = anchors_area + gt_boxes_area - (iw * ih)\n",
    "        \n",
    "        # Intersection (iw * ih) divided by Union (ua)\n",
    "        overlaps = iw * ih / ua\n",
    "\n",
    "        # mask the overlap here.\n",
    "        overlaps.masked_fill_(gt_area_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)\n",
    "        overlaps.masked_fill_(anchors_area_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)\n",
    "    else:\n",
    "        raise ValueError('anchors input dimension is not correct.')\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "def bbox_transform(ex_rois, gt_rois):\n",
    "    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n",
    "    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n",
    "\n",
    "    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n",
    "    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "    targets_dw = torch.log(gt_widths / ex_widths)\n",
    "    targets_dh = torch.log(gt_heights / ex_heights)\n",
    "\n",
    "    targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), 1)\n",
    "    return targets\n",
    "\n",
    "def bbox_transform_batch(ex_rois, gt_rois):\n",
    "\n",
    "    if ex_rois.dim() == 2:\n",
    "        ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "        ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "        ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n",
    "        ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n",
    "\n",
    "        gt_widths = gt_rois[:, :, 2] - gt_rois[:, :, 0] + 1.0\n",
    "        gt_heights = gt_rois[:, :, 3] - gt_rois[:, :, 1] + 1.0\n",
    "        gt_ctr_x = gt_rois[:, :, 0] + 0.5 * gt_widths\n",
    "        gt_ctr_y = gt_rois[:, :, 1] + 0.5 * gt_heights\n",
    "\n",
    "        targets_dx = (gt_ctr_x - ex_ctr_x.view(1,-1).expand_as(gt_ctr_x)) / ex_widths\n",
    "        targets_dy = (gt_ctr_y - ex_ctr_y.view(1,-1).expand_as(gt_ctr_y)) / ex_heights\n",
    "        targets_dw = torch.log(gt_widths / ex_widths.view(1,-1).expand_as(gt_widths))\n",
    "        targets_dh = torch.log(gt_heights / ex_heights.view(1,-1).expand_as(gt_heights))\n",
    "\n",
    "    elif ex_rois.dim() == 3:\n",
    "        ex_widths = ex_rois[:, :, 2] - ex_rois[:, :, 0] + 1.0\n",
    "        ex_heights = ex_rois[:,:, 3] - ex_rois[:,:, 1] + 1.0\n",
    "        ex_ctr_x = ex_rois[:, :, 0] + 0.5 * ex_widths\n",
    "        ex_ctr_y = ex_rois[:, :, 1] + 0.5 * ex_heights\n",
    "\n",
    "        gt_widths = gt_rois[:, :, 2] - gt_rois[:, :, 0] + 1.0\n",
    "        gt_heights = gt_rois[:, :, 3] - gt_rois[:, :, 1] + 1.0\n",
    "        gt_ctr_x = gt_rois[:, :, 0] + 0.5 * gt_widths\n",
    "        gt_ctr_y = gt_rois[:, :, 1] + 0.5 * gt_heights\n",
    "\n",
    "        targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "        targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "        targets_dw = torch.log(gt_widths / ex_widths)\n",
    "        targets_dh = torch.log(gt_heights / ex_heights)\n",
    "    else:\n",
    "        raise ValueError('ex_roi input dimension is not correct.')\n",
    "\n",
    "    targets = torch.stack(\n",
    "        (targets_dx, targets_dy, targets_dw, targets_dh),2)\n",
    "\n",
    "    return targets\n",
    "\n",
    "def compute_targets_atl(ex_rois, gt_rois):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "    \n",
    "    gt_rois_np = gt_rois.numpy()\n",
    "    targets = bbox_transform(ex_rois, torch.from_numpy(gt_rois_np[:, :4])).numpy()\n",
    "    return targets\n",
    "\n",
    "def compute_targets_ptl(ex_rois, gt_rois, labels):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    targets = bbox_transform(ex_rois, gt_rois)\n",
    "    # Normalize targets by a precomputed mean and stdev (optional)\n",
    "    # targets = ((targets - targets.new(BBOX_NORMALIZE_MEANS)) / targets.new(BBOX_NORMALIZE_STDS))\n",
    "    return torch.cat([labels.unsqueeze(1), targets], 1)\n",
    "\n",
    "def _get_bbox_regression_labels(bbox_target_data, num_classes):\n",
    "    \"\"\"\n",
    "    Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "    compact form N x (class, tx, ty, tw, th)\n",
    "    This function expands those targets into the 4-of-4*K representation used\n",
    "    by the network (i.e. only one class has non-zero targets).\n",
    "    Returns:\n",
    "        bbox_target (ndarray): N x 4K blob of regression targets\n",
    "        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n",
    "    \"\"\"\n",
    "    # Inputs are tensor\n",
    "\n",
    "    clss = bbox_target_data[:, 0]\n",
    "    bbox_targets = clss.new_zeros(clss.numel(), 4 * num_classes)\n",
    "    bbox_inside_weights = clss.new_zeros(bbox_targets.shape)\n",
    "    inds = (clss > 0).nonzero().view(-1)\n",
    "    if inds.numel() > 0:\n",
    "        clss = clss[inds].contiguous().view(-1, 1)\n",
    "        dim1_inds = inds.unsqueeze(1).expand(inds.size(0), 4)\n",
    "        dim2_inds = torch.cat([4 * clss, 4 * clss + 1, 4 * clss + 2, 4 * clss + 3], 1).long()\n",
    "        bbox_targets[dim1_inds, dim2_inds] = bbox_target_data[inds][:, 1:]\n",
    "        bbox_inside_weights[dim1_inds, dim2_inds] = bbox_targets.new(BBOX_INSIDE_WEIGHTS).view(-1, 4).expand_as(dim1_inds)\n",
    "\n",
    "    return bbox_targets, bbox_inside_weights\n",
    "\n",
    "def bbox_transform_inv(boxes, deltas):\n",
    "    # Input should be both tensor or both Variable and on the same device\n",
    "    if len(boxes) == 0:\n",
    "        return deltas.detach() * 0\n",
    "    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "\n",
    "    dx = deltas[:, 0::4]\n",
    "    dy = deltas[:, 1::4]\n",
    "    dw = deltas[:, 2::4]\n",
    "    dh = deltas[:, 3::4]\n",
    "\n",
    "    pred_ctr_x = dx * widths.unsqueeze(1) + ctr_x.unsqueeze(1)\n",
    "    pred_ctr_y = dy * heights.unsqueeze(1) + ctr_y.unsqueeze(1)\n",
    "    pred_w = torch.exp(dw) * widths.unsqueeze(1)\n",
    "    pred_h = torch.exp(dh) * heights.unsqueeze(1)\n",
    "\n",
    "    pred_boxes = torch.cat(\\\n",
    "      [_.unsqueeze(2) for _ in [pred_ctr_x - 0.5 * pred_w,\\\n",
    "                                pred_ctr_y - 0.5 * pred_h,\\\n",
    "                                pred_ctr_x + 0.5 * pred_w,\\\n",
    "                                pred_ctr_y + 0.5 * pred_h]], 2).view(len(boxes), -1)\n",
    "\n",
    "    return pred_boxes\n",
    "\n",
    "def bbox_transform_inv_batch(boxes, deltas, batch_size):\n",
    "    print('boxes', boxes.size())\n",
    "    widths = boxes[:, :, 2] - boxes[:, :, 0] + 1.0\n",
    "    heights = boxes[:, :, 3] - boxes[:, :, 1] + 1.0\n",
    "    ctr_x = boxes[:, :, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, :, 1] + 0.5 * heights\n",
    "\n",
    "    dx = deltas[:, :, 0::4]\n",
    "    dy = deltas[:, :, 1::4]\n",
    "    dw = deltas[:, :, 2::4]\n",
    "    dh = deltas[:, :, 3::4]\n",
    "\n",
    "    pred_ctr_x = dx * widths.unsqueeze(2) + ctr_x.unsqueeze(2)\n",
    "    pred_ctr_y = dy * heights.unsqueeze(2) + ctr_y.unsqueeze(2)\n",
    "    pred_w = torch.exp(dw) * widths.unsqueeze(2)\n",
    "    pred_h = torch.exp(dh) * heights.unsqueeze(2)\n",
    "\n",
    "    pred_boxes = deltas.clone()\n",
    "    # x1\n",
    "    pred_boxes[:, :, 0::4] = pred_ctr_x - 0.5 * pred_w\n",
    "    # y1\n",
    "    pred_boxes[:, :, 1::4] = pred_ctr_y - 0.5 * pred_h\n",
    "    # x2\n",
    "    pred_boxes[:, :, 2::4] = pred_ctr_x + 0.5 * pred_w\n",
    "    # y2\n",
    "    pred_boxes[:, :, 3::4] = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "    return pred_boxes\n",
    "\n",
    "def clip_boxes(boxes, im_shape):\n",
    "    \"\"\"\n",
    "  Clip boxes to image boundaries.\n",
    "  boxes must be tensor or Variable, im_shape can be anything but Variable\n",
    "  \"\"\"\n",
    "\n",
    "    if not hasattr(boxes, 'data'):\n",
    "        boxes_ = boxes.numpy()\n",
    "\n",
    "    boxes = boxes.view(boxes.size(0), -1, 4)\n",
    "    boxes = torch.stack(\\\n",
    "      [boxes[:,:,0].clamp(0, im_shape[1] - 1),\n",
    "       boxes[:,:,1].clamp(0, im_shape[0] - 1),\n",
    "       boxes[:,:,2].clamp(0, im_shape[1] - 1),\n",
    "       boxes[:,:,3].clamp(0, im_shape[0] - 1)], 2).view(boxes.size(0), -1)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def clip_boxes_batch(boxes, im_shape, batch_size):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    \"\"\"\n",
    "    num_rois = boxes.size(1)\n",
    "\n",
    "    boxes[boxes < 0] = 0\n",
    "    # batch_x = (im_shape[:,0]-1).view(batch_size, 1).expand(batch_size, num_rois)\n",
    "    # batch_y = (im_shape[:,1]-1).view(batch_size, 1).expand(batch_size, num_rois)\n",
    "\n",
    "    batch_x = im_shape[:, 1] - 1\n",
    "    batch_y = im_shape[:, 0] - 1\n",
    "\n",
    "    boxes[:,:,0][boxes[:,:,0] > batch_x] = batch_x\n",
    "    boxes[:,:,1][boxes[:,:,1] > batch_y] = batch_y\n",
    "    boxes[:,:,2][boxes[:,:,2] > batch_x] = batch_x\n",
    "    boxes[:,:,3][boxes[:,:,3] > batch_y] = batch_y\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def fix_sample_regions(fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image):\n",
    "    # Small modification to the original version where we ensure a fixed number of regions are sampled\n",
    "    if fg_inds.numel() == 0 and bg_inds.numel() == 0:\n",
    "        to_replace = all_rois.size(0) < rois_per_image\n",
    "        bg_inds = torch.from_numpy(npr.choice(np.arange(0, all_rois.size(0)), size=int(rois_per_image), replace=to_replace)).long()\n",
    "        fg_rois_per_image = 0\n",
    "    elif fg_inds.numel() > 0 and bg_inds.numel() > 0:\n",
    "        fg_rois_per_image = min(fg_rois_per_image, fg_inds.numel())\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(fg_rois_per_image),\n",
    "                replace=False)).long().to(gt_boxes.device)]\n",
    "        bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "        to_replace = bg_inds.numel() < bg_rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(bg_rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "    elif fg_inds.numel() > 0:\n",
    "        to_replace = fg_inds.numel() < rois_per_image\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = rois_per_image\n",
    "    elif bg_inds.numel() > 0:\n",
    "        to_replace = bg_inds.numel() < rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = 0\n",
    "\n",
    "    return fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image\n",
    "\n",
    "def unmap(data, count, inds, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of\n",
    "  size count) \"\"\"\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count, ), dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds] = data\n",
    "    else:\n",
    "        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms, RoIAlign\n",
    "\n",
    "hidden_dim = 64\n",
    "nb_objects = 3 # TODO: How about cases where images have different number of objects?\n",
    "num_of_class = 7\n",
    "\n",
    "im_size = 320\n",
    "ob_size = 20 # TODO: How about cases where objects have different sizes? Currently, H = W = ob_size\n",
    "bs = 10\n",
    "offset = (ob_size-1) // 2\n",
    "\n",
    "# Thresholds\n",
    "FG_THRESH = 0.5 # Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n",
    "# Overlap threshold for a ROI to be considered background (class = 0 if overlap in [LO, HI))\n",
    "BG_THRESH_HI = 0.5\n",
    "BG_THRESH_LO = 0.1\n",
    "\n",
    "PRE_NMS_TOPN = 12000\n",
    "POST_NMS_TOPN = 2000\n",
    "NMS_THRESH = 0.7\n",
    "\n",
    "POSITIVE_OVERLAP = 0.7\n",
    "NEGATIVE_OVERLAP = 0.3\n",
    "CLOBBER_POSITIVES = False\n",
    "RPN_BS = 1\n",
    "FG_FRACTION = 0.5\n",
    "RPN_POSITIVE_WEIGHT = -1.0\n",
    "POOLING_SIZE = 7\n",
    "\n",
    "class faster_R_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The main Faster R-CNN network used for this project.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(faster_R_CNN, self).__init__()\n",
    "        self.feat_stride = [16,]\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        self._layers = {}\n",
    "        self._act_summaries = {}\n",
    "        self._score_summaries = {}\n",
    "        self._event_summaries = {}\n",
    "        self._image_gt_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "        self._fc_channels = 25\n",
    "        self._net_conv_channels = 1024\n",
    "\n",
    "        # This results in num_anchors = 9\n",
    "        anchor_scales = (8, 16, 32)\n",
    "        anchor_ratios = (0.5, 1, 2)\n",
    "        self.n_anchors = len(anchor_scales) * len(anchor_ratios)  \n",
    "        \n",
    "        # HeadNet: Generating a series of Feature maps from the input image\n",
    "        \n",
    "        # Current Size: 3 x h x w\n",
    "        self.head_conv1 = nn.Conv2d(3,  hidden_dim,  kernel_size=4, stride=2, padding=1)\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_batch_norm1 = nn.BatchNorm2d(hidden_dim)\n",
    "        # Current Size: 64 x h/2 x w/2 \n",
    "        self.head_relu1 = nn.ReLU()\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_pool1 = nn.MaxPool2d([3,3], 1)\n",
    "        # Current Size: 64 x h/4 x w/4\n",
    "        self.head_layer1 = nn.Conv2d(hidden_dim,  hidden_dim*4,  kernel_size=3, padding=1)\n",
    "        self.head_relu2 = nn.ReLU()\n",
    "        # Current Size: 256 x h/4 x w/4\n",
    "        self.head_layer2 = nn.Conv2d(hidden_dim*4,  hidden_dim*8,  kernel_size=3, padding=1)\n",
    "        self.head_pool2 = nn.MaxPool2d([3,3], 1)\n",
    "        self.head_relu3 = nn.ReLU()\n",
    "        # Current Size: 512 x h/8 x w/8\n",
    "        self.head_layer3 = nn.Conv2d(hidden_dim*8,  hidden_dim*16,  kernel_size=3, padding=1)\n",
    "        self.head_pool3 = nn.MaxPool2d([3,3], 1)\n",
    "        self.head_relu4 = nn.ReLU()\n",
    "        # Current Size: 1024 x h/16 x w/16\n",
    "        \n",
    "        # Region Proposal Network\n",
    "        self.rpn_net = nn.Conv2d(self._net_conv_channels, 512 , kernel_size=3, padding=1)\n",
    "        self.rpn_cls_score_net = nn.Conv2d(512, self.n_anchors*2, [1,1])\n",
    "        self.rpn_bbox_pred_net = nn.Conv2d(512, self.n_anchors*4, [1,1])\n",
    "\n",
    "        # Classification Network\n",
    "        # [256, 256, 5, 5]\n",
    "        self.cls_score_net = nn.Linear(self._fc_channels, num_of_class)     \n",
    "        self.bbox_pred_net = nn.Linear(self._fc_channels, num_of_class*4)\n",
    "\n",
    "    def head_net(self):\n",
    "        return nn.Sequential(\n",
    "            self.head_conv1,\n",
    "            self.head_batch_norm1,\n",
    "            self.head_relu1,\n",
    "            self.head_pool1,\n",
    "            self.head_layer1,\n",
    "            self.head_relu2,\n",
    "            self.head_layer2,\n",
    "            self.head_pool2,\n",
    "            self.head_relu3,\n",
    "            self.head_layer3,\n",
    "            self.head_pool3,\n",
    "            self.head_relu4\n",
    "        )\n",
    "\n",
    "    def fc7(self):\n",
    "        return nn.Sequential(\n",
    "            # Current Size: n x 1024 x 7 x 7\n",
    "            # nn.Conv2d(self._predictions[\"rois\"].size(0), self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1024, self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.AvgPool2d([3,3], 1)\n",
    "            # Current Size: n x 4096 x 3 x 3\n",
    "        ) # 256\n",
    "        \n",
    "\n",
    "    def proposal_layer(self, cls_prob, bbox_pred, anchors, n_anchors):\n",
    "        '''\n",
    "        Prunes no. of boxes using NMS based on fg scores and transforms bbox using regression coeff\n",
    "        bbox_pred: bs * h * w * (num_anchors*4)  \n",
    "        '''\n",
    "        \n",
    "        # Get the scores and bounding boxes\n",
    "        scores = cls_prob[:, :, :, n_anchors:]\n",
    "        # rpn_bbox_pred = bbox_pred.view((-1, 4))\n",
    "        rpn_bbox_pred = bbox_pred.view(bs, -1, 4) # for batch\n",
    "        # scores = scores.contiguous().view(-1, 1)\n",
    "        scores = scores.reshape(bs, -1)  # for batch\n",
    "        # proposals = bbox_transform_inv(anchors, rpn_bbox_pred) # shift boxes based on prediction\n",
    "        proposals = bbox_transform_inv_batch(anchors, rpn_bbox_pred, bs) # for batch\n",
    "        # proposals = clip_boxes(proposals, self._im_info[:2])\n",
    "        proposals = clip_boxes_batch(proposals, self._im_info[:2], bs) # for batch\n",
    "\n",
    "        # NMS Selection, should include in final\n",
    "        \n",
    "#         # Pick the top region proposals\n",
    "#         scores, order = scores.view(-1).sort(descending=True)\n",
    "#         if PRE_NMS_TOPN > 0:\n",
    "#             order = order[:PRE_NMS_TOPN]\n",
    "#             scores = scores[:PRE_NMS_TOPN].view(-1, 1)\n",
    "#         proposals = proposals[order.data, :]\n",
    "\n",
    "#         # Non-maximal suppression\n",
    "#         keep = nms(proposals, scores.squeeze(1), NMS_THRESH)\n",
    "\n",
    "#         # Pick the top region proposals after NMS\n",
    "#         if POST_NMS_TOPN > 0:\n",
    "#             keep = keep[:POST_NMS_TOPN]\n",
    "#         proposals = proposals[keep, :]\n",
    "#         scores = scores[keep, ]\n",
    "\n",
    "        # Only support single image as input\n",
    "        batch_inds = proposals.new_zeros(proposals.size(0), 1)\n",
    "        blob = torch.cat((batch_inds, proposals), 1)\n",
    " \n",
    "        # For batch (NMS)\n",
    "        # scores_keep = scores\n",
    "        # proposals_keep = proposals\n",
    "        # _, order = torch.sort(scores_keep, 1, True)\n",
    "\n",
    "        # output = scores.new(bs, post_nms_topN, 5).zero_()\n",
    "        # for i in range(bs):\n",
    "        #     # # 3. remove predicted boxes with either height or width < threshold\n",
    "        #     # # (NOTE: convert min_size to input image scale stored in im_info[2])\n",
    "        #     proposals_single = proposals_keep[i]\n",
    "        #     scores_single = scores_keep[i]\n",
    "\n",
    "        #     # # 4. sort all (proposal, score) pairs by score from highest to lowest\n",
    "        #     # # 5. take top pre_nms_topN (e.g. 6000)\n",
    "        #     order_single = order[i]\n",
    "\n",
    "        #     if pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel():\n",
    "        #         order_single = order_single[:pre_nms_topN]\n",
    "\n",
    "        #     proposals_single = proposals_single[order_single, :]\n",
    "        #     scores_single = scores_single[order_single].view(-1,1)\n",
    "\n",
    "        #     # 6. apply nms (e.g. threshold = 0.7)\n",
    "        #     # 7. take after_nms_topN (e.g. 300)\n",
    "        #     # 8. return the top proposals (-> RoIs top)\n",
    "\n",
    "        #     keep_idx_i = nms(torch.cat((proposals_single, scores_single), 1), nms_thresh, force_cpu=not cfg.USE_GPU_NMS)\n",
    "        #     keep_idx_i = keep_idx_i.long().view(-1)\n",
    "\n",
    "        #     if post_nms_topN > 0:\n",
    "        #         keep_idx_i = keep_idx_i[:post_nms_topN]\n",
    "        #     proposals_single = proposals_single[keep_idx_i, :]\n",
    "        #     scores_single = scores_single[keep_idx_i, :]\n",
    "\n",
    "        #     # padding 0 at the end.\n",
    "        #     num_proposal = proposals_single.size(0)\n",
    "        #     output[i,:,0] = i\n",
    "        #     output[i,:num_proposal,1:] = proposals_single\n",
    "        \n",
    "        return blob, scores\n",
    "\n",
    "    # def anchor_target_layer(self, rpn_cls_score, gt_boxes, im_info, all_anchors, num_anchors):    \n",
    "    def anchor_target_layer(self, rpn_cls_score, gt_boxes, all_anchors, im_info=[320, 320, 1]):\n",
    "\n",
    "        '''\n",
    "\n",
    "        ### Parameters ###\n",
    "        rpn_cls_score: Class scores generated by the Region Proposal Network\n",
    "        gt_boxes: Ground Truth boxes\n",
    "        all_anchors: Anchor boxes generated by the anchor generation layer\n",
    "        \n",
    "        ### Fixed Parameters ###\n",
    "        im_info: Image dimensions\n",
    "        num_anchors: Number of different Anchor boxes used. By default, it is set to 9 here.\n",
    "\n",
    "        ### Additional information ###\n",
    "        POSITIVE_OVERLAP:       Threshold used to select if an anchor box is a good foreground box (Default: 0.7)\n",
    "\n",
    "        NEGATIVE_OVERLAP:       If the max overlap of a anchor from a ground truth box is lower than this thershold, it is marked as background. \n",
    "                                Boxes whose overlap is > than NEGATIVE_OVERLAP but < POSITIVE_OVERLAP are marked \n",
    "                                “don’t care”. (Default: 0.3)\n",
    "        \n",
    "        CLOBBER_POSITIVES:      If a particular anchor is satisfied by both the positive and the negative conditions,\n",
    "                                and if this value is set to False, then set the anchor to a negative example.\n",
    "                                Else, set the anchor to a positive example.\n",
    "        \n",
    "        RPN_BS:                 Total number of background and foreground anchors. (Default: 256)\n",
    "        \n",
    "        FG_FRACTION:            Fraction of the batch size that is foreground anchors (Default: 0.5). \n",
    "                                If the number of foreground anchors found is larger than RPN_BS * FG_FRACTION, \n",
    "                                the excess (indices are selected randomly) is marked “don’t care”.\n",
    "                            \n",
    "        RPN_POSITIVE_WEIGHT:    Using this value:\n",
    "                                Positive RPN examples are given a weight of RPN_POSITIVE_WEIGHT * 1 / num_of_positives\n",
    "                                Negative RPN examples are given a weight of (1 - RPN_POSITIVE_WEIGHT)\n",
    "                                Set to -1 by default, which will ensure uniform example weighting.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Map of shape (..., H, W)\n",
    "        height, width = rpn_cls_score.shape[1:3]\n",
    "\n",
    "        # Only keep anchors that are completely inside the image\n",
    "        inds_inside = np.where(\n",
    "            (all_anchors[:, 0] >= 0) &\n",
    "            (all_anchors[:, 1] >= 0) &\n",
    "            (all_anchors[:, 2] < im_info[1]) &  # width\n",
    "            (all_anchors[:, 3] < im_info[0])  # height\n",
    "        )[0]\n",
    "        anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "        # Label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        labels = np.empty((len(inds_inside), ), dtype=np.float32)\n",
    "        labels.fill(-1)\n",
    "\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        # Overlaps between the Anchors and the Ground Truth boxes\n",
    "        overlaps = bbox_overlaps(\n",
    "            np.ascontiguousarray(anchors, dtype=np.float),\n",
    "            np.ascontiguousarray(gt_boxes, dtype=np.float))\n",
    "        argmax_overlaps = overlaps.argmax(axis=1)\n",
    "        max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "        gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
    "        gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n",
    "        gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Positives clobber Negatives\"\n",
    "        if not CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Foreground label: for each Ground Truth box, anchor with highest overlap\n",
    "        labels[gt_argmax_overlaps] = 1\n",
    "\n",
    "        # Foreground label: above threshold IOU\n",
    "        labels[max_overlaps >= POSITIVE_OVERLAP] = 1\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Negatives clobber Positives\"\n",
    "        if CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Subsample positive labels if we have too many\n",
    "        num_fg = int(FG_FRACTION * RPN_BS)\n",
    "        fg_inds = np.where(labels == 1)[0]\n",
    "        if len(fg_inds) > num_fg:\n",
    "            disable_inds = npr.choice(fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        # Subsample negative labels if we have too many\n",
    "        num_bg = RPN_BS - np.sum(labels == 1)\n",
    "        bg_inds = np.where(labels == 0)[0]\n",
    "        if len(bg_inds) > num_bg:\n",
    "            disable_inds = npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        bbox_targets = compute_targets_atl(anchors, gt_boxes[argmax_overlaps, :])\n",
    "        \n",
    "        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        # Only the positive ones have regression targets\n",
    "        bbox_inside_weights[labels == 1, :] = np.array((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "        labels = labels.numpy()\n",
    "        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        if RPN_POSITIVE_WEIGHT < 0:\n",
    "            # Uniform weighting of examples (given non-uniform sampling)\n",
    "            num_examples = np.sum(labels >= 0)\n",
    "            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "        else:\n",
    "            assert ((RPN_POSITIVE_WEIGHT > 0) &\n",
    "                    (RPN_POSITIVE_WEIGHT < 1))\n",
    "            positive_weights = (\n",
    "                RPN_POSITIVE_WEIGHT / np.sum(labels == 1))\n",
    "            negative_weights = (\n",
    "                (1.0 - RPN_POSITIVE_WEIGHT) / np.sum(labels == 0))\n",
    "        bbox_outside_weights[labels == 1, :] = positive_weights\n",
    "        bbox_outside_weights[labels == 0, :] = negative_weights\n",
    "\n",
    "        # Map up to original set of anchors\n",
    "        total_anchors = all_anchors.shape[0]\n",
    "        labels = unmap(labels, total_anchors, inds_inside, fill=-1)\n",
    "\n",
    "        bbox_targets = unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n",
    "        bbox_inside_weights = unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n",
    "        bbox_outside_weights = unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n",
    "\n",
    "        # Labels\n",
    "        labels = labels.reshape((1, height, width, self.n_anchors)).transpose(0, 3, 1, 2)\n",
    "        labels = labels.reshape((1, 1, self.n_anchors * height, width))\n",
    "        rpn_labels = labels\n",
    "        \n",
    "        # Bounding boxes\n",
    "        bbox_targets = bbox_targets.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_targets = bbox_targets\n",
    "        bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_inside_weights = bbox_inside_weights\n",
    "        bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_outside_weights = bbox_outside_weights\n",
    "\n",
    "        # Re-shape for future use\n",
    "        rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\n",
    "        rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_labels = rpn_labels.long()\n",
    "\n",
    "        # Data storing\n",
    "        self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "        self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "        self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "        self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "\n",
    "        for k in self._anchor_targets.keys():\n",
    "            self._score_summaries[k] = self._anchor_targets[k]\n",
    "\n",
    "        return rpn_labels\n",
    "        \n",
    "    def proposal_target_layer(self, proposed_rois, proposed_roi_scores, gt_boxes):\n",
    "        '''\n",
    "        1. Calculate overlap between ROI and GT boxes\n",
    "        2. Select promising ROIs by comparing against threshold(s)\n",
    "        3. Compute bounding box target regression targets and get bounding box regression labels\n",
    "        '''\n",
    "        # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "\n",
    "        num_images = 1\n",
    "        rois_per_image = RPN_BS / num_images\n",
    "        fg_rois_per_image = int(round(FG_FRACTION * rois_per_image))\n",
    "\n",
    "        # Sample rois with classification labels and bounding box regression targets\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "        overlaps = bbox_overlaps(proposed_rois[:, 1:5].data, gt_boxes[:, :4].data)\n",
    "        max_overlaps, gt_assignment = overlaps.max(1)\n",
    "        labels = gt_boxes[gt_assignment, [4]]\n",
    "\n",
    "        # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "        fg_inds = (max_overlaps >= FG_THRESH).nonzero().view(-1)\n",
    "        \n",
    "        # Guard against the case when an image has fewer than fg_rois_per_image\n",
    "        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "        bg_inds = ((max_overlaps < BG_THRESH_HI) + (max_overlaps >= BG_THRESH_LO) == 2).nonzero().view(-1)\n",
    "\n",
    "        # Ensure a fixed number of regions are sampled (optional?)\n",
    "        fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image = fix_sample_regions(fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image)\n",
    "        \n",
    "        # The indices that we're selecting (both fg and bg)\n",
    "        keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "        # Select sampled values from various arrays:\n",
    "        labels = labels[keep_inds].contiguous()\n",
    "\n",
    "        # Clamp labels for the background RoIs to 0\n",
    "        labels[int(fg_rois_per_image):] = 0\n",
    "        rois_final = proposed_rois[keep_inds].contiguous()\n",
    "        roi_scores_final = proposed_roi_scores[keep_inds].contiguous()\n",
    "        # Compute bounding box target regression targets\n",
    "        bbox_target_data = compute_targets_ptl(rois_final[:, 1:5].data, gt_boxes[gt_assignment[keep_inds]][:, :4].data, labels.data)\n",
    "        bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(bbox_target_data, num_of_class)\n",
    "\n",
    "        # Reshape tensors\n",
    "        rois_final = rois_final.view(-1, 5)\n",
    "        roi_scores_final = roi_scores_final.view(-1)\n",
    "        labels = labels.view(-1, 1)\n",
    "        bbox_targets = bbox_targets.view(-1, num_of_class * 4)\n",
    "        bbox_inside_weights = bbox_inside_weights.view(-1, num_of_class * 4)\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "        \n",
    "        self._proposal_targets['rois'] = rois_final\n",
    "        self._proposal_targets['labels'] = labels.long()\n",
    "        self._proposal_targets['bbox_targets'] = bbox_targets\n",
    "        self._proposal_targets['bbox_inside_weights'] = bbox_inside_weights\n",
    "        self._proposal_targets['bbox_outside_weights'] = bbox_outside_weights\n",
    "\n",
    "        return rois_final, roi_scores_final \n",
    "\n",
    "    def region_proposal(self, net_conv, bb, anchors):\n",
    "        \"\"\"\n",
    "        Input: features from head network, bounding boxes, anchors generated\n",
    "        Output: rois\n",
    "        1. Proposal Layer\n",
    "        2. Anchor Target Layer\n",
    "        3. Compute RPN Loss\n",
    "        4. Proposal Target Layer\n",
    "        \n",
    "        Features -> class probabilities of anchors(fg or bg) and bbox coeff of anchors (to adjust them)\n",
    "        \"\"\"\n",
    "                                       \n",
    "        # TODO: Note down dimensions of features for each function\n",
    "        rpn = F.relu(self.rpn_net(net_conv))\n",
    "        rpn_cls_score = self.rpn_cls_score_net(rpn)\n",
    "                                       \n",
    "        # rpn_cls_score_reshape = rpn_cls_score.view(1, 2, -1, rpn_cls_score.size()[-1]) # batch * 2 * (num_anchors*h) * w\n",
    "        rpn_cls_score_reshape = rpn_cls_score.view(bs, 2, -1, rpn_cls_score.size()[-1]) # for batch\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, dim=1)\n",
    "\n",
    "        # Move channel to the last dimenstion, to fit the input of python functions\n",
    "        rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score).permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score = rpn_cls_score.permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score_reshape = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous() # batch * (num_anchors*h) * w * 2\n",
    "        # rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(-1, 2), 1)[1]\n",
    "        rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(bs, -1, 2), 1)[1]  # for batch (not sure)\n",
    "\n",
    "        rpn_bbox_pred = self.rpn_bbox_pred_net(rpn)\n",
    "        rpn_bbox_pred = rpn_bbox_pred.permute(0, 2, 3, 1).contiguous()  # batch * h * w * (num_anchors*4)                  \n",
    "\n",
    "        if self.mode == 'TRAIN':\n",
    "            rois, roi_scores = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, n_anchors=self.n_anchors)\n",
    "            rpn_labels = self.anchor_target_layer(rpn_cls_score, gt_boxes=bb, all_anchors=anchors)\n",
    "            rois, _ = self.proposal_target_layer(rois, roi_scores, gt_boxes=bb)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        self._predictions[\"rpn_cls_score\"] = rpn_cls_score\n",
    "        self._predictions[\"rpn_cls_score_reshape\"] = rpn_cls_score_reshape\n",
    "        self._predictions[\"rpn_cls_prob\"] = rpn_cls_prob\n",
    "        self._predictions[\"rpn_cls_pred\"] = rpn_cls_pred\n",
    "        self._predictions[\"rpn_bbox_pred\"] = rpn_bbox_pred\n",
    "        self._predictions[\"rois\"] = rois\n",
    "        \n",
    "        return rois\n",
    "    \n",
    "    def roi_align_layer(self, bottom, rois):\n",
    "        return RoIAlign((POOLING_SIZE, POOLING_SIZE), 1.0 / 16.0, 0)(bottom, rois)\n",
    "    \n",
    "    def _smooth_l1_loss(self, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n",
    "        sigma_2 = sigma**2\n",
    "        box_diff = bbox_pred - bbox_targets\n",
    "        in_box_diff = bbox_inside_weights * box_diff\n",
    "        abs_in_box_diff = torch.abs(in_box_diff)\n",
    "        smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
    "        in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
    "                      + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "        out_loss_box = bbox_outside_weights * in_loss_box\n",
    "        loss_box = out_loss_box\n",
    "        for i in sorted(dim, reverse=True):\n",
    "            loss_box = loss_box.sum(i)\n",
    "        loss_box = loss_box.mean()\n",
    "        return loss_box\n",
    "\n",
    "    def region_classification(self, fc7):\n",
    "        \"\"\"\n",
    "        cls_score\n",
    "            Linear layer (fc7 channels, num_classes)\n",
    "            torch max\n",
    "            softmax\n",
    "        bbox_pred\n",
    "            Linear layer (fc7 channels, num_classes*4)\n",
    "        \"\"\"\n",
    "        \n",
    "        cls_score = self.cls_score_net(fc7)\n",
    "        cls_pred = torch.max(cls_score, 0)[1]\n",
    "        cls_prob = F.softmax(cls_score, dim=0)\n",
    "        bbox_pred = self.bbox_pred_net(fc7)\n",
    "        \n",
    "        self._predictions[\"cls_score\"] = cls_score\n",
    "        self._predictions[\"cls_pred\"] = cls_pred\n",
    "        self._predictions[\"cls_prob\"] = cls_prob\n",
    "        self._predictions[\"bbox_pred\"] = bbox_pred\n",
    "\n",
    "        return cls_prob, bbox_pred\n",
    "\n",
    "    def add_losses(self, sigma_rpn=3.0):\n",
    "                                       \n",
    "        # RPN, class loss\n",
    "        rpn_cls_score = self._predictions['rpn_cls_score_reshape'].view(-1, 2)\n",
    "        rpn_label = self._anchor_targets['rpn_labels'].view(-1)\n",
    "        rpn_select = (rpn_label.data != -1).nonzero().view(-1)\n",
    "        rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\n",
    "        rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\n",
    "        rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "\n",
    "        # RPN, bbox loss\n",
    "        rpn_bbox_pred = self._predictions['rpn_bbox_pred']\n",
    "        rpn_bbox_targets = self._anchor_targets['rpn_bbox_targets']\n",
    "        rpn_bbox_inside_weights = self._anchor_targets['rpn_bbox_inside_weights']\n",
    "        rpn_bbox_outside_weights = self._anchor_targets['rpn_bbox_outside_weights']\n",
    "        rpn_loss_box = self._smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights, sigma=sigma_rpn, dim=[1, 2, 3])\n",
    "\n",
    "        # RCNN, class loss\n",
    "        cls_score = self._predictions[\"cls_score\"]\n",
    "        label = self._proposal_targets[\"labels\"].view(-1)     \n",
    "        cross_entropy = F.cross_entropy(cls_score.view(-1, num_of_class), label)\n",
    "\n",
    "        # RCNN, bbox loss\n",
    "        bbox_pred = self._predictions['bbox_pred']\n",
    "        bbox_targets = self._proposal_targets['bbox_targets']\n",
    "        bbox_inside_weights = self._proposal_targets['bbox_inside_weights']\n",
    "        bbox_outside_weights = self._proposal_targets['bbox_outside_weights']\n",
    "        loss_box = self._smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n",
    "\n",
    "        self._losses['cross_entropy'] = cross_entropy\n",
    "        self._losses['loss_box'] = loss_box\n",
    "        self._losses['rpn_cross_entropy'] = rpn_cross_entropy\n",
    "        self._losses['rpn_loss_box'] = rpn_loss_box\n",
    "\n",
    "        loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n",
    "        self._losses['total_loss'] = loss\n",
    "\n",
    "        for k in self._losses.keys():\n",
    "            self._event_summaries[k] = self._losses[k]\n",
    "\n",
    "        return loss                                       \n",
    "\n",
    "    def forward(self, x, bb, im_info=[320, 320, 1], train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mode = \"TRAIN\"\n",
    "        else:\n",
    "            self.mode = \"TEST\"\n",
    "        # Store image information\n",
    "        self._im_info = im_info\n",
    "                                       \n",
    "        # Pass the image through the Backbone ConvNet to generate the series of Feature maps\n",
    "        head_conv_net = self.head_net()\n",
    "        output_head = head_conv_net(x) # current: [1, 1024, 154, 154], should be [bs, 1024, 154, 154]\n",
    "        print(output_head.size())\n",
    "        anchors, length = generate_anchors(output_head.size(2), output_head.size(3))\n",
    "        anchors = torch.from_numpy(anchors)\n",
    "#         print(\"Output_head: \", output_head.shape)\n",
    "        rois = self.region_proposal(output_head, bb, anchors)\n",
    "#         print(\"ROIS: \", rois.shape) # [256, 5]\n",
    "        pool5 = self.roi_align_layer(output_head, rois)\n",
    "        # print(\"POOL5\", pool5.shape) # [256, 1024, 7, 7]\n",
    "        fc7 = self.fc7()(pool5)\n",
    "        # print(\"fc7: \", fc7.shape) # [256, 256, 5, 5]\n",
    "        fc7 = fc7.view(-1)\n",
    "        # print(\"fc7: \", fc7.shape) # [1638400]\n",
    "        cls_prob, bbox_pred = self.region_classification(fc7)\n",
    "        loss = self.add_losses()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "#         # Using x, the generated series of Feature maps (64 x H x W):\n",
    "        \n",
    "#         # (1) Predict Bbox Anchor positions\n",
    "#         scores_bbox_anch = self.conv_bbox_anch(x).squeeze()\n",
    "        \n",
    "#         # (2.1) Predict Classifications for Bboxes\n",
    "#         # If this is during training time, only need to predict Bbox classifications\n",
    "#         if train_flag:\n",
    "            \n",
    "#             # Initialise list\n",
    "#             bbox_list = []\n",
    "            \n",
    "#             # Iterate through each Bbox in an image, in each batch\n",
    "#             for b in range(batch_size):\n",
    "#                 for k in range(nb_objects):\n",
    "#                     xmin = bb[b][k, 0].long()\n",
    "#                     ymin = bb[b][k, 1].long()\n",
    "#                     xmax = bb[b][k, 2].long() # xmax - xmin = width\n",
    "#                     ymax = bb[b][k, 3].long() # ymax - ymin = height\n",
    "#                     bbox_list.append(x[b,:,xmin:xmax,ymin:ymax])\n",
    "\n",
    "#             # TODO: Objects may have different sizes\n",
    "#             for i in range(len(bbox_list)):\n",
    "#                 print(bbox_list[i].size())\n",
    "#             assert bbox_list[0].size() == bbox_list[1].size() # Quick check\n",
    "            \n",
    "#             # Current size: batch_size*nb_objects x hidden_dim x H x W\n",
    "#             bbox_list = torch.stack(bbox_list, dim = 0)\n",
    "\n",
    "#             # Current size: batch_size*nb_objects x hidden_dim*H*W\n",
    "#             bbox_list = bbox_list.view(-1, hidden_dim * ob_size**2)\n",
    "            \n",
    "#             # Current size: batch_size*nb_objects x num_of_class\n",
    "#             scores_bbox_class = self.linear_bbox_class(bbox_list)\n",
    "\n",
    "#         # (2.2) Predict Classifications for Bboxes\n",
    "#         # If this is during test time, go one step further to find the co-ordinates of the Bboxes\n",
    "#         else:\n",
    "            \n",
    "#             # Initialise list\n",
    "#             batch_bbox_list = []\n",
    "            \n",
    "#             # Iterate through each batch\n",
    "#             for b in range(batch_size):\n",
    "                \n",
    "#                 # For each image, we assume that there are K number of objects\n",
    "#                 # TODO: What if there are different numbers of objects?\n",
    "#                 # We first compute the co-ordinates of the top K Bboxes in terms of Anchor scores\n",
    "#                 scores_bbox_anch_b = scores_bbox_anch[b,:,:]\n",
    "#                 scores_bbox_anch_b = scores_bbox_anch_b.view(-1)\n",
    "#                 _, idx_largest = torch.sort(scores_bbox_anch_b, descending = True)\n",
    "#                 idx_largest = idx_largest[:nb_objects]\n",
    "                \n",
    "#                 # We can calculate the y-coordinate and the x-coordinate separately\n",
    "#                 idx_y = idx_largest // im_size # [nb_objects]\n",
    "#                 idx_x = idx_largest - (idx_y * im_size) # [nb_objects]\n",
    "#                 print(idx_largest)\n",
    "#                 print(idx_y)\n",
    "#                 print(idx_x)\n",
    "                \n",
    "#                 # We then extract the top K Bboxes in terms of Anchor scores\n",
    "#                 bbox = []\n",
    "#                 for k in range(nb_objects):\n",
    "#                     xmin = idx_x[k] - offset\n",
    "#                     ymin = idx_y[k] - offset\n",
    "#                     xmax = idx_x[k] - offset + ob_size # xmax - xmin = width\n",
    "#                     ymax = idx_y[k] - offset + ob_size # ymax - ymin = height\n",
    "#                     bbox.append(x[b,:,ymin:ymax,xmin:xmax])\n",
    "                \n",
    "#                 # Current size: nb_objects x hidden_dim x H x W\n",
    "#                 bbox = torch.stack(bbox, dim = 0)\n",
    "\n",
    "#                 # Current size: nb_objects x hidden_dim*H*W\n",
    "#                 bbox = bbox.view(-1, hidden_dim * ob_size**2)\n",
    "#                 batch_bbox_list.append(bbox)\n",
    "\n",
    "#             # Once we have run through all the batches, we can compute the class scores of the Bboxes\n",
    "            \n",
    "#             # Current size: batch_size*nb_objects x hidden_dim*H*W\n",
    "#             batch_bbox_list = torch.cat(batch_bbox_list, dim=0)\n",
    "                \n",
    "#             # Current size: batch_size*nb_objects x num_of_class            \n",
    "#             scores_bbox_class = self.linear_bbox_class(batch_bbox_list)\n",
    "        \n",
    "#         # Return the results from (1) and (2.1) / (2.2)\n",
    "#         return scores_bbox_class, scores_bbox_anch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster_R_CNN(\n",
      "  (head_conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (head_batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head_relu1): ReLU()\n",
      "  (head_pool1): MaxPool2d(kernel_size=[3, 3], stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (head_layer1): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_relu2): ReLU()\n",
      "  (head_layer2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool2): MaxPool2d(kernel_size=[3, 3], stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (head_relu3): ReLU()\n",
      "  (head_layer3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool3): MaxPool2d(kernel_size=[3, 3], stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (head_relu4): ReLU()\n",
      "  (rpn_net): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rpn_cls_score_net): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rpn_bbox_pred_net): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (cls_score_net): Linear(in_features=25, out_features=7, bias=True)\n",
      "  (bbox_pred_net): Linear(in_features=25, out_features=28, bias=True)\n",
      ")\n",
      "There are 10798468 (10.80 million) parameters in this neural network\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "net = faster_R_CNN()\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "print(utils.display_num_param(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1024, 154, 154])\n",
      "boxes torch.Size([213444, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4624\\1583832658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_bboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4624\\3248239868.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, bb, im_info, train_flag)\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0manchors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;31m#         print(\"Output_head: \", output_head.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[0mrois\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregion_proposal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;31m#         print(\"ROIS: \", rois.shape) # [256, 5]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mpool5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroi_align_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4624\\3248239868.py\u001b[0m in \u001b[0;36mregion_proposal\u001b[1;34m(self, net_conv, bb, anchors)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'TRAIN'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[0mrois\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroi_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproposal_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrpn_cls_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpn_bbox_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_anchors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_anchors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m             \u001b[0mrpn_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_target_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrpn_cls_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_anchors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mrois\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproposal_target_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroi_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4624\\3248239868.py\u001b[0m in \u001b[0;36mproposal_layer\u001b[1;34m(self, cls_prob, bbox_pred, anchors, n_anchors)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# for batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# proposals = bbox_transform_inv(anchors, rpn_bbox_pred) # shift boxes based on prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mproposals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbbox_transform_inv_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpn_bbox_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# for batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[1;31m# proposals = clip_boxes(proposals, self._im_info[:2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip_boxes_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_im_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# for batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4624\\561340256.py\u001b[0m in \u001b[0;36mbbox_transform_inv_batch\u001b[1;34m(boxes, deltas, batch_size)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbbox_transform_inv_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'boxes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mwidths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[0mheights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[0mctr_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mwidths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Set initial learning rate and Optimizer\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = init_lr)\n",
    "\n",
    "# Set training variables\n",
    "running_loss = 0\n",
    "num_batches = 0\n",
    "start = time.time()\n",
    "\n",
    "# Training process\n",
    "for epoch in range(10):\n",
    "    for data in train_dataloader:   \n",
    "        batch_images, batch_bboxes = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "        loss = net(batch_images, batch_bboxes)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', init_lr  ,'\\t loss=', total_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb3f132b8f46ef83ff81b7067b37036dbb49ec9e6fa8b80a12ce6c1c87b906f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('deeplearn_course': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
