{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Really Counts: A Cash Recognization System\n",
    "\n",
    "---\n",
    "\n",
    "**Group Members**:\n",
    "- Ek Chin Hui (A0205578X)\n",
    "- Lee Hyung Woon (A0218475X)\n",
    "- Toh Hai Jie Joey (A0205141Y)\n",
    "\n",
    "---\n",
    "\n",
    "Our project, named **What Really Counts**, is a Cash Recognization System for the Visually Impaired in Singapore. In Singapore, the disabled community face many challenges in their daily lives, and this is especially so for those who are hampered by visual impairments. One such challenge they face is cash payment, as they need to identify the correct combinations of bills and coins. Hence, our aim was to contruct a system that can help them overcome these challenges by employing a deep learning-based Object Detection model using Convolutional Neural Networks (CNN) - in particular, the Faster R-CNN model. \n",
    "\n",
    "**What Really Counts** is an architecture that detects and analyzes given images of Singapore Currencies (bills and/or coins), and is primarily designed to assist the visually impaired in identifying the correct combinations of bills and coins. The model uses CNNs to perform image classification on the objects detected in a given input image, through which we can ascertain the exact number and type of bills / coins present in the image, allowing us to calculate and return the sum of the currency to the user.\n",
    "\n",
    "For this project, we will gather and pre-process our own dataset, and then move onto training and testing of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "The following are modules that we used for this project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import nms, RoIAlign\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "For this project, we collected data by taking about 150 pictures of different combinations of bills and coins.\n",
    "\n",
    "As an initial proof of concept, we decided to focus on the 7 most common classes of currencies in Singapore: \\\\$10, \\\\$5, \\\\$2, \\\\$1, 50c, 20c, 10c.\n",
    "\n",
    "After we gathered the data, we manually labelled the bounding boxes and their respective classes using [LabelImg](https://github.com/tzutalin/labelImg), which produces an XML file for each image to store the bounding box and class information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set class labels\n",
    "\n",
    "coin_labels = ('10c', '20c', '50c', '$1', '$2', '$5', '$10')\n",
    "\n",
    "label_index_dict = {k:v+1 for v, k in enumerate(coin_labels)}\n",
    "label_index_dict['background'] = 0\n",
    "print(label_index_dict)\n",
    "\n",
    "index_label_dict = {v+1:k for v, k in enumerate(coin_labels)}\n",
    "index_label_dict[0] = 'background'\n",
    "print(index_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we defined a function to parse the XML files into a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(annotation_path):\n",
    "    '''\n",
    "    Function to convert XML data of a single image into an object.\n",
    "    The object contains the Bbox parameters and the corresponding label for each Bbox.\n",
    "    '''\n",
    "\n",
    "    # Parse the XML file into a tree structure\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    height = float(root.find('size').find('height').text)\n",
    "    width = float(root.find('size').find('width').text)\n",
    "\n",
    "    # Set initial lists\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "\n",
    "    # Loop over each Bbox found in the XML file\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        # Convert Bbox co-ordinates\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert Bbox Label\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_index_dict:\n",
    "            continue\n",
    "        labels.append(label_index_dict[label])\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we defined another function that makes use of the earlier `parse_annotation` function to convert the XML files into JSON objects.\n",
    "\n",
    "Each bounding box is stored as [xmin, ymin, xmax, ymax]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_json(*files):\n",
    "    '''\n",
    "    Function to convert image and XML data into two separate JSON objects.\n",
    "    One object for images, and another object for Bbox co-ordinate values and labels.\n",
    "    '''\n",
    "    \n",
    "    # Initialise lists\n",
    "    images_list = [] \n",
    "    objects_list = []\n",
    "    files = [file for sublist in files for file in sublist]\n",
    "    \n",
    "    # Set up two JSON files to be written\n",
    "    images_file = open(\"TRAIN_images.json\", 'w')\n",
    "    objects_file = open(\"TRAIN_objects.json\", 'w')\n",
    "    \n",
    "    # Iterate through each XML-Image pair\n",
    "    for file in files:\n",
    "    \n",
    "        # Add each image file path into the images list\n",
    "        file_path = os.path.splitext(file)[0]   \n",
    "        images_list.append(file_path + \".jpg\")\n",
    "        \n",
    "        # Add each XML object into the objects list\n",
    "        xml_dict = parse_annotation(file)\n",
    "        objects_list.append(xml_dict)\n",
    "    \n",
    "    # Write each list into the corresponding JSON files\n",
    "    json.dump(images_list, images_file)\n",
    "    json.dump(objects_list, objects_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we convert our dataset from XML to JSON accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_FILES = glob(r'dataset/ch dataset/*.xml')\n",
    "xml_to_json(CH_FILES)\n",
    "\n",
    "# JY_FILES = glob(r'dataset/joey dataset/*.xml')\n",
    "# HW_FILES = glob(r'dataset/hw dataset/*.xml')\n",
    "# xml_to_json(CH_FILES, HW_FILES)\n",
    "# xml_to_json(HW_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before getting started with our network, we need to do the necessary data preparation and pre-processing...\n",
    "\n",
    "The function `normalise_and_resize` was partially adapted from the [PyTorch Tutorial to Object Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py).\n",
    "\n",
    "First, we declare a function to resize and normalise the given images and the bounding boxes. By default, we set it to dimensions `(320, 320)`, which is the standard input size that we will use for our network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_and_resize(image, boxes, labels, dims = (320, 320)):\n",
    "    '''\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Resize the input images and the bounding boxes, and apply Normalisations.\n",
    "    \n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
    "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
    "    '''\n",
    "\n",
    "    # Mean and Standard deviation used for the base VGG from torchvision\n",
    "    # See: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Resize image and convert the image to Torch tensor\n",
    "    new_image = FT.resize(image, dims)\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "    \n",
    "    # Normalize the image by mean and standard deviation\n",
    "    new_image = FT.normalize(new_image, mean = mean, std = std)\n",
    "\n",
    "    # Resize Bounding boxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "    new_boxes = (boxes / old_dims) * new_dims\n",
    "    \n",
    "    return new_image, new_boxes, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we declare a class PascalVOCDataset (Dataset in Pascal VOC format) that we can use to create batches of data easily and efficiently..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader.\n",
    "    This class is primarily used to create batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_folder, split):\n",
    "        '''\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param split: split, one of 'TRAIN' or 'TEST'\n",
    "        '''\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.split = split.upper()\n",
    "        \n",
    "        # The values of split should only be either 'TRAIN' or 'TEST'\n",
    "        assert self.split in {'TRAIN', 'TEST'}\n",
    "\n",
    "        # Read from the JSON files (initially craeted from our XML files)\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        # Number of images must match the number of objects containing the Bboxes for each image\n",
    "        assert len(self.images) == len(self.objects)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        '''\n",
    "        ...\n",
    "        '''\n",
    "\n",
    "        # Read image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensor = transforms.Resize(size = (320, 320))(image_tensor)\n",
    "\n",
    "        # Read objects in this image (Bboxes, labels)\n",
    "        objects = self.objects[i]\n",
    "        box = torch.FloatTensor(objects['boxes'])\n",
    "        label = torch.LongTensor(objects['labels'])\n",
    "\n",
    "        # Apply normalisations and resizes\n",
    "        image, box, label = normalise_and_resize(image, box, label)\n",
    "        box_and_label = torch.cat([box, label.unsqueeze(1)], 1)\n",
    "        return image_tensor, box_and_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes using Python lists.\n",
    "        \n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        '''\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "\n",
    "        images = torch.stack(images, dim = 0)\n",
    "        return images, boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load our training and test dataset using PyTorch's DataLoader, which allows us to iterate through the dataset easily. The original dataset was randomly split into the train and test set, with a ratio of 80:20. \n",
    "\n",
    "Additionally, since our dataset was relatively small, we chose to stick to a batch size of 1 to achieve better training stability and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PascalVOCDataset(\".\", \"TRAIN\")\n",
    "length = len(dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [math.floor(0.8*length), math.ceil(0.2*length)])\n",
    "train_dataloader = DataLoader(train_data, batch_size = 1, shuffle = True, collate_fn = dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 1, shuffle = True, collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation\n",
    "\n",
    "To ensure that our Data Preparations are complete, we may use the `matplotlib` library to visualise our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import math\n",
    "\n",
    "color_maps = ['r', 'g', 'b', 'y', 'm', 'w', 'k', 'c']\n",
    "\n",
    "# Visualise data\n",
    "for data in train_dataloader:\n",
    "    for batch in range(1):\n",
    "        img = data[0][batch]\n",
    "        boxes = data[1][batch]\n",
    "#         labels = data[2][batch].tolist()\n",
    "#         named_labels = [index_label_dict[label] for label in labels]\n",
    "        plt.imshow(transforms.ToPILImage()(img))\n",
    "        ax = plt.gca()\n",
    "        labelled = set()\n",
    "        for i, box in enumerate(boxes):\n",
    "            w,h = box[2] - box[0], box[3] - box[1]\n",
    "            x,y = box[0].item(), box[1].item()\n",
    "            x = [x, x + w, x + w, x, x]\n",
    "            y = [y, y, y + h, y + h, y]\n",
    "            label = int(box[4].item())\n",
    "            if label not in labelled:\n",
    "                plt.plot(x,y, color = color_maps[label], label = index_label_dict[label])\n",
    "                labelled.add(label)\n",
    "            else:\n",
    "                plt.plot(x,y, color = color_maps[label])\n",
    "            plt.legend(loc = 'best')\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Model \n",
    "\n",
    "In order to simplify our model, we first declared a few utility functions.\n",
    "\n",
    "Firstly, we declare functions that will help us generate the anchors for our network. To do so efficiently, we need functions to help us switch the data format of the images (and the bounding boxes) from the Pascal VOC format to the YOLO format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pascal2yolo(anchor):\n",
    "    '''\n",
    "    Transforms anchor coordinates of the form [xmin, ymin, xmax, ymax] to [x_center, y_center, width, height]\n",
    "    '''\n",
    "    \n",
    "    width = anchor[2] - anchor[0] + 1\n",
    "    height = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + (width-1)/2 \n",
    "    y_ctr = anchor[1] + (height-1)/2\n",
    "    return width, height, x_ctr, y_ctr\n",
    "\n",
    "def yolo2pascal(width, height, x_ctr, y_ctr):\n",
    "    '''\n",
    "    Transforms anchor coordinates of the form [x_center, y_center, width, height] to [xmin, ymin, xmax, ymax]\n",
    "    '''\n",
    "    \n",
    "    width = width[:, np.newaxis]\n",
    "    height = height[:, np.newaxis]\n",
    "    anchors = np.hstack((x_ctr - 0.5 * (width - 1), y_ctr - 0.5 * (height - 1),\n",
    "                         x_ctr + 0.5 * (width - 1), y_ctr + 0.5 * (height - 1)))\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we declare utility functions for generating the anchors themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ratio_anchors(anchor, ratios=(0.5,1,2)):\n",
    "    '''\n",
    "    Generate anchors for each width:height ratio\n",
    "    '''\n",
    "    \n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor)\n",
    "    size = w*h\n",
    "    size_ratios = size / ratios\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    hs = np.round(ws * ratios)\n",
    "    anchors = yolo2pascal(ws, hs, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_scale_anchors(anchor, scales=np.array((8,16,32))):\n",
    "    '''\n",
    "    Generate anchors for each scale\n",
    "    '''\n",
    "    \n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor) \n",
    "    scaled_w = w * scales\n",
    "    scaled_h = h * scales\n",
    "    anchors = yolo2pascal(scaled_w, scaled_h, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_anchors(height, width, aspect_ratio=np.array((0.5,1,2)), stride_length=16, scales=np.array((8,16,32))):\n",
    "    '''\n",
    "    Generate anchors of differing scale and aspect ratios\n",
    "    '''\n",
    "    \n",
    "    base_anchor = pascal2yolo([0,0,15,15]) # 16, 16, 7.5, 7.5\n",
    "    ratio_anchors = generate_ratio_anchors(base_anchor, ratios=aspect_ratio)\n",
    "    anchors = np.vstack([\n",
    "        generate_scale_anchors(ratio_anchors[i, :], scales)\n",
    "        for i in range(ratio_anchors.shape[0])\n",
    "    ])\n",
    "    A = anchors.shape[0]\n",
    "    shift_x = np.arange(0, width) * stride_length\n",
    "    shift_y = np.arange(0, height) * stride_length\n",
    "    \n",
    "    # Shift each ratio\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n",
    "                        shift_y.ravel())).transpose()\n",
    "    K = shifts.shape[0]\n",
    "    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)) # H, W, C\n",
    "    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n",
    "    length = np.int32(anchors.shape[0])\n",
    "\n",
    "    return anchors, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we declare a couple of utility functions to help us manipulate and deal with bounding boxes.\n",
    "\n",
    "BBox transformations using deltas & inverse ...\n",
    "\n",
    "Clip bboxes to image boundaries...\n",
    "\n",
    "Calculate bbox overlaps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transform(ex_rois, gt_rois):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    \n",
    "    ex_rois = ex_rois.to(device)\n",
    "    gt_rois = gt_rois.to(device)\n",
    "    \n",
    "    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n",
    "    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n",
    "\n",
    "    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n",
    "    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "    targets_dw = torch.log(gt_widths / ex_widths)\n",
    "    targets_dh = torch.log(gt_heights / ex_heights)\n",
    "\n",
    "    targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), 1)\n",
    "    return targets.cpu()\n",
    "\n",
    "def bbox_transform_inv(boxes, deltas):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return deltas.detach() * 0\n",
    "    boxes = boxes.to(device)\n",
    "    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "    dx = deltas[:, 0::4]\n",
    "    dy = deltas[:, 1::4]\n",
    "    dw = deltas[:, 2::4]\n",
    "    dh = deltas[:, 3::4]\n",
    "\n",
    "    pred_ctr_x = dx * widths.unsqueeze(1) + ctr_x.unsqueeze(1)\n",
    "    pred_ctr_y = dy * heights.unsqueeze(1) + ctr_y.unsqueeze(1)\n",
    "    pred_w = torch.exp(dw) * widths.unsqueeze(1)\n",
    "    pred_h = torch.exp(dh) * heights.unsqueeze(1)\n",
    "\n",
    "    pred_boxes = torch.cat([_.unsqueeze(2) for _ in [pred_ctr_x - 0.5 * pred_w,\n",
    "        pred_ctr_y - 0.5 * pred_h,\n",
    "        pred_ctr_x + 0.5 * pred_w,\n",
    "        pred_ctr_y + 0.5 * pred_h]], 2).view(len(boxes), -1)\n",
    "    return pred_boxes\n",
    "\n",
    "def clip_boxes(boxes, im_shape):\n",
    "    '''\n",
    "    Clip boxes to image boundaries\n",
    "    '''\n",
    "\n",
    "    if not hasattr(boxes, 'data'):\n",
    "        boxes_ = boxes.numpy()\n",
    "\n",
    "    boxes = boxes.view(boxes.size(0), -1, 4)\n",
    "    boxes = torch.stack([boxes[:,:,0].clamp(0, im_shape[1] - 1),\n",
    "        boxes[:,:,1].clamp(0, im_shape[0] - 1),\n",
    "        boxes[:,:,2].clamp(0, im_shape[1] - 1),\n",
    "        boxes[:,:,3].clamp(0, im_shape[0] - 1)], 2).view(boxes.size(0), -1)\n",
    "    return boxes\n",
    "\n",
    "def bbox_overlaps(boxes, query_boxes):\n",
    "    '''\n",
    "    Computes the overlapped area between boxes and query boxes (ground truth boxes)\n",
    "    using intersection-over-union\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: (N, 4) ndarray or tensor or variable\n",
    "    query_boxes: (K, 4) ndarray or tensor or variable\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) overlap between boxes and query_boxes\n",
    "    '''\n",
    "    \n",
    "    # If input is ndarray, turn the overlaps back to ndarray when return\n",
    "    if isinstance(boxes, np.ndarray):\n",
    "        boxes = torch.from_numpy(boxes)\n",
    "        query_boxes = torch.from_numpy(query_boxes)\n",
    "        out_fn = lambda x: x.numpy()  \n",
    "    else:\n",
    "        out_fn = lambda x: x\n",
    "\n",
    "    boxes = boxes.to(device)\n",
    "    query_boxes = query_boxes.to(device)\n",
    "    box_areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    query_areas = (query_boxes[:, 2] - query_boxes[:, 0] + 1) * (query_boxes[:, 3] - query_boxes[:, 1] + 1)\n",
    "\n",
    "    iw = (torch.min(boxes[:, 2:3], query_boxes[:, 2:3].t()) - torch.max(boxes[:, 0:1], query_boxes[:, 0:1].t()) + 1).clamp(min=0)\n",
    "    ih = (torch.min(boxes[:, 3:4], query_boxes[:, 3:4].t()) - torch.max(boxes[:, 1:2], query_boxes[:, 1:2].t()) + 1).clamp(min=0)\n",
    "    ua = box_areas.view(-1, 1) + query_areas.view(1, -1) - iw * ih\n",
    "    overlaps = iw * ih / ua\n",
    "    return out_fn(overlaps.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unmap function for the anchor target layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmap(data, count, inds, fill=0):\n",
    "    '''\n",
    "    Unmap a subset of item (data) back to the original set of items (of size count)\n",
    "    '''\n",
    "    \n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count, ), dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds] = data\n",
    "    else:\n",
    "        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_regression_labels(bbox_target_data, num_classes):\n",
    "    '''\n",
    "    Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "    compact form N x (class, tx, ty, tw, th)\n",
    "    This function expands those targets into the 4-of-4*K representation used\n",
    "    by the network (i.e. only one class has non-zero targets).\n",
    "    \n",
    "    Returns:\n",
    "        bbox_target (ndarray): N x 4K blob of regression targets\n",
    "        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n",
    "    '''\n",
    "\n",
    "    clss = bbox_target_data[:, 0]\n",
    "    bbox_targets = clss.new_zeros(clss.numel(), 4 * num_classes)\n",
    "    bbox_inside_weights = clss.new_zeros(bbox_targets.shape)\n",
    "    inds = (clss > 0).nonzero().view(-1)\n",
    "    if inds.numel() > 0:\n",
    "        clss = clss[inds].contiguous().view(-1, 1)\n",
    "        dim1_inds = inds.unsqueeze(1).expand(inds.size(0), 4)\n",
    "        dim2_inds = torch.cat([4 * clss, 4 * clss + 1, 4 * clss + 2, 4 * clss + 3], 1).long()\n",
    "        test = bbox_target_data[inds][:, 1:]\n",
    "        bbox_targets[dim1_inds, dim2_inds] = test\n",
    "        bbox_inside_weights[dim1_inds, dim2_inds] = bbox_targets.new(BBOX_INSIDE_WEIGHTS).view(-1, 4).expand_as(dim1_inds)\n",
    "\n",
    "    return bbox_targets, bbox_inside_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_sample_regions(fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image):\n",
    "    '''\n",
    "    Function to ensure total number of foreground and background ROIs is constant\n",
    "    by randomly repeating background indices to make up for lesser foreground indices.\n",
    "    '''\n",
    "    \n",
    "    if fg_inds.numel() == 0 and bg_inds.numel() == 0:\n",
    "        to_replace = all_rois.size(0) < rois_per_image\n",
    "        bg_inds = torch.from_numpy(npr.choice(np.arange(0, all_rois.size(0)), size=int(rois_per_image), replace=to_replace)).long()\n",
    "        fg_rois_per_image = 0\n",
    "    elif fg_inds.numel() > 0 and bg_inds.numel() > 0:\n",
    "        fg_rois_per_image = min(fg_rois_per_image, fg_inds.numel())\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(fg_rois_per_image),\n",
    "                replace=False)).long().to(gt_boxes.device)]\n",
    "        bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "        to_replace = bg_inds.numel() < bg_rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(bg_rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "    elif fg_inds.numel() > 0:\n",
    "        to_replace = fg_inds.numel() < rois_per_image\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = rois_per_image\n",
    "    elif bg_inds.numel() > 0:\n",
    "        to_replace = bg_inds.numel() < rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = 0\n",
    "\n",
    "    return fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Model\n",
    "\n",
    "First, we set the network constants as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set network constants\n",
    "HIDDEN_DIM = 64\n",
    "NUM_OF_CLASS = 8\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Thresholds\n",
    "# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n",
    "# Overlap threshold for a ROI to be considered background (class = 0 if overlap in [LO, HI))\n",
    "FG_THRESH = 0.5 \n",
    "BG_THRESH_HI = 0.5\n",
    "BG_THRESH_LO = 0.1\n",
    "\n",
    "PRE_NMS_TOPN = 12000\n",
    "POST_NMS_TOPN = 2000\n",
    "NMS_THRESH = 0.7\n",
    "\n",
    "POSITIVE_OVERLAP = 0.7\n",
    "NEGATIVE_OVERLAP = 0.3\n",
    "CLOBBER_POSITIVES = False\n",
    "RPN_BATCH_SIZE = 8\n",
    "FG_FRACTION = 0.5\n",
    "RPN_POSITIVE_WEIGHT = -1.0\n",
    "POOLING_SIZE = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow of our faster RCNN model is as follows:\n",
    "### 1. Head Network\n",
    "The head network consists of 4 convolutional layers with max pooling and RELU activation between each layer. It serves as the \"backbone\" network and its purpose is to produce convolutional feature maps to be used in anchor generation and region proposal network.\n",
    "### 2. Anchor Generation Layer\n",
    "The anchor generation layer generates a fixed number of anchors. We first generated 9 anchors of differing scales (8, 16, 32) and aspect ratios (0.5, 1, 2) and then duplicated these anchors by translating them across uniformly spaced grid points spanning the input image.\n",
    "### 3. Region Proposal Network (RPN)\n",
    "The region proposal layer runs feature maps produced by the head network through a convolutional layer followed by RELU. Its purpose is to produce the background/foreground class scores and probabilities, and corresponding bounding box regression coefficients.\n",
    "### 4. Proposal Layer\n",
    "The proposal layer prunes the number of boxes and transforms the bounding boxes. The pruning is done by applying non-maximum suppression based on the foreground scores, while the transformation is done by applying the regression coefficients generated by the RPN to the corresponding anchor boxes. The purpose of this layer is to produce ROIs.\n",
    "### 5. Anchor Target Layer\n",
    "The anchor target layer selects promising anchors from the anchors generated by first computing the overlap between the anchors and the ground truth boxes. The anchors will then be selected based on certain overlap thresholds. These anchors will then be used to train the RPN. \n",
    "### 6. Proposal Target Layer\n",
    "The proposal target layer selects promising ROIs that are generated by first computing the max overlap between each ROI and all ground truth boxes. The ROIs will then be selected based on certain overlap thresholds.\n",
    "### 7. ROI Pooling Layer\n",
    "The ROI pooling layer crops the features produced by the head network using the ROI Align method (interpolation).\n",
    "### 8. Region Classification Network (RCNN)\n",
    "The region classification layer passes the feature maps produced by the head network through a single convolutional layer and two fully connected (FC) layers. The first FC layer produces the class probability distribution for each region proposal and the second FC layer produces a set of class specific bounding box regressors.\n",
    "### 9. Computing Loss\n",
    "The total loss is the sum of the classification loss and bounding box regression loss in both the RPN and RCNN. The classification loss uses cross entropy loss to penalize incorrectly classified boxes while the regression loss uses a function of the distance between the true regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class faster_R_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The main Faster R-CNN network used for this project.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(faster_R_CNN, self).__init__()\n",
    "        self.feat_stride = [16,]\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        self._layers = {}\n",
    "        self._act_summaries = {}\n",
    "        self._score_summaries = {}\n",
    "        self._event_summaries = {}\n",
    "        self._image_gt_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "        self._fc_channels = RPN_BATCH_SIZE*RPN_BATCH_SIZE*25\n",
    "        self._net_conv_channels = 1024\n",
    "\n",
    "        # This results in num_anchors = 9\n",
    "        anchor_scales = (8, 16, 32)\n",
    "        anchor_ratios = (0.5, 1, 2)\n",
    "        self.n_anchors = len(anchor_scales) * len(anchor_ratios)  \n",
    "        \n",
    "        # HeadNet: Generating a series of Feature maps from the input image\n",
    "        \n",
    "        # Current Size: 3 x h x w\n",
    "        self.head_conv1 = nn.Conv2d(3,  HIDDEN_DIM,  kernel_size=4, stride=2, padding=1)\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_batch_norm1 = nn.BatchNorm2d(HIDDEN_DIM)\n",
    "        # Current Size: 64 x h/2 x w/2 \n",
    "        self.head_relu1 = nn.ReLU()\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_pool1 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        # Current Size: 64 x h/4 x w/4\n",
    "        self.head_layer1 = nn.Conv2d(HIDDEN_DIM,  HIDDEN_DIM*4,  kernel_size=3, padding=1)\n",
    "        self.head_relu2 = nn.ReLU()\n",
    "        # Current Size: 256 x h/4 x w/4\n",
    "        self.head_layer2 = nn.Conv2d(HIDDEN_DIM*4,  HIDDEN_DIM*8,  kernel_size=3, padding=1)\n",
    "        self.head_pool2 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        self.head_relu3 = nn.ReLU()\n",
    "        # Current Size: 512 x h/8 x w/8\n",
    "        self.head_layer3 = nn.Conv2d(HIDDEN_DIM*8,  HIDDEN_DIM*16,  kernel_size=3, padding=1)\n",
    "        self.head_pool3 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        self.head_relu4 = nn.ReLU()\n",
    "        # Current Size: 1024 x h/16 x w/16\n",
    "        \n",
    "        # Region Proposal Network\n",
    "        self.rpn_net = nn.Conv2d(self._net_conv_channels, 512 , kernel_size=3, padding=1)\n",
    "        self.rpn_cls_score_net = nn.Conv2d(512, self.n_anchors*2, [1,1])\n",
    "        self.rpn_bbox_pred_net = nn.Conv2d(512, self.n_anchors*4, [1,1])\n",
    "\n",
    "        # Classification Network\n",
    "        # [256, 256, 5, 5]\n",
    "        self.cls_score_net = nn.Linear(self._fc_channels, RPN_BATCH_SIZE*NUM_OF_CLASS)     \n",
    "        self.bbox_pred_net = nn.Linear(self._fc_channels, RPN_BATCH_SIZE*NUM_OF_CLASS*4)\n",
    "\n",
    "    def head_net(self):\n",
    "        return nn.Sequential(\n",
    "            self.head_conv1,\n",
    "            self.head_batch_norm1,\n",
    "            self.head_relu1,\n",
    "            self.head_pool1,\n",
    "            self.head_layer1,\n",
    "            self.head_relu2,\n",
    "            self.head_layer2,\n",
    "            self.head_pool2,\n",
    "            self.head_relu3,\n",
    "            self.head_layer3,\n",
    "            self.head_pool3,\n",
    "            self.head_relu4\n",
    "        )\n",
    "\n",
    "    def fc7(self):\n",
    "        return nn.Sequential(\n",
    "            # Current Size: n x 1024 x 7 x 7\n",
    "            # nn.Conv2d(self._predictions[\"rois\"].size(0), self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1024, self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.AvgPool2d([3,3], 1)\n",
    "            # Current Size: n x 4096 x 3 x 3\n",
    "        ) # 256\n",
    "        \n",
    "\n",
    "    def proposal_layer(self, cls_prob, bbox_pred, anchors, n_anchors):\n",
    "        '''\n",
    "        Prunes no. of boxes using NMS based on fg scores and transforms bbox using regression coeff\n",
    "        bbox_pred: BATCH_SIZE * h * w * (num_anchors*4)  \n",
    "        '''\n",
    "        \n",
    "        # Get the scores and bounding boxes\n",
    "        scores = cls_prob[:, :, :, n_anchors:]\n",
    "        rpn_bbox_pred = bbox_pred.view((-1, 4))\n",
    "        scores = scores.contiguous().view(-1, 1)\n",
    "        proposals = bbox_transform_inv(anchors, rpn_bbox_pred) # shift boxes based on prediction\n",
    "        proposals = clip_boxes(proposals, self._im_info[:2])\n",
    "\n",
    "        # NMS Selection\n",
    "        \n",
    "        # Pick the top region proposals\n",
    "        scores, order = scores.view(-1).sort(descending=True)\n",
    "        if PRE_NMS_TOPN > 0:\n",
    "            order = order[:PRE_NMS_TOPN]\n",
    "            scores = scores[:PRE_NMS_TOPN].view(-1, 1)\n",
    "        proposals = proposals[order.data, :]\n",
    "\n",
    "        # Non-maximal suppression\n",
    "        keep = nms(proposals, scores.squeeze(1), NMS_THRESH)\n",
    "\n",
    "        # Pick the top region proposals after NMS\n",
    "        if POST_NMS_TOPN > 0:\n",
    "            keep = keep[:POST_NMS_TOPN]\n",
    "        proposals = proposals[keep, :]\n",
    "        scores = scores[keep, ]\n",
    "\n",
    "        # Only support single image as input\n",
    "        batch_inds = proposals.new_zeros(proposals.size(0), 1)\n",
    "        blob = torch.cat((batch_inds, proposals), 1)\n",
    "        \n",
    "        return blob, scores\n",
    "\n",
    "    def anchor_target_layer(self, rpn_cls_score, gt_boxes, all_anchors, im_info=[320, 320, 1]):\n",
    "        '''\n",
    "        ### Parameters ###\n",
    "        rpn_cls_score: Class scores generated by the Region Proposal Network\n",
    "        gt_boxes: Ground Truth boxes\n",
    "        all_anchors: Anchor boxes generated by the anchor generation layer\n",
    "        \n",
    "        ### Fixed Parameters ###\n",
    "        im_info: Image dimensions\n",
    "        num_anchors: Number of different Anchor boxes used. By default, it is set to 9 here.\n",
    "\n",
    "        ### Additional information ###\n",
    "        POSITIVE_OVERLAP:       Threshold used to select if an anchor box is a good foreground box (Default: 0.7)\n",
    "\n",
    "        NEGATIVE_OVERLAP:       If the max overlap of a anchor from a ground truth box is lower than this thershold, it is marked as background. \n",
    "                                Boxes whose overlap is > than NEGATIVE_OVERLAP but < POSITIVE_OVERLAP are marked \n",
    "                                “don’t care”. (Default: 0.3)\n",
    "        \n",
    "        CLOBBER_POSITIVES:      If a particular anchor is satisfied by both the positive and the negative conditions,\n",
    "                                and if this value is set to False, then set the anchor to a negative example.\n",
    "                                Else, set the anchor to a positive example.\n",
    "        \n",
    "        RPN_BATCH_SIZE:                 Total number of background and foreground anchors. (Default: 256)\n",
    "        \n",
    "        FG_FRACTION:            Fraction of the batch size that is foreground anchors (Default: 0.5). \n",
    "                                If the number of foreground anchors found is larger than RPN_BATCH_SIZE * FG_FRACTION, \n",
    "                                the excess (indices are selected randomly) is marked “don’t care”.\n",
    "                            \n",
    "        RPN_POSITIVE_WEIGHT:    Using this value:\n",
    "                                Positive RPN examples are given a weight of RPN_POSITIVE_WEIGHT * 1 / num_of_positives\n",
    "                                Negative RPN examples are given a weight of (1 - RPN_POSITIVE_WEIGHT)\n",
    "                                Set to -1 by default, which will ensure uniform example weighting.\n",
    "        '''\n",
    "\n",
    "        # Map of shape (..., H, W)\n",
    "        height, width = rpn_cls_score.shape[1:3]\n",
    "\n",
    "        # Only keep anchors that are completely inside the image\n",
    "        inds_inside = np.where(\n",
    "            (all_anchors[:, 0] >= 0) &\n",
    "            (all_anchors[:, 1] >= 0) &\n",
    "            (all_anchors[:, 2] < im_info[1]) &  # width\n",
    "            (all_anchors[:, 3] < im_info[0])  # height\n",
    "        )[0]\n",
    "        anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "        # Label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        labels = np.empty((len(inds_inside), ), dtype=np.float32)\n",
    "        labels.fill(-1)\n",
    "\n",
    "        # BUG: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        # Overlaps between the Anchors and the Ground Truth boxes\n",
    "        overlaps = bbox_overlaps(\n",
    "            np.ascontiguousarray(anchors, dtype=np.float),\n",
    "            np.ascontiguousarray(gt_boxes, dtype=np.float))\n",
    "        argmax_overlaps = overlaps.argmax(axis=1)\n",
    "        max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "        gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
    "        gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n",
    "        gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Positives clobber Negatives\"\n",
    "        if not CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Foreground label: for each Ground Truth box, anchor with highest overlap\n",
    "        labels[gt_argmax_overlaps] = 1\n",
    "\n",
    "        # Foreground label: above threshold IOU\n",
    "        labels[max_overlaps >= POSITIVE_OVERLAP] = 1\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Negatives clobber Positives\"\n",
    "        if CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Subsample positive labels if we have too many\n",
    "        num_fg = int(FG_FRACTION * RPN_BATCH_SIZE)\n",
    "        fg_inds = np.where(labels == 1)[0]\n",
    "        if len(fg_inds) > num_fg:\n",
    "            disable_inds = npr.choice(fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        # Subsample negative labels if we have too many\n",
    "        num_bg = RPN_BATCH_SIZE - np.sum(labels == 1)\n",
    "        bg_inds = np.where(labels == 0)[0]\n",
    "        if len(bg_inds) > num_bg:\n",
    "            disable_inds = npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        labels = torch.from_numpy(labels)\n",
    "                \n",
    "        gt_rois = gt_boxes[argmax_overlaps, :].numpy()\n",
    "        bbox_targets = bbox_transform(anchors, torch.from_numpy(gt_rois_np[:, :4])).numpy()\n",
    "        \n",
    "        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        # Only the positive ones have regression targets\n",
    "        bbox_inside_weights[labels == 1, :] = np.array((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "        labels = labels.numpy()\n",
    "        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        if RPN_POSITIVE_WEIGHT < 0:\n",
    "            # Uniform weighting of examples (given non-uniform sampling)\n",
    "            num_examples = np.sum(labels >= 0)\n",
    "            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "        else:\n",
    "            assert ((RPN_POSITIVE_WEIGHT > 0) &\n",
    "                    (RPN_POSITIVE_WEIGHT < 1))\n",
    "            positive_weights = (\n",
    "                RPN_POSITIVE_WEIGHT / np.sum(labels == 1))\n",
    "            negative_weights = (\n",
    "                (1.0 - RPN_POSITIVE_WEIGHT) / np.sum(labels == 0))\n",
    "        bbox_outside_weights[labels == 1, :] = positive_weights\n",
    "        bbox_outside_weights[labels == 0, :] = negative_weights\n",
    "\n",
    "        # Map up to original set of anchors\n",
    "        total_anchors = all_anchors.shape[0]\n",
    "        labels = unmap(labels, total_anchors, inds_inside, fill=-1)\n",
    "\n",
    "        bbox_targets = unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n",
    "        bbox_inside_weights = unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n",
    "        bbox_outside_weights = unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n",
    "\n",
    "        # Labels\n",
    "        labels = labels.reshape((1, height, width, self.n_anchors)).transpose(0, 3, 1, 2)\n",
    "        labels = labels.reshape((1, 1, self.n_anchors * height, width))\n",
    "        rpn_labels = labels\n",
    "        \n",
    "        # Bounding boxes\n",
    "        bbox_targets = bbox_targets.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_targets = bbox_targets\n",
    "        bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_inside_weights = bbox_inside_weights\n",
    "        bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_outside_weights = bbox_outside_weights\n",
    "\n",
    "        # Re-shape for future use\n",
    "        rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\n",
    "        rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_labels = rpn_labels.long()\n",
    "\n",
    "        # Data storing\n",
    "        self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "        self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "        self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "        self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "\n",
    "        for k in self._anchor_targets.keys():\n",
    "            self._score_summaries[k] = self._anchor_targets[k]\n",
    "\n",
    "        return rpn_labels\n",
    "        \n",
    "    def proposal_target_layer(self, proposed_rois, proposed_roi_scores, gt_boxes):\n",
    "        '''\n",
    "        1. Calculate overlap between ROI and GT boxes\n",
    "        2. Select promising ROIs by comparing against threshold(s)\n",
    "        3. Compute bounding box target regression targets and get bounding box regression labels\n",
    "        '''\n",
    "        # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        # print(\"gt_boxes: \", gt_boxes)\n",
    "        num_images = 1\n",
    "        rois_per_image = RPN_BATCH_SIZE / num_images\n",
    "        # print(\"rois per image\", rois_per_image)\n",
    "        fg_rois_per_image = int(round(FG_FRACTION * rois_per_image))\n",
    "\n",
    "        # Sample rois with classification labels and bounding box regression targets\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "        overlaps = bbox_overlaps(proposed_rois[:, 1:5].data, gt_boxes[:, :4].data)\n",
    "        max_overlaps, gt_assignment = overlaps.max(1)\n",
    "        labels = gt_boxes[gt_assignment, [4]]\n",
    "\n",
    "        # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "        fg_inds = (max_overlaps >= FG_THRESH).nonzero().view(-1)\n",
    "        \n",
    "        # Guard against the case when an image has fewer than fg_rois_per_image\n",
    "        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "        bg_inds = ((max_overlaps < BG_THRESH_HI) + (max_overlaps >= BG_THRESH_LO) == 2).nonzero().view(-1)\n",
    "\n",
    "        # Ensure a fixed number of regions are sampled (optional?)\n",
    "        fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image = fix_sample_regions(fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image)\n",
    "        \n",
    "        # The indices that we're selecting (both fg and bg)\n",
    "        keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "        # Select sampled values from various arrays:\n",
    "        labels = labels[keep_inds].contiguous()\n",
    "\n",
    "        # Clamp labels for the background RoIs to 0\n",
    "        labels[int(fg_rois_per_image):] = 0\n",
    "        rois_final = proposed_rois[keep_inds].contiguous()\n",
    "        roi_scores_final = proposed_roi_scores[keep_inds].contiguous()\n",
    "        \n",
    "        # Compute bounding box target regression targets\n",
    "        ex_rois = rois_final[:, 1:5].data\n",
    "        gt_rois = gt_boxes[gt_assignment[keep_inds]][:, :4].data\n",
    "        targets = bbox_transform(ex_rois, gt_rois)\n",
    "        bbox_target_data = torch.cat([labels.data.unsqueeze(1), targets], 1)\n",
    "        bbox_targets, bbox_inside_weights = get_bbox_regression_labels(bbox_target_data, NUM_OF_CLASS)\n",
    "\n",
    "        # Reshape tensors\n",
    "        rois_final = rois_final.view(-1, 5)\n",
    "        roi_scores_final = roi_scores_final.view(-1)\n",
    "        labels = labels.view(-1, 1)\n",
    "        bbox_targets = bbox_targets.view(-1, NUM_OF_CLASS * 4)\n",
    "        bbox_inside_weights = bbox_inside_weights.view(-1, NUM_OF_CLASS * 4)\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "\n",
    "        self._proposal_targets['rois'] = rois_final\n",
    "        self._proposal_targets['labels'] = labels.long()\n",
    "        self._proposal_targets['bbox_targets'] = bbox_targets\n",
    "        self._proposal_targets['bbox_inside_weights'] = bbox_inside_weights\n",
    "        self._proposal_targets['bbox_outside_weights'] = bbox_outside_weights\n",
    "\n",
    "        return rois_final, roi_scores_final \n",
    "\n",
    "    def region_proposal(self, net_conv, bb, anchors):\n",
    "        \"\"\"\n",
    "        Input: features from head network, bounding boxes, anchors generated\n",
    "        Output: rois\n",
    "        1. Proposal Layer\n",
    "        2. Anchor Target Layer\n",
    "        3. Compute RPN Loss\n",
    "        4. Proposal Target Layer\n",
    "        \n",
    "        Features -> class probabilities of anchors(fg or bg) and bbox coeff of anchors (to adjust them)\n",
    "        \"\"\"\n",
    "                                       \n",
    "        rpn = F.relu(self.rpn_net(net_conv))\n",
    "        rpn_cls_score = self.rpn_cls_score_net(rpn)\n",
    "                                       \n",
    "        rpn_cls_score_reshape = rpn_cls_score.view(1, 2, -1, rpn_cls_score.size()[-1]) # batch * 2 * (num_anchors*h) * w\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, dim=1)\n",
    "\n",
    "        # Move channel to the last dimenstion, to fit the input of python functions\n",
    "        rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score).permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score = rpn_cls_score.permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score_reshape = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous() # batch * (num_anchors*h) * w * 2\n",
    "        rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(-1, 2), 1)[1]\n",
    "\n",
    "        rpn_bbox_pred = self.rpn_bbox_pred_net(rpn)\n",
    "        rpn_bbox_pred = rpn_bbox_pred.permute(0, 2, 3, 1).contiguous()  # batch * h * w * (num_anchors*4)                  \n",
    "\n",
    "        if self.mode == 'TRAIN':\n",
    "            rois, roi_scores = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, n_anchors=self.n_anchors)\n",
    "            rpn_labels = self.anchor_target_layer(rpn_cls_score, gt_boxes=bb, all_anchors=anchors)\n",
    "            rois, _ = self.proposal_target_layer(rois, roi_scores, gt_boxes=bb)\n",
    "        else:\n",
    "            rois, _ = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, n_anchors=self.n_anchors)\n",
    "        \n",
    "        self._predictions[\"rpn_cls_score\"] = rpn_cls_score\n",
    "        self._predictions[\"rpn_cls_score_reshape\"] = rpn_cls_score_reshape\n",
    "        self._predictions[\"rpn_cls_prob\"] = rpn_cls_prob\n",
    "        self._predictions[\"rpn_cls_pred\"] = rpn_cls_pred\n",
    "        self._predictions[\"rpn_bbox_pred\"] = rpn_bbox_pred\n",
    "        self._predictions[\"rois\"] = rois\n",
    "        \n",
    "        return rois\n",
    "    \n",
    "    def roi_align_layer(self, bottom, rois):\n",
    "        return RoIAlign((POOLING_SIZE, POOLING_SIZE), 1.0 / 16.0, 0)(bottom, rois)\n",
    "    \n",
    "    def _smooth_l1_loss(self, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n",
    "        sigma_2 = sigma**2\n",
    "        box_diff = bbox_pred - bbox_targets\n",
    "        in_box_diff = bbox_inside_weights * box_diff\n",
    "        abs_in_box_diff = torch.abs(in_box_diff)\n",
    "        smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
    "        in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
    "                      + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "        out_loss_box = bbox_outside_weights * in_loss_box\n",
    "        loss_box = out_loss_box\n",
    "        for i in sorted(dim, reverse=True):\n",
    "            loss_box = loss_box.sum(i)\n",
    "        loss_box = loss_box.mean()\n",
    "        return loss_box\n",
    "\n",
    "    def region_classification(self, fc7):\n",
    "        \"\"\"\n",
    "        cls_score\n",
    "            Linear layer (fc7 channels, num_classes)\n",
    "            torch max\n",
    "            softmax\n",
    "        bbox_pred\n",
    "            Linear layer (fc7 channels, num_classes*4)\n",
    "        \"\"\"\n",
    "        self.cls_score_net.to(device)\n",
    "        self.bbox_pred_net.to(device)\n",
    "        cls_score = self.cls_score_net(fc7)\n",
    "        cls_score = cls_score.view(-1, NUM_OF_CLASS) # Class scores for each anchor\n",
    "        cls_pred = torch.max(cls_score, 1)[1]\n",
    "        cls_prob = F.softmax(cls_score, dim=1)\n",
    "        bbox_pred = self.bbox_pred_net(fc7)\n",
    "        # print(\"cls score: \", cls_score.shape)\n",
    "        # print(\"bbox pred: \", bbox_pred.shape)\n",
    "        \n",
    "        self._predictions[\"cls_score\"] = cls_score\n",
    "        self._predictions[\"cls_pred\"] = cls_pred\n",
    "        self._predictions[\"cls_prob\"] = cls_prob\n",
    "        self._predictions[\"bbox_pred\"] = bbox_pred\n",
    "\n",
    "        return cls_prob, bbox_pred\n",
    "\n",
    "    def add_losses(self, sigma_rpn=3.0):\n",
    "                                       \n",
    "        # RPN, class loss\n",
    "        rpn_cls_score = self._predictions['rpn_cls_score_reshape'].view(-1, 2).to(device)\n",
    "        rpn_label = self._anchor_targets['rpn_labels'].view(-1).to(device)\n",
    "        rpn_select = (rpn_label.data != -1).nonzero().view(-1)\n",
    "        rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\n",
    "        rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\n",
    "        rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "\n",
    "        # RPN, bbox loss\n",
    "        rpn_bbox_pred = self._predictions['rpn_bbox_pred'].to(device)\n",
    "        rpn_bbox_targets = self._anchor_targets['rpn_bbox_targets'].to(device)\n",
    "        rpn_bbox_inside_weights = self._anchor_targets['rpn_bbox_inside_weights'].to(device)\n",
    "        rpn_bbox_outside_weights = self._anchor_targets['rpn_bbox_outside_weights'].to(device)\n",
    "        rpn_loss_box = self._smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights, sigma=sigma_rpn, dim=[1, 2, 3])\n",
    "\n",
    "        # RCNN, class loss\n",
    "        cls_score = self._predictions[\"cls_score\"].to(device)\n",
    "        # print(\"label: \", self._proposal_targets[\"labels\"].shape)\n",
    "        label = self._proposal_targets[\"labels\"].view(-1).to(device)\n",
    "        # print(\"cls_score: \", cls_score.shape)\n",
    "        # print(\"label: \", label.shape)\n",
    "        cross_entropy = F.cross_entropy(cls_score.view(-1, NUM_OF_CLASS), label)\n",
    "\n",
    "        # RCNN, bbox loss\n",
    "        bbox_pred = self._predictions['bbox_pred'].to(device)\n",
    "        bbox_pred = bbox_pred.view(RPN_BATCH_SIZE, -1)\n",
    "        bbox_targets = self._proposal_targets['bbox_targets'].to(device)\n",
    "        bbox_inside_weights = self._proposal_targets['bbox_inside_weights'].to(device)\n",
    "        bbox_outside_weights = self._proposal_targets['bbox_outside_weights'].to(device)\n",
    "        # print(\"bbox_pred: \", bbox_pred.shape)\n",
    "        # print(\"bbox_targets: \", bbox_targets.shape)\n",
    "\n",
    "        loss_box = self._smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n",
    "\n",
    "        self._losses['cross_entropy'] = cross_entropy\n",
    "        self._losses['loss_box'] = loss_box\n",
    "        self._losses['rpn_cross_entropy'] = rpn_cross_entropy\n",
    "        self._losses['rpn_loss_box'] = rpn_loss_box\n",
    "\n",
    "        loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n",
    "        self._losses['total_loss'] = loss\n",
    "\n",
    "        for k in self._losses.keys():\n",
    "            self._event_summaries[k] = self._losses[k]\n",
    "\n",
    "        return loss                                       \n",
    "\n",
    "    def forward(self, x, bb, im_info=[320, 320, 1], train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mode = \"TRAIN\"\n",
    "        else:\n",
    "            self.mode = \"TEST\"\n",
    "        # Store image information\n",
    "        self._im_info = im_info\n",
    "                                       \n",
    "        # Pass the image through the Backbone ConvNet to generate the series of Feature maps\n",
    "        head_conv_net = self.head_net()\n",
    "        output_head = head_conv_net(x) # current: [1, 1024, 154, 154]\n",
    "        anchors, length = generate_anchors(output_head.size(2), output_head.size(3))\n",
    "        anchors = torch.from_numpy(anchors)\n",
    "        \n",
    "        # print(\"Output_head: \", output_head.shape) # [1,1024,20,20]\n",
    "        rois = self.region_proposal(output_head, bb, anchors)\n",
    "        # print(\"ROIS: \", rois.shape) # [RPN_BATCH_SIZE, 5]\n",
    "        pool5 = self.roi_align_layer(output_head, rois)\n",
    "        # print(\"POOL5\", pool5.shape) # [RPN_BATCH_SIZE, 1024, 7, 7]\n",
    "        fc7 = self.fc7()\n",
    "        fc7.to(device)\n",
    "        fc7_out = fc7(pool5)\n",
    "        # print(\"fc7: \", fc7_out.shape) # [RPN_BATCH_SIZE, RPN_BATCH_SIZE, 5, 5]\n",
    "        fc7_out = fc7_out.view(-1)\n",
    "        # print(\"fc7: \", fc7_out.shape) # [RPN_BATCH_SIZE * 5*5] 6400\n",
    "        n_rois = rois.shape[0]\n",
    "        self.cls_score_net = nn.Linear(self._fc_channels, n_rois*NUM_OF_CLASS)\n",
    "        self.bbox_pred_net = nn.Linear(self._fc_channels, n_rois*NUM_OF_CLASS*4)\n",
    "        cls_prob, bbox_pred = self.region_classification(fc7_out)\n",
    "        # bbox_pred [RPN_BATCH_SIZE, 28]\n",
    "        # bbox_targets [RPN_BATCH_SIZE, 28]\n",
    "        # label [RPN_BATCH_SIZE]\n",
    "        # cls_score [RPN_BATCH_SIZE, 7]\n",
    "        # print(\"bbox_pred: \", bbox_pred)\n",
    "        if self.mode == 'TEST':\n",
    "#             print(\"n_rois: \", n_rois)\n",
    "            # print(\"bbox_pred shape: \", bbox_pred.shape) # [512] -> n_rois * n_class * 4\n",
    "            # stds = bbox_pred.data.new((0.1, 0.1, 0.2, 0.2)).repeat(NUM_OF_CLASS).expand_as(bbox_pred)\n",
    "            # means = bbox_pred.data.new((0.0, 0.0, 0.0, 0.0)).repeat(NUM_OF_CLASS).expand_as(bbox_pred)\n",
    "            # self._predictions[\"bbox_pred\"] = bbox_pred.mul(stds).add(means)\n",
    "            return self._predictions[\"cls_score\"], self._predictions[\"cls_pred\"], self._predictions[\"cls_prob\"], self._predictions[\"bbox_pred\"], self._predictions[\"rois\"]\n",
    "            # return bbox_pred.mul(stds).add(means)\n",
    "        else:\n",
    "            loss = self.add_losses()\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "To train the model, we used an initial learning rate of 0.01 and divided it by 1.5 every 10 epochs. We trained for a total of 20 epochs with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "net = faster_R_CNN()\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "print(utils.display_num_param(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set initial learning rate and Optimizer\n",
    "lr = 0.010\n",
    "EPOCHS = 20\n",
    "\n",
    "# Set training variables\n",
    "running_loss = 0\n",
    "num_batches = 0\n",
    "start = time.time()\n",
    "\n",
    "# Training process\n",
    "for epoch in range(EPOCHS):\n",
    "    # learning rate strategy : divide the learning rate by 1.5 every 10 epochs\n",
    "    if epoch%10==0 and epoch>=10: \n",
    "        lr = lr / 1.5\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        batch_images, batch_bboxes = data[0], data[1]\n",
    "        batch_images = batch_images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = net(batch_images, batch_bboxes)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', lr  ,'\\t loss=', total_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "\n",
    "torch.save(net, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Our trained Faster RCNN model was evaluated based on the mean average precision (mAP) method taught in lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classification(gt_boxes, pred_boxes, cls_pred):\n",
    "    '''\n",
    "    \n",
    "    Calculates the sample classifications for a given set of predicted boxes.\n",
    "        \n",
    "    gt_boxes: n x 5 \n",
    "        n: number of ground truth boxes labelled on the image)\n",
    "    \n",
    "    pred_boxes: n_rois x 8 x 4\n",
    "        n_rois: number of rois generated\n",
    "    \n",
    "    cls_pred: n_rois\n",
    "        (Class prediction for each of the ROIs)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Convert the boxes and predictions into Numpy arrays\n",
    "    gt_boxes = gt_boxes.cpu().numpy()\n",
    "    pred_boxes = pred_boxes.cpu().numpy()\n",
    "    cls_pred = cls_pred.cpu().numpy()\n",
    "    \n",
    "    # Get the labels of the ground truth boxes\n",
    "    gt_lbl = []\n",
    "    for gt_box in gt_boxes:\n",
    "        gt_lbl.append(gt_box[4])\n",
    "    \n",
    "    # tp = true positive, fp = false positive\n",
    "    # tn = true negative, fn = false negative\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for roi_idx in range(pred_boxes.shape[0]):    \n",
    "        # Obtain the class label, between 0.0 ~ 7.0\n",
    "        roi_cls_pred = cls_pred[roi_idx].astype(np.float32)\n",
    "\n",
    "        # Overlaps for positive examples\n",
    "        p_overlaps = []\n",
    "        # Overlaps for negative examples\n",
    "        n_overlaps = []\n",
    "        \n",
    "        for lbl_idx in range(len(gt_lbl)):\n",
    "            \n",
    "            # If the class label exists in the list of ground truth labels, it is a positive\n",
    "            if gt_lbl[lbl_idx] == roi_cls_pred:                \n",
    "                pred_box = torch.unsqueeze(torch.from_numpy(pred_boxes[roi_idx][roi_cls_pred.astype(int)]), 0)\n",
    "                gt_box = torch.unsqueeze(torch.from_numpy(gt_boxes[lbl_idx]), 0)\n",
    "                overlap = bbox_overlaps(pred_box, gt_box)\n",
    "                \n",
    "                # If the overlap is greater than the positive overlap threshold, true positive\n",
    "                if overlap >= POSITIVE_OVERLAP:\n",
    "                    tp = tp + 1\n",
    "                # If not, false positive\n",
    "                else:\n",
    "                    fp = fp + 1\n",
    "            \n",
    "            # If the class label does not exist in the list of ground truth labels, it is a negative\n",
    "            else:\n",
    "                pred_box = torch.unsqueeze(torch.from_numpy(pred_boxes[roi_idx][roi_cls_pred.astype(int)]), 0)\n",
    "                gt_box = torch.unsqueeze(torch.from_numpy(gt_boxes[lbl_idx]), 0)\n",
    "                overlap = bbox_overlaps(pred_box, gt_box)\n",
    "                \n",
    "                # If the overlap is greater than the positive overlap threshold, false negative\n",
    "                if overlap >= POSITIVE_OVERLAP:\n",
    "                    fn = fn + 1\n",
    "                # If not, true negative\n",
    "                else:\n",
    "                    tn = tn + 1\n",
    "  \n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def precision_recall(tp, fp, tn, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return precision, recall\n",
    "    \n",
    "def mean_average_precision(precisions, recalls):\n",
    "    '''\n",
    "    \n",
    "    Calculates the mean Average Precision (mAP) score.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    mAP = np.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=10000)\n",
    "\n",
    "def eval_on_test_set(test_loader):\n",
    "    # net = faster_R_CNN()\n",
    "    # net.to(device)\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    TEST_NMS = 0.3\n",
    "    thresh = 0.12\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    count = 0\n",
    "    \n",
    "    # Classification values\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for data in test_loader:\n",
    "        with torch.no_grad():\n",
    "            batch_images, batch_bboxes = data[0], data[1]\n",
    "            batch_images = batch_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cls_score, cls_pred, cls_prob, bbox_pred, rois = net(batch_images, batch_bboxes, train_flag=False)\n",
    "\n",
    "            boxes = rois[:, 1:5]\n",
    "            batch_bboxes = batch_bboxes[0]\n",
    "            n_anchors = int(bbox_pred.shape[0] / (NUM_OF_CLASS * 4))\n",
    "            scores = cls_prob.view(n_anchors, -1).cpu()\n",
    "            box_deltas = bbox_pred.view(n_anchors, -1)\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas)\n",
    "            pred_boxes = clip_boxes(pred_boxes, [320,320])\n",
    "            boxes = pred_boxes\n",
    "            pred_boxes = pred_boxes.view(-1, NUM_OF_CLASS, 4)\n",
    "\n",
    "            tp, fp, tn, fp = sample_classification(batch_bboxes, pred_boxes, cls_pred)\n",
    "            precision, recall = precision_recall(tp, fp, tn, fp)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            \n",
    "            predictions = {}\n",
    "            # error = utils.get_error( scores , minibatch_label)\n",
    "            # Skip j = 0, because it's the background class\n",
    "            for j in range(1, NUM_OF_CLASS):\n",
    "                inds = np.where(scores[:, j] > thresh)[0]\n",
    "                cls_scores = scores[inds, j].cpu()\n",
    "                cls_boxes = boxes[inds, j * 4:(j + 1) * 4].cpu()\n",
    "                cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n",
    "                .astype(np.float32, copy=False)\n",
    "                keep = nms(\n",
    "                    cls_boxes, cls_scores,\n",
    "                    TEST_NMS).numpy() if cls_dets.size > 0 else []\n",
    "                cls_dets = cls_dets[keep, :]\n",
    "                label = index_label_dict[j]\n",
    "                if label in predictions.keys():\n",
    "                    predictions[label].append(cls_dets)\n",
    "                else:\n",
    "                    predictions[label] = [cls_dets]\n",
    "#             print(\"predictions: \", predictions, '\\n')\n",
    "#             print(\"batch_bbox: \", batch_bboxes, '\\n\\n\\n')\n",
    "            # running_error += error.item()\n",
    "\n",
    "            num_batches+=1\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    mAP = mean_average_precision(precisions, recalls)\n",
    "    print( 'test error  = ', total_error*100 ,'percent\\nMean Average Precision (MAP) Score = ', mAP)\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "net = torch.load(\"model\")\n",
    "net.to(device)\n",
    "eval_on_test_set(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb3f132b8f46ef83ff81b7067b37036dbb49ec9e6fa8b80a12ce6c1c87b906f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
