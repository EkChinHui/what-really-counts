{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What Really Counts: A Cash Recognization System**\n",
    "\n",
    "---\n",
    "\n",
    "**Group Members**:\n",
    "- Ek Chin Hui (A0205578X)\n",
    "- Lee Hyung Woon (A0218475X)\n",
    "- Toh Hai Jie Joey (A0205141Y)\n",
    "\n",
    "---\n",
    "\n",
    "Our project, named **What Really Counts**, is a Cash Recognization System for the Visually Impaired in Singapore. In Singapore, the disabled community face many challenges in their daily lives, and this is especially so for those who are hampered by visual impairments. One such challenge they face is cash payment, as they need to identify the correct combinations of bills and coins. As such, we embarked on a mission to construct a system that can help them overcome these challenges by employing a deep learning-based Object Detection model using Convolutional Neural Networks (CNN) - in particular, the Faster R-CNN model. The model uses CNNs to perform image classification on the objects detected in a given input image, through which we can ascertain the exact number and type of bills / coins present in the image, allowing us to calculate and return the sum of the currency to the user.\n",
    "\n",
    "Here are the steps we took for this project:\n",
    "\n",
    "1. Data Collection\n",
    "2. Data Preparation\n",
    "3. Data Visualisation\n",
    "4. Building the Model\n",
    "5. Training the Model\n",
    "6. Testing the Model\n",
    "7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import nms, RoIAlign\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Collection**\n",
    "\n",
    "In order to build a system that can detect bills and coins, we first needed a dataset of images containing Singapore currency. Hence, our group decided to manually create the dataset by taking pictures of different combinations of bills and coins on different backgrounds. In total, around 200~ images were taken. \n",
    "\n",
    "As an initial proof-of-concept, we decided to focus our model on the 7 most common types of Singapore currency used in our daily lives: \\$10 bills, \\$5 bills, \\$2 bills, \\$1 coins, 50¢ coins, 20¢ coins, and 10¢ coins. These correspond to our class labels as shown below: `('10c', '20c', '50c', '$1', '$2', '$5', '$10')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set class labels\n",
    "coin_labels = ('10c', '20c', '50c', '$1', '$2', '$5', '$10')\n",
    "\n",
    "label_index_dict = {k : v + 1 for v, k in enumerate(coin_labels)}\n",
    "label_index_dict['background'] = 0\n",
    "print(label_index_dict)\n",
    "\n",
    "index_label_dict = {v + 1 : k for v, k in enumerate(coin_labels)}\n",
    "index_label_dict[0] = 'background'\n",
    "print(index_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gathering the data, the images were manually labelled one-by-one using [LabelImg](https://github.com/tzutalin/labelImg), which allowed us to label the bounding boxes and their respective classes easily. LabelImg also had a feature to produce an XML file for each image with the bounding boxes and class information, which made it much more straightforward for us to deal with the data.\n",
    "\n",
    "We then defined the `parse_annotation` function to parse the XML files into an object, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(annotation_path):\n",
    "    '''\n",
    "    Converts the XML data of a single image into an object.\n",
    "\n",
    "    :param annotation_path: The file path of the XML file\n",
    "    :return: An object (Python dictionary) with the bounding box parameters, and the corresponding labels for each bounding box\n",
    "    '''\n",
    "\n",
    "    # Parse the XML file into a tree structure\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    height = float(root.find('size').find('height').text)\n",
    "    width = float(root.find('size').find('width').text)\n",
    "\n",
    "    # Set initial lists\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "\n",
    "    # Loop over each Bbox found in the XML file\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        # Convert Bbox co-ordinates\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert Bbox Label\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_index_dict:\n",
    "            continue\n",
    "        labels.append(label_index_dict[label])\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we defined a function that makes use of the `parse_annotation` function to convert the XML files into JSON objects and store them within local JSON files. Here, each bounding box is stored as [xmin, ymin, xmax, ymax]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_json(*files):\n",
    "    '''\n",
    "    Converts an image (.jpg) and its corresponding XML file (.xml) into two separate local JSON objects.\n",
    "\n",
    "    :param *files: All files in the specified directory, including images and corresponding XML files\n",
    "    '''\n",
    "    \n",
    "    # Initialise lists\n",
    "    images_list = [] \n",
    "    objects_list = []\n",
    "    files = [file for sublist in files for file in sublist]\n",
    "    \n",
    "    # Set up two JSON files to be written\n",
    "    images_file = open(\"TRAIN_images.json\", 'w')\n",
    "    objects_file = open(\"TRAIN_objects.json\", 'w')\n",
    "    \n",
    "    # Iterate through each XML-Image pair\n",
    "    for file in files:\n",
    "    \n",
    "        # Add each image file path into the images list\n",
    "        file_path = os.path.splitext(file)[0]   \n",
    "        images_list.append(file_path + \".jpg\")\n",
    "        \n",
    "        # Add each XML object into the objects list\n",
    "        xml_dict = parse_annotation(file)\n",
    "        objects_list.append(xml_dict)\n",
    "    \n",
    "    # Write each list into the corresponding JSON files\n",
    "    json.dump(images_list, images_file)\n",
    "    json.dump(objects_list, objects_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we converted our dataset from XML to JSON accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH_FILES = glob(r'dataset/ch dataset/*.xml')\n",
    "xml_to_json(CH_FILES)\n",
    "\n",
    "# JY_FILES = glob(r'dataset/joey dataset/*.xml')\n",
    "# HW_FILES = glob(r'dataset/hw dataset/*.xml')\n",
    "# xml_to_json(CH_FILES, HW_FILES)\n",
    "# xml_to_json(HW_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Preparation**\n",
    "\n",
    "With our raw data stored in local JSON files, we needed to further process the images into datasets for training and testing our system. The `DataLoader` and `Dataset` utils in PyTorch were used for organising our dataset.\n",
    "\n",
    "First, we defined the `resize_and_normalise` function to resize and normalise the given images and the bounding boxes accordingly. This function was partially adapted from the [PyTorch Tutorial to Object Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py).\n",
    "\n",
    "We set the default new image dimensions to `(320, 320)`, the standard input size that we used for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_normalise(image, boxes, dims = (320, 320)):\n",
    "    '''\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Resizes the input images and the bounding boxes, and applies Normalisations afterwards.\n",
    "    \n",
    "    :param image: The input image\n",
    "    :param boxes: The coordinates of the bounding boxes in the image\n",
    "    :param dims: The target dimensions of the resize\n",
    "    :return: The transformed image and the transformed bounding box coordinates\n",
    "    '''\n",
    "\n",
    "    # Mean and Standard deviation used for the base VGG from torchvision\n",
    "    # See: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Resize image and convert the image to Torch tensor\n",
    "    new_image = FT.resize(image, dims)\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "    \n",
    "    # Normalize the image by mean and standard deviation\n",
    "    new_image = FT.normalize(new_image, mean = mean, std = std)\n",
    "\n",
    "    # Resize Bounding boxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "    new_boxes = (boxes / old_dims) * new_dims\n",
    "    \n",
    "    return new_image, new_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we declared a `WRCDataset` class which, as the name suggests, is simply a Dataset with the images that we collected (WRC standing for \"What Really Counts\"). This class was used to organise the data, and split them into the training dataset and the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRCDataset(Dataset):\n",
    "    '''\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data_folder):\n",
    "        '''\n",
    "        Initialises the WRCDataset object.\n",
    "\n",
    "        :param data_folder: The data folder path where the data files are stored\n",
    "        '''\n",
    "\n",
    "        # Set the data folder path\n",
    "        self.data_folder = data_folder\n",
    "\n",
    "        # Read from the JSON files (initially craeted from our XML files)\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        # Number of images must match the number of objects containing the Bboxes for each image\n",
    "        assert len(self.images) == len(self.objects)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of data points in the WRCDataset.\n",
    "\n",
    "        :return: The length of the WRCDataset\n",
    "        '''\n",
    "\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        '''\n",
    "        Gets data from the WRCDataset with the given index.\n",
    "\n",
    "        :param i: The index of the data to be returned\n",
    "        :return: The specified data at the given index i\n",
    "        '''\n",
    "\n",
    "        # Read image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensor = transforms.Resize(size = (320, 320))(image_tensor)\n",
    "\n",
    "        # Read objects in this image (Bboxes, labels)\n",
    "        objects = self.objects[i]\n",
    "        box = torch.FloatTensor(objects['boxes'])\n",
    "        label = torch.LongTensor(objects['labels'])\n",
    "\n",
    "        # Apply normalisations and resizes\n",
    "        image, box = resize_and_normalise(image, box)\n",
    "        box_and_label = torch.cat([box, label.unsqueeze(1)], 1)\n",
    "        return image_tensor, box_and_label\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        A collate function to be passed to the DataLoader, describing how to combine tensors of different sizes using Python lists.\n",
    "\n",
    "        :param batch: An iterable set created from the __getitem__() function\n",
    "        :return: A tensor of images and a list of tensor of bounding boxes\n",
    "        '''\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "\n",
    "        images = torch.stack(images, dim = 0)\n",
    "        return images, boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We organised the data using the WRCDataset class, and then split it into the training and testing datasets randomly with a ratio of 80:20. Afterwards, we loaded the training and testing datasets using the DataLoader util from PyTorch, which allowed us to iterate through the dataset with ease. \n",
    "\n",
    "We decided to stick to a batch size of 1 to achieve better training stability and generalization performance, as our dataset was relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WRCDataset(\".\")\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [math.floor(0.8 * len(dataset)), math.ceil(0.2 * len(dataset))])\n",
    "train_dataloader = DataLoader(train_data, batch_size = 1, shuffle = True, collate_fn = dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 1, shuffle = True, collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Visualisation**\n",
    "\n",
    "In order to ensure that the Data has been prepared properly, we used the `matplotlib` library to visualise the data in our dataset, and defined the `visualise_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import math\n",
    "\n",
    "color_maps = ['r', 'g', 'b', 'y', 'm', 'w', 'k', 'c']\n",
    "\n",
    "# Visualise data\n",
    "def visualise_data(dataloader):\n",
    "    '''\n",
    "    Visualises the first image and its corresponding bounding boxes using matplotlib.\n",
    "\n",
    "    :param dataloader: The dataloader containing the dataset to be visualised\n",
    "    '''\n",
    "    \n",
    "    for data in dataloader:\n",
    "        for batch in range(1):\n",
    "            img = data[0][batch]\n",
    "            boxes = data[1][batch]\n",
    "    #         labels = data[2][batch].tolist()\n",
    "    #         named_labels = [index_label_dict[label] for label in labels]\n",
    "            plt.imshow(transforms.ToPILImage()(img))\n",
    "            ax = plt.gca()\n",
    "            labelled = set()\n",
    "            for i, box in enumerate(boxes):\n",
    "                w,h = box[2] - box[0], box[3] - box[1]\n",
    "                x,y = box[0].item(), box[1].item()\n",
    "                x = [x, x + w, x + w, x, x]\n",
    "                y = [y, y, y + h, y + h, y]\n",
    "                label = int(box[4].item())\n",
    "                if label not in labelled:\n",
    "                    plt.plot(x,y, color = color_maps[label], label = index_label_dict[label])\n",
    "                    labelled.add(label)\n",
    "                else:\n",
    "                    plt.plot(x,y, color = color_maps[label])\n",
    "                plt.legend(loc = 'best')\n",
    "            break\n",
    "        break\n",
    "\n",
    "visualise_data(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Building the Model**\n",
    "\n",
    "Before building the actual network, we defined several utility functions that were used at various points throughout our network.\n",
    "\n",
    "First, the `pascal_to_yolo` and the `yolo_to_pascal` functions helped us switch between the two data format as needed: the Pascal VOC data format and the YOLO data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pascal_to_yolo(anchor):\n",
    "    '''\n",
    "    Transforms anchor coordinates of the form [xmin, ymin, xmax, ymax] to [x_center, y_center, width, height].\n",
    "\n",
    "    :param anchor: The anchor to be transformed from Pascal VOC data format to YOLO data format\n",
    "    :return: The anchor in the YOLO data format\n",
    "    '''\n",
    "    \n",
    "    width = anchor[2] - anchor[0] + 1\n",
    "    height = anchor[3] - anchor[1] + 1\n",
    "    x_center = anchor[0] + (width - 1) / 2 \n",
    "    y_center = anchor[1] + (height - 1) / 2\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "def yolo_to_pascal(x_center, y_center, width, height):\n",
    "    '''\n",
    "    Transforms anchor coordinates of the form [x_center, y_center, width, height] to [xmin, ymin, xmax, ymax].\n",
    "\n",
    "    :param x_center: The x_center of the anchor in YOLO data format\n",
    "    :param y_center: The y_center of the anchor in YOLO data format\n",
    "    :param width: The width of the anchor in YOLO data format\n",
    "    :param height: The height of the anchor in YOLO data format\n",
    "    :return: The anchor in the Pascal VOC data format\n",
    "    '''\n",
    "    \n",
    "    width = width[:, np.newaxis]\n",
    "    height = height[:, np.newaxis]\n",
    "    anchors = np.hstack((x_center - 0.5 * (width - 1), y_center - 0.5 * (height - 1), x_center + 0.5 * (width - 1), y_center + 0.5 * (height - 1)))\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `pascal_to_yolo` and the `yolo_to_pascal` functions, we defined the functions for generating the anchors.\n",
    "\n",
    "The base anchor we used was `[0, 0, 15, 15]` in Pascal VOC data format (i.e. xmin = 0, ymin = 0, xmax = 15, ymax = 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ratio_anchors(anchor, ratios = np.array((0.5, 1, 2))):\n",
    "    '''\n",
    "    Generate anchors for each width:height ratio.\n",
    "\n",
    "    :param anchor: The anchors to be resized\n",
    "    :param ratios: The ratios to resize the anchors with\n",
    "    :return: The resized anchors\n",
    "    '''\n",
    "    \n",
    "    # Resize widths and heights\n",
    "    x_center, y_center, width, height = pascal_to_yolo(anchor)\n",
    "    size = width * height\n",
    "    size_ratios = size / ratios\n",
    "    resized_width = np.round(np.sqrt(size_ratios))\n",
    "    resized_height = np.round(resized_width * ratios)\n",
    "    \n",
    "    # Return the resized anchors\n",
    "    anchors = yolo_to_pascal(x_center, y_center, resized_width, resized_height)\n",
    "    return anchors\n",
    "\n",
    "def generate_scale_anchors(anchor, scales = np.array((8, 16, 32))):\n",
    "    '''\n",
    "    Generate anchors for each scale.\n",
    "\n",
    "    :param anchor: The anchors to be rescaled\n",
    "    :param ratios: The scales to rescale the anchors with\n",
    "    :return: The rescaled anchors\n",
    "    '''\n",
    "    \n",
    "    # Rescale widths and heights\n",
    "    x_center, y_center, width, height = pascal_to_yolo(anchor) \n",
    "    scaled_width = width * scales\n",
    "    scaled_height = height * scales\n",
    "\n",
    "    # Return the rescaled anchors\n",
    "    anchors = yolo_to_pascal(x_center, y_center, scaled_width, scaled_height)\n",
    "    return anchors\n",
    "\n",
    "def generate_anchors(height, width, aspect_ratio = np.array((0.5, 1, 2)), scales = np.array((8, 16, 32)), stride_length = 16):\n",
    "    '''\n",
    "    Generate anchors using the scales and the aspect ratios given.\n",
    "\n",
    "    :param height: The height of the filter map to apply the anchors onto\n",
    "    :param width: The width of the filter map to apply the anchors onto\n",
    "    :param aspect_ratio: The aspect ratio for the anchors to be resized according to\n",
    "    :param scales: The scales for the anchors to be rescaled according to\n",
    "    :param stride_length: The stride of the anchors\n",
    "    :return: The generated anchors from resizing and rescaling\n",
    "    '''\n",
    "    \n",
    "    # Resize and Rescale the base anchor\n",
    "    base_anchor = pascal_to_yolo([0, 0, 15, 15])\n",
    "    ratio_anchors = generate_ratio_anchors(base_anchor, ratios=aspect_ratio)\n",
    "    anchors = np.vstack([\n",
    "        generate_scale_anchors(ratio_anchors[i, :], scales)\n",
    "        for i in range(ratio_anchors.shape[0])\n",
    "    ])\n",
    "    anchors_length = anchors.shape[0]\n",
    "    \n",
    "    # Shift each ratio\n",
    "    shift_x = np.arange(0, width) * stride_length\n",
    "    shift_y = np.arange(0, height) * stride_length\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n",
    "    shifts_length = shifts.shape[0]\n",
    "\n",
    "    # Reshape the anchors accordingly\n",
    "    anchors = anchors.reshape((1, anchors_length, 4)) + shifts.reshape((1, shifts_length, 4)).transpose((1, 0, 2))\n",
    "    anchors = anchors.reshape((shifts_length * anchors_length, 4)).astype(np.float32, copy=False)\n",
    "\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the network, there were times when we needed the deltas between the predicted bounding boxes and the ground truth bounding boxes, and hence we also defined two functions to convert between the two: the `transform_bbox_to_delta` and the `transform_delta_to_bbox` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_bbox_to_delta(pred_boxes, gt_boxes):\n",
    "    '''\n",
    "    Transforms a given set of predicted / expected bounding boxes into a set of deltas, using the given ground truth bounding boxes\n",
    "\n",
    "    :param pred_boxes: The predicted bounding boxes / regions of interest\n",
    "    :param gt_boxes: The ground truth bounding boxes / regions of interest\n",
    "    :return: The set of deltas from the predicted bounding boxes to the ground truth bounding boxes\n",
    "    '''\n",
    "    \n",
    "    pred_boxes = pred_boxes.to(device)\n",
    "    gt_boxes = gt_boxes.to(device)\n",
    "    \n",
    "    # Obtain the values for the pred_boxes\n",
    "    pred_widths = pred_boxes[:, 2] - pred_boxes[:, 0] + 1.0\n",
    "    pred_heights = pred_boxes[:, 3] - pred_boxes[:, 1] + 1.0\n",
    "    pred_x_center = pred_boxes[:, 0] + 0.5 * pred_widths\n",
    "    pred_y_center = pred_boxes[:, 1] + 0.5 * pred_heights\n",
    "\n",
    "    # Obtain the values for the gt_boxes\n",
    "    gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0] + 1.0\n",
    "    gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1] + 1.0\n",
    "    gt_x_center = gt_boxes[:, 0] + 0.5 * gt_widths\n",
    "    gt_y_center = gt_boxes[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    # Calculate the values for the deltas\n",
    "    targets_dx = (gt_x_center - pred_x_center) / pred_widths\n",
    "    targets_dy = (gt_y_center - pred_y_center) / pred_heights\n",
    "    targets_dw = torch.log(gt_widths / pred_widths)\n",
    "    targets_dh = torch.log(gt_heights / pred_heights)\n",
    "    targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), 1)\n",
    "    return targets.cpu()\n",
    "\n",
    "def transform_delta_to_bbox(gt_boxes, deltas):\n",
    "    '''\n",
    "    Transforms a given set of deltas and the ground truth bounding boxes into predicted / expected bounding boxes.\n",
    "    Inverse of the transform_bbox_to_delta function.\n",
    "    \n",
    "    :param gt_boxes: The ground truth bounding boxes / regions of interest\n",
    "    :param deltas: The set of deltas generated from the predicted bounding boxes to the ground truth bounding boxes\n",
    "    :return: The predicted bounding boxes / regions of interest\n",
    "    '''\n",
    "\n",
    "    gt_boxes_length = len(gt_boxes)\n",
    "    if gt_boxes_length == 0:\n",
    "        return deltas.detach() * 0\n",
    "    \n",
    "    # Obtain the values for the gt_boxes\n",
    "    gt_boxes = gt_boxes.to(device)\n",
    "    gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0] + 1.0\n",
    "    gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1] + 1.0\n",
    "    gt_x_center = gt_boxes[:, 0] + 0.5 * gt_widths\n",
    "    gt_y_center = gt_boxes[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    # Obtain the values for the deltas\n",
    "    dx = deltas[:, 0::4]\n",
    "    dy = deltas[:, 1::4]\n",
    "    dw = deltas[:, 2::4]\n",
    "    dh = deltas[:, 3::4]\n",
    "\n",
    "    # Calculate the values for the pred_boxes\n",
    "    pred_x_center = dx * gt_widths.unsqueeze(1) + gt_x_center.unsqueeze(1)\n",
    "    pred_y_center = dy * gt_heights.unsqueeze(1) + gt_y_center.unsqueeze(1)\n",
    "    pred_widths = torch.exp(dw) * gt_widths.unsqueeze(1)\n",
    "    pred_heights = torch.exp(dh) * gt_heights.unsqueeze(1)\n",
    "    pred_boxes = torch.cat([_.unsqueeze(2) for _ in [pred_x_center - 0.5 * pred_widths,\n",
    "        pred_y_center - 0.5 * pred_heights,\n",
    "        pred_x_center + 0.5 * pred_widths,\n",
    "        pred_y_center + 0.5 * pred_heights]], 2).view(gt_boxes_length, -1)\n",
    "    return pred_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also defined two useful utility functions: `clip_boxes`, which ensures that any coordinate values of the given bounding boxes that lie outside the image size are clipped, and `bbox_overlaps`, which calculates the overlaps (IoU) between two sets of given bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_boxes(boxes, im_size):\n",
    "    '''\n",
    "    Clip given bounding boxes to image boundaries.\n",
    "\n",
    "    :param boxes: Bounding boxes to be clipped to image boundaries\n",
    "    :param im_size: The size of the image\n",
    "    :return: The clipped bounding boxes\n",
    "    '''\n",
    "\n",
    "    if not hasattr(boxes, 'data'):\n",
    "        boxes = boxes.numpy()\n",
    "\n",
    "    boxes = boxes.view(boxes.size(0), -1, 4)\n",
    "    boxes = torch.stack([boxes[:, :, 0].clamp(0, im_size[1] - 1),\n",
    "        boxes[:, :, 1].clamp(0, im_size[0] - 1),\n",
    "        boxes[:, :, 2].clamp(0, im_size[1] - 1),\n",
    "        boxes[:, :, 3].clamp(0, im_size[0] - 1)], 2).view(boxes.size(0), -1)\n",
    "    return boxes\n",
    "\n",
    "def bbox_overlaps(boxes, query_boxes):\n",
    "    '''\n",
    "    Computes the overlapped area between boxes and query boxes using Intersection-over-Union (IoU).\n",
    "\n",
    "    :param boxes: The first set of boxes to be compared\n",
    "    :param query_boxes: The second set of boxes to be compared\n",
    "    :return: Theoverlap between boxes and query_boxes\n",
    "    '''\n",
    "    \n",
    "    # If input is ndarray, turn the overlaps back to ndarray when return\n",
    "    # Allows for handling of either tensors or numpy ndarrays as inputs\n",
    "    if isinstance(boxes, np.ndarray):\n",
    "        boxes = torch.from_numpy(boxes)\n",
    "        query_boxes = torch.from_numpy(query_boxes)\n",
    "        out_fn = lambda x: x.numpy()  \n",
    "    else:\n",
    "        out_fn = lambda x: x\n",
    "\n",
    "    boxes = boxes.to(device)\n",
    "    query_boxes = query_boxes.to(device)\n",
    "    box_areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    query_areas = (query_boxes[:, 2] - query_boxes[:, 0] + 1) * (query_boxes[:, 3] - query_boxes[:, 1] + 1)\n",
    "\n",
    "    # Calculate the IoU\n",
    "    intersection_width = (torch.min(boxes[:, 2:3], query_boxes[:, 2:3].t()) - torch.max(boxes[:, 0:1], query_boxes[:, 0:1].t()) + 1).clamp(min=0)\n",
    "    intersection_height = (torch.min(boxes[:, 3:4], query_boxes[:, 3:4].t()) - torch.max(boxes[:, 1:2], query_boxes[:, 1:2].t()) + 1).clamp(min=0)\n",
    "    union = box_areas.view(-1, 1) + query_areas.view(1, -1) - intersection_width * intersection_height\n",
    "    overlaps = intersection_width * intersection_height / union\n",
    "    return out_fn(overlaps.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also defined the `unmap` function, which was needed at various points throughout the Anchor Target Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmap(data, count, inds, fill=0):\n",
    "    '''\n",
    "    Unmaps a subset of items back to the original set of items (which has size count).\n",
    "\n",
    "    :param data: The subset of items to be unmapped back to the original set of items\n",
    "    :param count: The size of the original set of items\n",
    "    :param inds: The indexes of the subset of items in the original set of items\n",
    "    :param fill: The value to fill the new additional items with\n",
    "    :return: The unmapped set of data\n",
    "    '''\n",
    "    \n",
    "    if len(data.shape) == 1:\n",
    "        unmapped_data = np.empty((count, ), dtype=np.float32)\n",
    "        unmapped_data.fill(fill)\n",
    "        unmapped_data[inds] = data\n",
    "    else:\n",
    "        unmapped_data = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n",
    "        unmapped_data.fill(fill)\n",
    "        unmapped_data[inds, :] = data\n",
    "    \n",
    "    return unmapped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we needed our own version of the [Smooth L1 Loss](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html) to work with our tensors, and so we defined the `smooth_l1_loss` function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma = 1.0, dim = [1]):\n",
    "    '''\n",
    "    Calculates the Smooth L1 Loss from the given tensors.\n",
    "\n",
    "    :param data: The subset of items to be unmapped back to the original set of items\n",
    "    :param count: The size of the original set of items\n",
    "    :param inds: The indexes of the subset of items in the original set of items\n",
    "    :param fill: The value to fill the new additional items with\n",
    "    :return: The unmapped set of data\n",
    "    '''\n",
    "\n",
    "    sigma_2 = sigma ** 2\n",
    "    box_diff = bbox_pred - bbox_targets\n",
    "    in_box_diff = bbox_inside_weights * box_diff\n",
    "    abs_in_box_diff = torch.abs(in_box_diff)\n",
    "    smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
    "    \n",
    "    in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "    out_loss_box = bbox_outside_weights * in_loss_box\n",
    "    loss_box = out_loss_box\n",
    "    \n",
    "    for i in sorted(dim, reverse=True):\n",
    "        loss_box = loss_box.sum(i)\n",
    "    loss_box = loss_box.mean()\n",
    "    \n",
    "    return loss_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Faster R-CNN Model**\n",
    "\n",
    "First, we set the network constants - fixed values that need to be accessed at various points throughout our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set network constants\n",
    "HIDDEN_DIM = 64\n",
    "NUM_OF_CLASS = 8\n",
    "BATCH_SIZE = 1\n",
    "IM_SIZE = [320, 320, 1]\n",
    "\n",
    "# Thresholds\n",
    "# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n",
    "# Overlap threshold for a ROI to be considered background (class = 0 if overlap in [LO, HI))\n",
    "FG_THRESH = 0.5 \n",
    "BG_THRESH_HI = 0.5\n",
    "BG_THRESH_LO = 0.1\n",
    "FG_FRACTION = 0.5\n",
    "\n",
    "PRE_NMS_TOPN = 12000\n",
    "POST_NMS_TOPN = 2000\n",
    "NMS_THRESH = 0.7\n",
    "\n",
    "POSITIVE_OVERLAP = 0.7\n",
    "NEGATIVE_OVERLAP = 0.3\n",
    "CLOBBER_POSITIVES = False\n",
    "POOLING_SIZE = 7\n",
    "RPN_BATCH_SIZE = 8\n",
    "RPN_POSITIVE_WEIGHT = -1.0\n",
    "\n",
    "BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n",
    "BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n",
    "BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "ANCHOR_SCALES = (8, 16, 32)\n",
    "ANCHOR_RATIOS = (0.5, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow of our faster RCNN model is as follows:\n",
    "\n",
    "### 1. Head Network Layer\n",
    "\n",
    "The Head Network Layer, which serves as the backbone of our network, consists of 4 convolutional layers, with max pooling and RELU activation between each layer. It takes in the input data from the training / testing dataset, and produces convolutional feature maps that will be used in the Anchor Generation Layer and the Region Proposal Network (RPN).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Anchor Generation Layer\n",
    "\n",
    "The Anchor Generation Layer takes in the convolutional feature maps produced by the Head Network Layer, and generates a fixed number of anchors. For our network, we used a base anchor at `[0, 0, 15, 15]` (Pascal VOC format), and used the aspect ratios of (0.5, 1, 2) and scales of (8, 16, 32) in order to generate a total of 9 anchors. Then, we translated these anchors across uniformly spaced grid points spanning the input image, effectively duplicating these anchors multiple times. The generated set of anchors will be passed in the Region Proposal Network (RPN), whose job is to identify and determine which of these anchors are good bounding boxes containing foreground objects.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Region Proposal Network (RPN)\n",
    "\n",
    "The Region Proposal Network (RPN) Layer takes in the convolutional feature maps produced by the Head Network Layer, and runs them through an additional convolutional layer, followed by a RELU activation. Its purpose is to produce the background / foreground class scores and probabilities, as well as corresponding bounding box regression coefficients. \n",
    "\n",
    "The RPN Layer has three sub-layers: the Proposal Layer, the Anchor Target Layer, and the Proposal Target Layer.\n",
    "\n",
    "#### 3.1. Proposal Layer\n",
    "\n",
    "The Proposal Layer takes the anchors generated by the Anchor Generation Layer, and prunes them by applying the Non-Maximum Suppression (NMS) based on foreground scores. After pruning through NMS, the Proposal Layer returns the Region-of-Interest (RoI) Proposals, or RoIs that are more likely to be foreground regions. \n",
    "\n",
    "#### 3.2. Anchor Target Layer\n",
    "\n",
    "The Anchor Target Layer takes the anchors generated by the Anchor Generation Layer, and identifies promising foreground and background anchors.\n",
    "\n",
    "- Promising foreground anchors: If the anchor's overlap with some ground truth box is higher than a threshold, it is a promising foreground anchor.\n",
    "- Promising background anchors: If the anchor's overlap with some ground truth box is lower than a threshold, it is a promising background anchor.\n",
    "\n",
    "Using these selected anchors, a set of bounding box regressors (measure of how far each anchor target is from the closest bounding box) will also be calculated. The selected anchors and the set of bounding box regressors are then used to train the RPN.\n",
    "\n",
    "#### 3.3. Compute RPN Loss\n",
    "\n",
    "The RPN Loss is the loss that needs to be minimized to train the RPN network properly. It is a combination of:\n",
    "\n",
    "- Proportion of bounding boxes produced by the RPN that are correctly classified as either foreground or background\n",
    "- Distance measure between the predicted and target regression coefficients\n",
    "\n",
    "#### 3.4. Proposal Target Layer\n",
    "\n",
    "The Proposal Target Layer takes the RoI Proposals generated by the Proposal Layer, and further prunes them to produce class-specific bounding box regression targets that can be used to train the classification layer.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ROI Pooling Layer\n",
    "\n",
    "The ROI Pooling Layer takes in the bounding box coordinates of the RoI Proposals produced by the Proposal Target layer, and \"crops\" (or apply spatial transformations) the bounding boxes to integer boundaries using the ROI Align interpolation method.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Region Classification Network (RCNN)\n",
    "\n",
    "The Region Classification Network (RCNN) Layer takes in the output feature maps produced by the ROI Pooling Layer, and subsequently passes them through an additional series of convolutional layers.\n",
    "\n",
    "The RCNN Layer has a sub-layer: the Classification Layer.\n",
    "\n",
    "#### 5.1. Classification Layer\n",
    "\n",
    "The Classification Layer consists of a series of layers (a single convolutional layer and two Fully Connected (FC) layers) that produces the class probability distribution for each RoI Proposal, as well as a set of class-specific bounding box regressors.\n",
    "\n",
    "#### 5.2 Compute RCNN Loss\n",
    "\n",
    "The RCNN Loss, similar to the RPN Loss, is the loss that needs to be minimized to train the RCNN network properly. It is a combination of:\n",
    "\n",
    "- Proportion of bounding boxes produced by the RPN that are correctly classified as the correct object class\n",
    "- Distance measure between the predicted and target regression coefficients\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Compute Total Loss\n",
    "\n",
    "The total loss is the sum of the RPN Loss and the RCNN Loss, which is then backpropagated back through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class faster_R_CNN(nn.Module):\n",
    "    '''\n",
    "    The main Faster R-CNN network used for this project.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(faster_R_CNN, self).__init__()\n",
    "        \n",
    "        # Python dictionaries to contain the various values obtained during the training process\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        \n",
    "        # Network constants\n",
    "        self.fc_channels = RPN_BATCH_SIZE * RPN_BATCH_SIZE * 25\n",
    "        self.net_conv_channels = 1024\n",
    "\n",
    "        # Set the number of anchors used for convolutions\n",
    "        self.num_of_anchors = len(ANCHOR_SCALES) * len(ANCHOR_RATIOS)  \n",
    "        \n",
    "        # Head Net: Generating a series of Feature maps from the input image\n",
    "        # Current Size: 3 x h x w\n",
    "        self.head_conv1 = nn.Conv2d(3,  HIDDEN_DIM,  kernel_size = 4, stride = 2, padding = 1)\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_batch_norm1 = nn.BatchNorm2d(HIDDEN_DIM)\n",
    "        # Current Size: 64 x h/2 x w/2 \n",
    "        self.head_relu1 = nn.ReLU()\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_pool1 = nn.MaxPool2d([3, 3], padding = 1, stride = 2)\n",
    "        # Current Size: 64 x h/4 x w/4\n",
    "        self.head_layer1 = nn.Conv2d(HIDDEN_DIM,  HIDDEN_DIM * 4,  kernel_size = 3, padding = 1)\n",
    "        self.head_relu2 = nn.ReLU()\n",
    "        # Current Size: 256 x h/4 x w/4\n",
    "        self.head_layer2 = nn.Conv2d(HIDDEN_DIM * 4,  HIDDEN_DIM * 8,  kernel_size = 3, padding = 1)\n",
    "        self.head_pool2 = nn.MaxPool2d([3, 3], padding = 1, stride = 2)\n",
    "        self.head_relu3 = nn.ReLU()\n",
    "        # Current Size: 512 x h/8 x w/8\n",
    "        self.head_layer3 = nn.Conv2d(HIDDEN_DIM * 8,  HIDDEN_DIM * 16,  kernel_size = 3, padding = 1)\n",
    "        self.head_pool3 = nn.MaxPool2d([3, 3], padding = 1, stride = 2)\n",
    "        self.head_relu4 = nn.ReLU()\n",
    "        # Current Size: 1024 x h/16 x w/16\n",
    "        \n",
    "        # Region Proposal Network\n",
    "        self.rpn_net = nn.Conv2d(self.net_conv_channels, 512 , kernel_size = 3, padding = 1)\n",
    "        self.rpn_cls_score_net = nn.Conv2d(512, self.num_of_anchors * 2, [1, 1])\n",
    "        self.rpn_bbox_pred_net = nn.Conv2d(512, self.num_of_anchors * 4, [1, 1])\n",
    "\n",
    "        # Classification Network\n",
    "        self.cls_score_net = nn.Linear(self.fc_channels, RPN_BATCH_SIZE * NUM_OF_CLASS)     \n",
    "        self.bbox_pred_net = nn.Linear(self.fc_channels, RPN_BATCH_SIZE * NUM_OF_CLASS * 4)\n",
    "\n",
    "    def head_net_layer(self):\n",
    "        '''\n",
    "        1. Head Network Layer\n",
    "        '''\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            self.head_conv1,\n",
    "            self.head_batch_norm1,\n",
    "            self.head_relu1,\n",
    "            self.head_pool1,\n",
    "            self.head_layer1,\n",
    "            self.head_relu2,\n",
    "            self.head_layer2,\n",
    "            self.head_pool2,\n",
    "            self.head_relu3,\n",
    "            self.head_layer3,\n",
    "            self.head_pool3,\n",
    "            self.head_relu4\n",
    "        )\n",
    "    \n",
    "    def anchor_generation_layer(self, head_net_output):\n",
    "        '''\n",
    "        2. Anchor Generation Layer\n",
    "        '''\n",
    "        \n",
    "        return torch.from_numpy(generate_anchors(head_net_output.size(2), head_net_output.size(3)))\n",
    "    \n",
    "    def proposal_layer(self, cls_prob, bbox_pred, anchors, num_of_anchors):\n",
    "        '''\n",
    "        3.1. Proposal Layer\n",
    "        '''\n",
    "        \n",
    "        # Get the scores and bounding boxes\n",
    "        scores = cls_prob[:, :, :, num_of_anchors:]\n",
    "        rpn_bbox_pred = bbox_pred.view((-1, 4))\n",
    "        scores = scores.contiguous().view(-1, 1)\n",
    "        proposals = transform_delta_to_bbox(anchors, rpn_bbox_pred)\n",
    "        proposals = clip_boxes(proposals, IM_SIZE)\n",
    "\n",
    "        # Pick the top region proposals before NMS\n",
    "        scores, order = scores.view(-1).sort(descending=True)\n",
    "        if PRE_NMS_TOPN > 0:\n",
    "            order = order[:PRE_NMS_TOPN]\n",
    "            scores = scores[:PRE_NMS_TOPN].view(-1, 1)\n",
    "        proposals = proposals[order.data, :]\n",
    "\n",
    "        # NMS Selection\n",
    "        keep = nms(proposals, scores.squeeze(1), NMS_THRESH)\n",
    "\n",
    "        # Pick the top region proposals after NMS\n",
    "        if POST_NMS_TOPN > 0:\n",
    "            keep = keep[:POST_NMS_TOPN]\n",
    "        proposals = proposals[keep, :]\n",
    "        scores = scores[keep, ]\n",
    "\n",
    "        # Only support single image as input\n",
    "        batch_inds = proposals.new_zeros(proposals.size(0), 1)\n",
    "        blob = torch.cat((batch_inds, proposals), 1)\n",
    "        \n",
    "        return blob, scores\n",
    "\n",
    "    def anchor_target_layer(self, rpn_cls_score, gt_boxes, all_anchors):\n",
    "        '''\n",
    "        3.2. Anchor Target Layer\n",
    "        '''\n",
    "\n",
    "        # Map of shape (..., H, W)\n",
    "        height, width = rpn_cls_score.shape[1:3]\n",
    "\n",
    "        # Only keep anchors that are completely inside the image\n",
    "        inds_inside = np.where(\n",
    "            (all_anchors[:, 0] >= 0) &\n",
    "            (all_anchors[:, 1] >= 0) &\n",
    "            (all_anchors[:, 2] < IM_SIZE[1]) &  # Width\n",
    "            (all_anchors[:, 3] < IM_SIZE[0])  # Height\n",
    "        )[0]\n",
    "        anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "        # Label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        labels = np.empty((len(inds_inside)), dtype = np.float32)\n",
    "        labels.fill(-1)\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        \n",
    "        # Overlaps between the Anchors and the Ground Truth boxes\n",
    "        overlaps = bbox_overlaps(np.ascontiguousarray(anchors, dtype = np.float), np.ascontiguousarray(gt_boxes, dtype = np.float))\n",
    "        argmax_overlaps = overlaps.argmax(axis = 1)\n",
    "        max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "        gt_argmax_overlaps = overlaps.argmax(axis = 0)\n",
    "        gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n",
    "        gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        if not CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Foreground label: for each Ground Truth box, anchor with highest overlap\n",
    "        labels[gt_argmax_overlaps] = 1\n",
    "\n",
    "        # Foreground label: above threshold IOU\n",
    "        labels[max_overlaps >= POSITIVE_OVERLAP] = 1\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        if CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Subsample positive labels if we have too many\n",
    "        num_of_fg = int(FG_FRACTION * RPN_BATCH_SIZE)\n",
    "        fg_inds = np.where(labels == 1)[0]\n",
    "        if len(fg_inds) > num_of_fg:\n",
    "            disable_inds = npr.choice(fg_inds, size = (len(fg_inds) - num_of_fg), replace = False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        # Subsample negative labels if we have too many\n",
    "        num_of_bg = RPN_BATCH_SIZE - np.sum(labels == 1)\n",
    "        bg_inds = np.where(labels == 0)[0]\n",
    "        if len(bg_inds) > num_of_bg:\n",
    "            disable_inds = npr.choice(bg_inds, size = (len(bg_inds) - num_of_bg), replace = False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        bbox_targets = np.zeros((len(inds_inside), 4), dtype = np.float32)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        gt_rois_np = gt_boxes[argmax_overlaps, :].numpy()\n",
    "        bbox_targets = transform_bbox_to_delta(anchors, torch.from_numpy(gt_rois_np[:, :4])).numpy()\n",
    "        \n",
    "        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype = np.float32)\n",
    "        # Only the positive ones have regression targets\n",
    "        bbox_inside_weights[labels == 1, :] = np.array((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "        labels = labels.numpy()\n",
    "        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype = np.float32)\n",
    "        if RPN_POSITIVE_WEIGHT < 0:\n",
    "            # Uniform weighting of examples (given non-uniform sampling)\n",
    "            num_examples = np.sum(labels >= 0)\n",
    "            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "        else:\n",
    "            positive_weights = (RPN_POSITIVE_WEIGHT / np.sum(labels == 1))\n",
    "            negative_weights = ((1.0 - RPN_POSITIVE_WEIGHT) / np.sum(labels == 0))\n",
    "        bbox_outside_weights[labels == 1, :] = positive_weights\n",
    "        bbox_outside_weights[labels == 0, :] = negative_weights\n",
    "\n",
    "        # Map up to original set of anchors\n",
    "        total_anchors = all_anchors.shape[0]\n",
    "        labels = unmap(labels, total_anchors, inds_inside, fill = -1)\n",
    "\n",
    "        bbox_targets = unmap(bbox_targets, total_anchors, inds_inside, fill = 0)\n",
    "        bbox_inside_weights = unmap(bbox_inside_weights, total_anchors, inds_inside, fill = 0)\n",
    "        bbox_outside_weights = unmap(bbox_outside_weights, total_anchors, inds_inside, fill = 0)\n",
    "\n",
    "        # Labels\n",
    "        labels = labels.reshape((1, height, width, self.num_of_anchors)).transpose(0, 3, 1, 2)\n",
    "        labels = labels.reshape((1, 1, self.num_of_anchors * height, width))\n",
    "        rpn_labels = labels\n",
    "        \n",
    "        # Bounding boxes\n",
    "        bbox_targets = bbox_targets.reshape((1, height, width, self.num_of_anchors * 4))\n",
    "        rpn_bbox_targets = bbox_targets\n",
    "        bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, self.num_of_anchors * 4))\n",
    "        rpn_bbox_inside_weights = bbox_inside_weights\n",
    "        bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, self.num_of_anchors * 4))\n",
    "        rpn_bbox_outside_weights = bbox_outside_weights\n",
    "\n",
    "        # Re-shape for future use\n",
    "        rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\n",
    "        rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_labels = rpn_labels.long()\n",
    "\n",
    "        # Data storing\n",
    "        self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "        self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "        self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "        self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "\n",
    "        return rpn_labels\n",
    "    \n",
    "    def compute_rpn_loss(self):\n",
    "        '''\n",
    "        3.3. Compute RPN Loss\n",
    "        '''\n",
    "        \n",
    "        # RPN, class loss\n",
    "        rpn_cls_score = self._predictions['rpn_cls_score_reshape'].view(-1, 2).to(device)\n",
    "        rpn_label = self._anchor_targets['rpn_labels'].view(-1).to(device)\n",
    "        rpn_select = (rpn_label.data != -1).nonzero().view(-1)\n",
    "        rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\n",
    "        rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\n",
    "        rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "\n",
    "        # RPN, bbox loss\n",
    "        rpn_bbox_pred = self._predictions['rpn_bbox_pred'].to(device)\n",
    "        rpn_bbox_targets = self._anchor_targets['rpn_bbox_targets'].to(device)\n",
    "        rpn_bbox_inside_weights = self._anchor_targets['rpn_bbox_inside_weights'].to(device)\n",
    "        rpn_bbox_outside_weights = self._anchor_targets['rpn_bbox_outside_weights'].to(device)\n",
    "        rpn_loss_box = smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights, sigma = 3.0, dim = [1, 2, 3])\n",
    "\n",
    "        self._losses['rpn_cross_entropy'] = rpn_cross_entropy\n",
    "        self._losses['rpn_loss_box'] = rpn_loss_box\n",
    "        \n",
    "        return rpn_cross_entropy + rpn_loss_box\n",
    "        \n",
    "    def proposal_target_layer(self, proposed_rois, proposed_roi_scores, gt_boxes):\n",
    "        '''\n",
    "        3.4. Proposal Target Layer\n",
    "        '''\n",
    "        \n",
    "        # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        num_images = 1\n",
    "        rois_per_image = RPN_BATCH_SIZE / num_images\n",
    "        fg_rois_per_image = int(round(FG_FRACTION * rois_per_image))\n",
    "\n",
    "        # Sample rois with classification labels and bounding box regression targets\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "        overlaps = bbox_overlaps(proposed_rois[:, 1:5].data, gt_boxes[:, :4].data)\n",
    "        max_overlaps, gt_assignment = overlaps.max(1)\n",
    "        labels = gt_boxes[gt_assignment, [4]]\n",
    "\n",
    "        # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "        fg_inds = (max_overlaps >= FG_THRESH).nonzero().view(-1)\n",
    "        \n",
    "        # Guard against the case when an image has fewer than fg_rois_per_image\n",
    "        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "        bg_inds = ((max_overlaps < BG_THRESH_HI) + (max_overlaps >= BG_THRESH_LO) == 2).nonzero().view(-1)\n",
    "\n",
    "        # Ensure total number of foreground and background ROIs is constant by randomly repeating background indices to make up for lesser foreground indices\n",
    "        # This ensures that a fixed number of regions are sampled\n",
    "        if fg_inds.numel() == 0 and bg_inds.numel() == 0:\n",
    "            to_replace = proposed_rois.size(0) < rois_per_image\n",
    "            bg_inds = torch.from_numpy(npr.choice(np.arange(0, proposed_rois.size(0)), size=int(rois_per_image), replace=to_replace)).long()\n",
    "            fg_rois_per_image = 0\n",
    "        elif fg_inds.numel() > 0 and bg_inds.numel() > 0:\n",
    "            fg_rois_per_image = min(fg_rois_per_image, fg_inds.numel())\n",
    "            fg_inds = fg_inds[torch.from_numpy(\n",
    "                npr.choice(\n",
    "                    np.arange(0, fg_inds.numel()),\n",
    "                    size = int(fg_rois_per_image),\n",
    "                    replace = False)).long().to(gt_boxes.device)]\n",
    "            bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "            to_replace = bg_inds.numel() < bg_rois_per_image\n",
    "            bg_inds = bg_inds[torch.from_numpy(\n",
    "                npr.choice(\n",
    "                    np.arange(0, bg_inds.numel()),\n",
    "                    size = int(bg_rois_per_image),\n",
    "                    replace = to_replace)).long().to(gt_boxes.device)]\n",
    "        elif fg_inds.numel() > 0:\n",
    "            to_replace = fg_inds.numel() < rois_per_image\n",
    "            fg_inds = fg_inds[torch.from_numpy(\n",
    "                npr.choice(\n",
    "                    np.arange(0, fg_inds.numel()),\n",
    "                    size = int(rois_per_image),\n",
    "                    replace = to_replace)).long().to(gt_boxes.device)]\n",
    "            fg_rois_per_image = rois_per_image\n",
    "        elif bg_inds.numel() > 0:\n",
    "            to_replace = bg_inds.numel() < rois_per_image\n",
    "            bg_inds = bg_inds[torch.from_numpy(\n",
    "                npr.choice(\n",
    "                    np.arange(0, bg_inds.numel()),\n",
    "                    size = int(rois_per_image),\n",
    "                    replace = to_replace)).long().to(gt_boxes.device)]\n",
    "            fg_rois_per_image = 0\n",
    "        \n",
    "        # The indices that we're selecting (both fg and bg)\n",
    "        keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "        # Select sampled values from various arrays:\n",
    "        labels = labels[keep_inds].contiguous()\n",
    "\n",
    "        # Clamp labels for the background RoIs to 0\n",
    "        labels[int(fg_rois_per_image):] = 0\n",
    "        rois_final = proposed_rois[keep_inds].contiguous()\n",
    "        roi_scores_final = proposed_roi_scores[keep_inds].contiguous()\n",
    "        \n",
    "        # Compute bounding box target regression targets\n",
    "        ex_rois = rois_final[:, 1:5].data\n",
    "        gt_rois = gt_boxes[gt_assignment[keep_inds]][:, :4].data\n",
    "        targets = transform_bbox_to_delta(ex_rois, gt_rois)\n",
    "        bbox_target_data = torch.cat([labels.data.unsqueeze(1), targets], 1)\n",
    "\n",
    "        # Bounding-box regression targets (bbox_target_data) are stored in a compact form N x (class, tx, ty, tw, th)\n",
    "        # This expands those targets into the 4-of-4*K representation used by the network (i.e. only one class has non-zero targets).\n",
    "        clss = bbox_target_data[:, 0]\n",
    "        bbox_targets = clss.new_zeros(clss.numel(), 4 * NUM_OF_CLASS)\n",
    "        bbox_inside_weights = clss.new_zeros(bbox_targets.shape)\n",
    "        inds = (clss > 0).nonzero().view(-1)\n",
    "        if inds.numel() > 0:\n",
    "            clss = clss[inds].contiguous().view(-1, 1)\n",
    "            dim1_inds = inds.unsqueeze(1).expand(inds.size(0), 4)\n",
    "            dim2_inds = torch.cat([4 * clss, 4 * clss + 1, 4 * clss + 2, 4 * clss + 3], 1).long()\n",
    "            test = bbox_target_data[inds][:, 1:]\n",
    "            bbox_targets[dim1_inds, dim2_inds] = test\n",
    "            bbox_inside_weights[dim1_inds, dim2_inds] = bbox_targets.new(BBOX_INSIDE_WEIGHTS).view(-1, 4).expand_as(dim1_inds)\n",
    "\n",
    "        # Reshape tensors\n",
    "        rois_final = rois_final.view(-1, 5)\n",
    "        roi_scores_final = roi_scores_final.view(-1)\n",
    "        labels = labels.view(-1, 1)\n",
    "        bbox_targets = bbox_targets.view(-1, NUM_OF_CLASS * 4)\n",
    "        bbox_inside_weights = bbox_inside_weights.view(-1, NUM_OF_CLASS * 4)\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "\n",
    "        self._proposal_targets['rois'] = rois_final\n",
    "        self._proposal_targets['labels'] = labels.long()\n",
    "        self._proposal_targets['bbox_targets'] = bbox_targets\n",
    "        self._proposal_targets['bbox_inside_weights'] = bbox_inside_weights\n",
    "        self._proposal_targets['bbox_outside_weights'] = bbox_outside_weights\n",
    "\n",
    "        return rois_final, roi_scores_final\n",
    "\n",
    "    def region_proposal(self, net_conv, bboxes, anchors):\n",
    "        '''\n",
    "        3. Region Proposal Network (RPN)\n",
    "        '''\n",
    "                                       \n",
    "        rpn = F.relu(self.rpn_net(net_conv))\n",
    "        rpn_cls_score = self.rpn_cls_score_net(rpn)\n",
    "                                       \n",
    "        rpn_cls_score_reshape = rpn_cls_score.view(1, 2, -1, rpn_cls_score.size()[-1]) # batch * 2 * (num_of_anchors * h) * w\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, dim = 1)\n",
    "\n",
    "        # Move channel to the last dimension, to fit the input of python functions\n",
    "        rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score).permute(0, 2, 3, 1) # batch * h * w * (num_of_anchors * 2)\n",
    "        rpn_cls_score = rpn_cls_score.permute(0, 2, 3, 1) # batch * h * w * (num_of_anchors * 2)\n",
    "        rpn_cls_score_reshape = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous() # batch * (num_of_anchors * h) * w * 2\n",
    "        rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(-1, 2), 1)[1]\n",
    "\n",
    "        rpn_bbox_pred = self.rpn_bbox_pred_net(rpn)\n",
    "        rpn_bbox_pred = rpn_bbox_pred.permute(0, 2, 3, 1).contiguous()  # batch * h * w * (num_of_anchors * 4)                  \n",
    "\n",
    "        if self.mode == 'TRAIN':\n",
    "            rois, roi_scores = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors = anchors, num_of_anchors = self.num_of_anchors)\n",
    "            rpn_labels = self.anchor_target_layer(rpn_cls_score, gt_boxes = bboxes, all_anchors = anchors)\n",
    "            rois, _ = self.proposal_target_layer(rois, roi_scores, gt_boxes = bboxes)\n",
    "        else:\n",
    "            rois, _ = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors = anchors, num_of_anchors = self.num_of_anchors)\n",
    "        \n",
    "        self._predictions[\"rpn_cls_score\"] = rpn_cls_score\n",
    "        self._predictions[\"rpn_cls_score_reshape\"] = rpn_cls_score_reshape\n",
    "        self._predictions[\"rpn_cls_prob\"] = rpn_cls_prob\n",
    "        self._predictions[\"rpn_cls_pred\"] = rpn_cls_pred\n",
    "        self._predictions[\"rpn_bbox_pred\"] = rpn_bbox_pred\n",
    "        self._predictions[\"rois\"] = rois\n",
    "        \n",
    "        return rois\n",
    "\n",
    "    def roi_align_layer(self, bottom, rois):\n",
    "        '''\n",
    "        4. ROI Pooling Layer\n",
    "        '''\n",
    "        \n",
    "        return RoIAlign((POOLING_SIZE, POOLING_SIZE), 1.0 / 16.0, 0)(bottom, rois)\n",
    "    \n",
    "    def region_classification(self, fc7):\n",
    "        '''\n",
    "        5. Region Classification Network (RCNN)\n",
    "        '''\n",
    "        \n",
    "        self.cls_score_net.to(device)\n",
    "        self.bbox_pred_net.to(device)\n",
    "        cls_score = self.cls_score_net(fc7)\n",
    "        cls_score = cls_score.view(-1, NUM_OF_CLASS)\n",
    "        cls_pred = torch.max(cls_score, 1)[1]\n",
    "        cls_prob = F.softmax(cls_score, dim = 1)\n",
    "        bbox_pred = self.bbox_pred_net(fc7)\n",
    "        \n",
    "        self._predictions[\"cls_score\"] = cls_score\n",
    "        self._predictions[\"cls_pred\"] = cls_pred\n",
    "        self._predictions[\"cls_prob\"] = cls_prob\n",
    "        self._predictions[\"bbox_pred\"] = bbox_pred\n",
    "\n",
    "        return cls_prob, bbox_pred\n",
    "    \n",
    "    def compute_rcnn_loss(self):\n",
    "        '''\n",
    "        5.2. Compute RCNN Loss\n",
    "        '''\n",
    "        \n",
    "        # RCNN, class loss\n",
    "        cls_score = self._predictions[\"cls_score\"].to(device)\n",
    "        label = self._proposal_targets[\"labels\"].view(-1).to(device)\n",
    "        cross_entropy = F.cross_entropy(cls_score.view(-1, NUM_OF_CLASS), label)\n",
    "\n",
    "        # RCNN, bbox loss\n",
    "        bbox_pred = self._predictions['bbox_pred'].to(device)\n",
    "        bbox_targets = self._proposal_targets['bbox_targets'].to(device)\n",
    "        bbox_inside_weights = self._proposal_targets['bbox_inside_weights'].to(device)\n",
    "        bbox_outside_weights = self._proposal_targets['bbox_outside_weights'].to(device)\n",
    "        bbox_pred = bbox_pred.view(RPN_BATCH_SIZE, -1)\n",
    "        loss_box = smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n",
    "\n",
    "        self._losses['cross_entropy'] = cross_entropy\n",
    "        self._losses['loss_box'] = loss_box\n",
    "        \n",
    "        return cross_entropy + loss_box\n",
    "    \n",
    "    def compute_total_loss(self):\n",
    "        '''\n",
    "        6. Compute Total Loss\n",
    "        '''\n",
    "        \n",
    "        rpn_loss = self.compute_rpn_loss()\n",
    "        rcnn_loss = self.compute_rcnn_loss()\n",
    "        total_loss = rpn_loss + rcnn_loss\n",
    "        self._losses['total_loss'] = total_loss\n",
    "        return total_loss\n",
    "\n",
    "    def fc7(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(self.net_conv_channels, self._predictions[\"rois\"].size(0),  kernel_size = 3, padding = 1),\n",
    "            nn.AvgPool2d([3, 3], 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, bboxes, train_flag = True):\n",
    "        if train_flag:\n",
    "            self.mode = \"TRAIN\"\n",
    "        else:\n",
    "            self.mode = \"TEST\"\n",
    "                                       \n",
    "        # Pass the image through the Backbone ConvNet to generate the series of Feature maps\n",
    "        head_net = self.head_net_layer()\n",
    "        output = head_net(x)\n",
    "        anchors = self.anchor_generation_layer(output)\n",
    "        rois = self.region_proposal(output, bboxes, anchors) # [RPN_BATCH_SIZE, 5]\n",
    "        pool5 = self.roi_align_layer(output, rois)\n",
    "        \n",
    "        fc7 = self.fc7()\n",
    "        fc7.to(device)\n",
    "        fc7_out = fc7(pool5) # [RPN_BATCH_SIZE, RPN_BATCH_SIZE, 5, 5]\n",
    "        fc7_out = fc7_out.view(-1) # [RPN_BATCH_SIZE * 5 * 5]\n",
    "\n",
    "        n_rois = rois.shape[0]\n",
    "        self.cls_score_net = nn.Linear(self.fc_channels, n_rois * NUM_OF_CLASS)\n",
    "        self.bbox_pred_net = nn.Linear(self.fc_channels, n_rois * NUM_OF_CLASS * 4)\n",
    "        cls_prob, bbox_pred = self.region_classification(fc7_out)\n",
    "\n",
    "        if self.mode == 'TEST':\n",
    "            # stds = bbox_pred.data.new((0.1, 0.1, 0.2, 0.2)).repeat(NUM_OF_CLASS).expand_as(bbox_pred)\n",
    "            # means = bbox_pred.data.new((0.0, 0.0, 0.0, 0.0)).repeat(NUM_OF_CLASS).expand_as(bbox_pred)\n",
    "            # self._predictions[\"bbox_pred\"] = bbox_pred.mul(stds).add(means)\n",
    "            return self._predictions[\"cls_score\"], self._predictions[\"cls_pred\"], self._predictions[\"cls_prob\"], self._predictions[\"bbox_pred\"], self._predictions[\"rois\"]\n",
    "        else:\n",
    "            loss = self.compute_total_loss()\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Training the Model**\n",
    "To train the model, we used an initial learning rate of 0.01 and divided it by 1.5 every 10 epochs. We trained for a total of 20 epochs with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "net = faster_R_CNN()\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "print(utils.display_num_param(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set initial learning rate and Optimizer\n",
    "lr = 0.005\n",
    "EPOCHS = 20\n",
    "\n",
    "# Set training variables\n",
    "running_loss = 0\n",
    "num_batches = 0\n",
    "start = time.time()\n",
    "\n",
    "# Training process\n",
    "for epoch in range(EPOCHS):\n",
    "    # learning rate strategy : divide the learning rate by 1.5 every 10 epochs\n",
    "    if epoch%10==0 and epoch>=10: \n",
    "        lr = lr / 1.5\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        batch_images, batch_bboxes = data[0], data[1]\n",
    "        batch_images = batch_images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = net(batch_images, batch_bboxes)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', lr  ,'\\t loss=', total_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "\n",
    "torch.save(net, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Testing the Model**\n",
    "Our trained Faster RCNN model was evaluated based on the mean average precision (mAP) method taught in lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classification(gt_boxes, pred_boxes, POSITIVE_OVERLAP):\n",
    "    '''\n",
    "    Calculates the sample classifications for a given set of predicted boxes.\n",
    "        \n",
    "    gt_boxes: n x 5 \n",
    "        n: number of ground truth boxes labelled on the image)\n",
    "    \n",
    "    pred_boxes: n_rois x 5\n",
    "        n_rois: number of rois filtered through NMS    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Convert the boxes and predictions into Numpy arrays\n",
    "    gt_boxes = gt_boxes.cpu().numpy()\n",
    "    pred_boxes = pred_boxes.cpu().numpy()\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for pred_box in pred_boxes:\n",
    "        \n",
    "        # Obtain the class label for the current predicted box, between 0.0 ~ 7.0\n",
    "        roi_cls_pred = pred_box[4].astype(np.int32)\n",
    "        \n",
    "        for gt_box in gt_boxes:\n",
    "            \n",
    "            pred_box_no_label = torch.unsqueeze(torch.from_numpy(pred_box[:4]), 0)\n",
    "            gt_box_no_label = torch.unsqueeze(torch.from_numpy(gt_box[:4]), 0)\n",
    "            overlap = bbox_overlaps(pred_box_no_label, gt_box_no_label)\n",
    "            \n",
    "            # If the class label exists in the list of ground truth labels, it is a positive\n",
    "            if gt_box[4].astype(np.int32) == roi_cls_pred:                \n",
    "                \n",
    "                # If the overlap is greater than the positive overlap threshold, true positive\n",
    "                if overlap >= POSITIVE_OVERLAP:\n",
    "                    tp += 1\n",
    "                # If not, false positive\n",
    "                else:\n",
    "                    fp += 1\n",
    "            \n",
    "            # If the class label does not exist in the list of ground truth labels, it is a negative\n",
    "            else:\n",
    "                \n",
    "                # If the overlap is greater than the positive overlap threshold, false negative\n",
    "                if overlap >= POSITIVE_OVERLAP:\n",
    "                    fn += 1\n",
    "                # If not, true negative\n",
    "                else:\n",
    "                    tn += 1\n",
    "    \n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def precision_recall(tp, fp, tn, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return precision, recall\n",
    "    \n",
    "def mean_average_precision(precisions, recalls):\n",
    "    '''\n",
    "    Calculates the mean Average Precision (mAP) score.\n",
    "    '''\n",
    "    \n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    mAP = np.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\n",
    "    return mAP\n",
    "\n",
    "def filter_predictions(scores, pred_boxes, TEST_NMS_THRESH, PROB_THRESH):\n",
    "    \n",
    "    predictions = {index_label_dict[i]: [] for i in range(NUM_OF_CLASS)}\n",
    "    filtered_pred_boxes = torch.Tensor([])\n",
    "\n",
    "    for j in range(1, NUM_OF_CLASS):\n",
    "        \n",
    "        # Get the label for the current iteration\n",
    "        label = index_label_dict[j]\n",
    "        \n",
    "        # Get the co-ordinates corresponding to the labels only\n",
    "        inds = np.where(scores[:, j] > PROB_THRESH)[0]\n",
    "        cls_scores = scores[inds, j].cpu() # n x 1\n",
    "        cls_boxes = pred_boxes[inds, j * 4:(j + 1) * 4].cpu() # n x 4\n",
    "        cls_labels = torch.Tensor([j]).repeat(cls_boxes.size()[0], 1) # n x 1 (n-times repeated tensor of j)\n",
    "        cls_pred_boxes = torch.cat((cls_boxes, cls_labels), 1) # n x 5\n",
    "        cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32, copy=False)        \n",
    "\n",
    "        # Use NMS to filter predicted boxes through the NMS Threshold\n",
    "        keep = nms(cls_boxes, cls_scores, TEST_NMS_THRESH).numpy() if cls_dets.size > 0 else []\n",
    "\n",
    "        # Update predictions\n",
    "        cls_dets = cls_dets[keep, :]\n",
    "        predictions[label] = cls_dets\n",
    "        \n",
    "        # Update filtered predicted boxes\n",
    "        cls_pred_boxes = cls_pred_boxes[keep, :]\n",
    "        filtered_pred_boxes = torch.cat((filtered_pred_boxes, cls_pred_boxes), 0)\n",
    "        \n",
    "    return predictions, filtered_pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set(test_loader):\n",
    "    \n",
    "    TEST_NMS_THRESH = 0.3\n",
    "    PROB_THRESH = 0.12\n",
    "    TEST_POSITIVE_OVERLAP = 0.2\n",
    "\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    \n",
    "    # Classification values\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    tp_total = 0\n",
    "    fp_total = 0\n",
    "    tn_total = 0\n",
    "    fn_total = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        with torch.no_grad():\n",
    "            batch_images, batch_bboxes = data[0], data[1]\n",
    "            batch_images = batch_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cls_score, cls_pred, cls_prob, bbox_pred, rois = net(batch_images, batch_bboxes, train_flag=False)\n",
    "\n",
    "            boxes = rois[:, 1:5]\n",
    "            batch_bboxes = batch_bboxes[0]\n",
    "            num_of_anchors = int(bbox_pred.shape[0] / (NUM_OF_CLASS * 4))\n",
    "            scores = cls_prob.view(num_of_anchors, -1).cpu()\n",
    "            box_deltas = bbox_pred.view(num_of_anchors, -1)\n",
    "            pred_boxes = transform_delta_to_bbox(boxes, box_deltas)\n",
    "            pred_boxes = clip_boxes(pred_boxes, IM_SIZE)\n",
    "                        \n",
    "            # Filter the Ground truth boxes and Predicted boxes\n",
    "            _, filtered_pred_boxes = filter_predictions(scores, pred_boxes, TEST_NMS_THRESH, PROB_THRESH)\n",
    "            \n",
    "            # Calculate the mAP Score\n",
    "            tp, fp, tn, fn = sample_classification(batch_bboxes, filtered_pred_boxes, TEST_POSITIVE_OVERLAP)\n",
    "            tp_total = tp_total + tp\n",
    "            fp_total = fp_total + fp\n",
    "            tn_total = tn_total + tn\n",
    "            fn_total = fn_total + fn\n",
    "            precision, recall = precision_recall(tp, fp, tn, fn)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            \n",
    "            num_batches+=1\n",
    "            print(\"TP:\", tp_total, \"\\nFP:\", fp_total, \"\\nTN:\", tn_total, \"\\nFP:\", fn_total, \"\\n\")\n",
    "            \n",
    "    total_error = running_error / num_batches\n",
    "    mAP = mean_average_precision(precisions, recalls)\n",
    "    print( 'test error  = ', total_error*100 ,'percent\\nMean Average Precision (MAP) Score = ', mAP)\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "net = torch.load(\"model\")\n",
    "net.to(device)\n",
    "eval_on_test_set(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def display_img_bbox(img, gt_bboxes, predictions=()):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    \n",
    "    plt.imshow(transforms.ToPILImage()(img))\n",
    "    gt_bbox_color = 'g'\n",
    "    pred_color = 'r'\n",
    "    fs = 10\n",
    "    # ax = plt.gca()\n",
    "    # labelled = set()\n",
    "\n",
    "    # plot gt_boxes\n",
    "    for bbox in gt_bboxes:\n",
    "        w,h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        x,y = bbox[0], bbox[1]\n",
    "        x = [x, x + w, x + w, x, x]\n",
    "        y = [y, y, y + h, y + h, y]\n",
    "        label_idx = bbox[4].item()\n",
    "        label = index_label_dict[label_idx]\n",
    "        plt.text(x[0], y[0], label, fontsize=fs)\n",
    "        plt.plot(x,y, color = gt_bbox_color, label = label)\n",
    "    \n",
    "    # plot predicted boxes\n",
    "    for bbox in predictions:\n",
    "        w,h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        x,y = bbox[0], bbox[1]\n",
    "        x = [x, x + w, x + w, x, x]\n",
    "        y = [y, y, y + h, y + h, y]\n",
    "        label = index_label_dict[bbox[4]]\n",
    "        plt.text(x[0], y[0], label, fontsize=fs)\n",
    "        plt.plot(x,y, color = pred_color, label = label)\n",
    "\n",
    "def display_predictions(img, gt_bbox, max_per_image=5):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        cls_score, cls_pred, cls_prob, bbox_pred, rois = net(img, None, train_flag=False)\n",
    "        boxes = rois[:, 1:5]\n",
    "        num_of_anchors = int(bbox_pred.shape[0] / (NUM_OF_CLASS * 4))\n",
    "        scores = cls_prob.view(num_of_anchors, -1)\n",
    "        box_deltas = bbox_pred.view(num_of_anchors, -1)\n",
    "        boxes = transform_delta_to_bbox(boxes, box_deltas)\n",
    "        boxes = clip_boxes(boxes, IM_SIZE)\n",
    "        predictions, _ = filter_predictions(scores.cpu(), boxes.cpu(), 0.3, 0.129)\n",
    "        img = img.view(-1, IM_SIZE[1], IM_SIZE[0])\n",
    "        predictions = {label: bboxes[:1] for label, bboxes in predictions.items()}\n",
    "        predicted_boxes = []\n",
    "        for label, bboxes in predictions.items():\n",
    "            for bbox in bboxes:\n",
    "                bbox_list = bbox.tolist()\n",
    "                bbox_list.insert(4, label_index_dict[label])\n",
    "                predicted_boxes.append(bbox_list)\n",
    "        top_n_predicted_boxes = sorted(predicted_boxes, key=lambda x: x[4])[:max_per_image]\n",
    "        display_img_bbox(img, gt_bbox[0], top_n_predicted_boxes)\n",
    "\n",
    "net = torch.load('model')\n",
    "device = torch.device('cuda')\n",
    "net.to(device)\n",
    "img, bbox = iter(test_dataloader).next()\n",
    "display_predictions(img, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb3f132b8f46ef83ff81b7067b37036dbb49ec9e6fa8b80a12ce6c1c87b906f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
