{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Really Counts: A Cash Recognization System\n",
    "\n",
    "---\n",
    "\n",
    "**Group Members**:\n",
    "- Ek Chin Hui ()\n",
    "- Lee Hyung Woon (A0218475X)\n",
    "- Toh Hai Jie Joey (A0205141Y)\n",
    "\n",
    "---\n",
    "\n",
    "Our project, named **What Really Counts**, is a Cash Recognization System for the Visually Impaired in Singapore. In Singapore, the disabled community face many challenges in their daily lives, and this is especially so for those who are hampered by visual impairments. One such challenge they face is cash payment, as they need to identify the correct combinations of bills and coins. Hence, our aim was to contruct a system that can help them overcome these challenges by employing a deep learning-based Object Detection model using Convolutional Neural Networks (CNN) - in particular, the Faster R-CNN model. \n",
    "\n",
    "**What Really Counts** is an architecture that detects and analyzes given images of Singapore Currencies (bills and/or coins), and is primarily designed to assist the visually impaired in identifying the correct combinations of bills and coins. The model uses CNNs to perform image classification on the objects detected in a given input image, through which we can ascertain the exact number and type of bills / coins present in the image, allowing us to calculate and return the sum of the currency to the user.\n",
    "\n",
    "For this project, we will gather and pre-process our own dataset, and then move onto training and testing of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used\n",
    "\n",
    "The following are modules that we used for this project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection & Preprocessing\n",
    "\n",
    "For this project, we collected data by taking pictures...\n",
    "\n",
    "As an initial proof of concept, we decided to focus on the 7 most common classes of currencies in Singapore: $10, $5, $2, $1, 50c, 20c, 10c...\n",
    "\n",
    "After we gathered the data, we did data cleaning, augmentation, labelling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10c': 1, '20c': 2, '50c': 3, '$1': 4, '$2': 5, '$5': 6, '$10': 7, 'background': 0}\n",
      "{1: '10c', 2: '20c', 3: '50c', 4: '$1', 5: '$2', 6: '$5', 7: '$10', 0: 'background'}\n"
     ]
    }
   ],
   "source": [
    "# Set class labels\n",
    "\n",
    "coin_labels = ('10c', '20c', '50c', '$1', '$2', '$5', '$10')\n",
    "\n",
    "label_index_dict = {k:v+1 for v, k in enumerate(coin_labels)}\n",
    "label_index_dict['background'] = 0\n",
    "print(label_index_dict)\n",
    "\n",
    "index_label_dict = {v+1:k for v, k in enumerate(coin_labels)}\n",
    "index_label_dict[0] = 'background'\n",
    "print(index_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<<<<< local\n",
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"\n",
    "    Function to convert XML data of a single image into an object.\n",
    "    The object contains Bbox parameters and the corresponding label for each Bbox.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the XML file into a tree structure\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    height = float(root.find('size').find('height').text)\n",
    "    width = float(root.find('size').find('width').text)\n",
    "\n",
    "    # Set initial lists\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    # difficulties = list()\n",
    "\n",
    "    # Loop over each Bbox found in the XML file\n",
    "    for object in root.iter('object'):\n",
    "        # Convert Bbox co-ordinates\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "        # xmin = float(bbox.find('xmin').text)/width\n",
    "        # ymin = float(bbox.find('ymin').text)/height\n",
    "        # xmax = float(bbox.find('xmax').text)/width\n",
    "        # ymax = float(bbox.find('ymax').text)/height\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert Bbox Label\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_index_dict:\n",
    "            continue\n",
    "        labels.append(label_index_dict[label])\n",
    "\n",
    "        # Convert Bbox Difficulty\n",
    "        # difficult = int(object.find('difficult').text == '1')\n",
    "        # difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "def xml_to_json(*files):\n",
    "    \"\"\"\n",
    "    Function to convert image and XML data into two separate JSON objects.\n",
    "    One object for images, and another object for Bbox co-ordinate values and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise lists\n",
    "    images_list = [] \n",
    "    objects_list = []\n",
    "    files = [file for sublist in files for file in sublist]\n",
    "    # print(files)\n",
    "    # Set up two JSON files to be written\n",
    "    images_file = open(\"TRAIN_images.json\", 'w')\n",
    "    objects_file = open(\"TRAIN_objects.json\", 'w')\n",
    "    \n",
    "    # Iterate through each XML-Image pair\n",
    "    for file in files:\n",
    "    \n",
    "        # Add each image file path into the images list\n",
    "        file_path = os.path.splitext(file)[0]   \n",
    "        images_list.append(file_path + \".jpg\")\n",
    "        \n",
    "        # Add each XML object into the objects list\n",
    "        xml_dict = parse_annotation(file)\n",
    "        objects_list.append(xml_dict)\n",
    "    \n",
    "    # Write each list into the corresponding JSON files\n",
    "    json.dump(images_list, images_file)\n",
    "    json.dump(objects_list, objects_file)\n",
    "\n",
    "CH_FILES = glob(r'dataset/ch dataset/*.xml')\n",
    "JY_FILES = glob(r'dataset/joey dataset/*.xml')\n",
    "HW_FILES = glob(r'dataset/hw dataset/*.xml')\n",
    "# xml_to_json(CH_FILES, HW_FILES)\n",
    "xml_to_json(CH_FILES, JY_FILES)\n",
    "# xml_to_json(HW_FILES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, boxes, dims = (320, 320), return_percent_coords = False):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Resize image. For the SSD300, resize to (300, 300).\n",
    "    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n",
    "    you may choose to retain them.\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resize image\n",
    "    new_image = FT.resize(image, dims)\n",
    "\n",
    "    # Resize Bboxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims  # percent coordinates\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, boxes, labels):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Apply the transformations above.\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
    "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
    "    \"\"\"\n",
    "\n",
    "    # Mean and Standard deviation used for the base VGG from torchvision\n",
    "    # See: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "\n",
    "    # Resize image - this also converts absolute boundary coordinates to their fractional form\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims = (320, 320))\n",
    "\n",
    "    # Convert PIL image to Torch tensor\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "\n",
    "    # Normalize by mean and standard deviation of ImageNet data that the base torchvision VGG was trained on\n",
    "    new_image = FT.normalize(new_image, mean = mean, std = std)\n",
    "\n",
    "    return new_image, new_boxes, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader.\n",
    "    This class is primarily used to create batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_folder, split):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param split: split, one of 'TRAIN' or 'TEST'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {'TRAIN', 'TEST'}\n",
    "\n",
    "        # Read from the JSON files\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        # Number of images must match the number of objects containing the Bboxes for each image\n",
    "        assert len(self.images) == len(self.objects)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # Read image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensor = transforms.Resize(size = (320, 320))(image_tensor)\n",
    "\n",
    "        # Read objects in this image (Bboxes, labels)\n",
    "        objects = self.objects[i]\n",
    "        box = torch.FloatTensor(objects['boxes']) # (n_objects, 4xy values and 1 )\n",
    "        label = torch.LongTensor(objects['labels']) # (n_objects)\n",
    "\n",
    "        # Apply transformations\n",
    "        image, box, label = transform(image, box, label)\n",
    "        box_and_label = torch.cat([box, label.unsqueeze(1)], 1)\n",
    "        return image_tensor, box_and_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "\n",
    "        images = torch.stack(images, dim = 0)\n",
    "        return images, boxes  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import math\n",
    "\n",
    "color_maps = ['r', 'g', 'b', 'y', 'm', 'w', 'k', 'c']\n",
    "bs = 1\n",
    "dataset = PascalVOCDataset(\".\", \"TRAIN\")\n",
    "length = len(dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [math.floor(0.8*length), math.ceil(0.2*length)])\n",
    "train_dataloader = DataLoader(train_data, batch_size = bs, shuffle = True, collate_fn = dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size = bs, shuffle = True, collate_fn = dataset.collate_fn)\n",
    "\n",
    "\n",
    "# for data in train_dataloader:\n",
    "#     print(data)\n",
    "#     break\n",
    "# TODO: Currently, the object itself is being resized. \n",
    "# Need to instead use the resize() method to resize the bounding boxes instead?\n",
    "# This should ideally be done in the Faster R-CNN network itself\n",
    "# We'll need to ROI-Pool both the Bounding box Coordinates and the ROI itself\n",
    "\n",
    "# Visualise data\n",
    "# for data in train_dataloader:\n",
    "#     count = 1\n",
    "#     # if count == 1:\n",
    "#     #     print(data[0])\n",
    "#     #     print(data[0].size())\n",
    "#     #     print(data[0].type())\n",
    "#     count = count + 1\n",
    "#     for batch in range(bs):\n",
    "#         img = data[0][batch]\n",
    "#         boxes = data[1][batch]\n",
    "# #         labels = data[2][batch].tolist()\n",
    "# #         named_labels = [index_label_dict[label] for label in labels]\n",
    "#         plt.imshow(transforms.ToPILImage()(img))\n",
    "#         ax = plt.gca()\n",
    "#         labelled = set()\n",
    "#         for i, box in enumerate(boxes):\n",
    "#             w,h = box[2] - box[0], box[3] - box[1]\n",
    "#             x,y = box[0].item(), box[1].item()\n",
    "#             x = [x, x + w, x + w, x, x]\n",
    "#             y = [y, y, y + h, y + h, y]\n",
    "#             label = int(box[4].item())\n",
    "#             if label not in labelled:\n",
    "#                 plt.plot(x,y, color = color_maps[label], label = index_label_dict[label])\n",
    "#                 labelled.add(label)\n",
    "#             else:\n",
    "#                 plt.plot(x,y, color = color_maps[label])\n",
    "#             plt.legend(loc = 'best')\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n",
    "BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n",
    "BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "def pascal2yolo(anchor):\n",
    "    \"\"\"\n",
    "    Transforms anchor coordinates of the form [xmin, ymin, xmax, ymax] to [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # TODO: +1? for width and height\n",
    "    width = anchor[2] - anchor[0] + 1\n",
    "    height = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + (width-1)/2 \n",
    "    y_ctr = anchor[1] + (height-1)/2\n",
    "    return width, height, x_ctr, y_ctr\n",
    "\n",
    "def yolo2pascal(width, height, x_ctr, y_ctr):\n",
    "    \"\"\"\n",
    "    Transforms anchor coordinates of the form [x_center, y_center, width, height] to [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    width = width[:, np.newaxis]\n",
    "    height = height[:, np.newaxis]\n",
    "    anchors = np.hstack((x_ctr - 0.5 * (width - 1), y_ctr - 0.5 * (height - 1),\n",
    "                         x_ctr + 0.5 * (width - 1), y_ctr + 0.5 * (height - 1)))\n",
    "    return anchors\n",
    "\n",
    "def generate_ratio_anchors(anchor, ratios=(0.5,1,2)):\n",
    "    \"\"\"\n",
    "    Generate anchors based on given width:height ratio\n",
    "    \"\"\"\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor)\n",
    "    size = w*h\n",
    "    size_ratios = size / ratios\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    hs = np.round(ws * ratios)\n",
    "    anchors = yolo2pascal(ws, hs, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_scale_anchors(anchor, scales=np.array((8,16,32))):\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor) \n",
    "    scaled_w = w * scales\n",
    "    scaled_h = h * scales\n",
    "    anchors = yolo2pascal(scaled_w, scaled_h, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_anchors(height, width, aspect_ratio=np.array((0.5,1,2)), stride_length=16, scales=np.array((8,16,32))):\n",
    "    # Generate anchors of differing scale and aspect ratios\n",
    "    # return anchors and anchor length\n",
    "    base_anchor = pascal2yolo([0,0,15,15]) # 16, 16, 7.5, 7.5\n",
    "    ratio_anchors = generate_ratio_anchors(base_anchor, ratios=aspect_ratio)\n",
    "    anchors = np.vstack([\n",
    "        generate_scale_anchors(ratio_anchors[i, :], scales)\n",
    "        for i in range(ratio_anchors.shape[0])\n",
    "    ])\n",
    "    A = anchors.shape[0]\n",
    "    shift_x = np.arange(0, width) * stride_length\n",
    "    shift_y = np.arange(0, height) * stride_length\n",
    "    # Shift each ratio and \n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n",
    "                        shift_y.ravel())).transpose()\n",
    "    K = shifts.shape[0]\n",
    "    # width changes faster, so here it is H, W, C\n",
    "    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n",
    "    length = np.int32(anchors.shape[0])\n",
    "\n",
    "    return anchors, length\n",
    "\n",
    "def bbox_overlaps(boxes, query_boxes):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: (N, 4) ndarray or tensor or variable\n",
    "    query_boxes: (K, 4) ndarray or tensor or variable\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, np.ndarray):\n",
    "        boxes = torch.from_numpy(boxes)\n",
    "        query_boxes = torch.from_numpy(query_boxes)\n",
    "        out_fn = lambda x: x.numpy()  # If input is ndarray, turn the overlaps back to ndarray when return\n",
    "    else:\n",
    "        out_fn = lambda x: x\n",
    "\n",
    "    boxes = boxes.to(device)\n",
    "    query_boxes = query_boxes.to(device)\n",
    "    box_areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    query_areas = (query_boxes[:, 2] - query_boxes[:, 0] + 1) * (query_boxes[:, 3] - query_boxes[:, 1] + 1)\n",
    "\n",
    "    iw = (torch.min(boxes[:, 2:3], query_boxes[:, 2:3].t()) - torch.max(boxes[:, 0:1], query_boxes[:, 0:1].t()) + 1).clamp(min=0)\n",
    "    ih = (torch.min(boxes[:, 3:4], query_boxes[:, 3:4].t()) - torch.max(boxes[:, 1:2], query_boxes[:, 1:2].t()) + 1).clamp(min=0)\n",
    "    ua = box_areas.view(-1, 1) + query_areas.view(1, -1) - iw * ih\n",
    "    overlaps = iw * ih / ua\n",
    "    return out_fn(overlaps.cpu())\n",
    "\n",
    "def bbox_transform(ex_rois, gt_rois):\n",
    "    ex_rois = ex_rois.to(device)\n",
    "    gt_rois = gt_rois.to(device)\n",
    "    \n",
    "    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n",
    "    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n",
    "\n",
    "    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n",
    "    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "    targets_dw = torch.log(gt_widths / ex_widths)\n",
    "    targets_dh = torch.log(gt_heights / ex_heights)\n",
    "\n",
    "    targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), 1)\n",
    "    return targets.cpu()\n",
    "\n",
    "def compute_targets_atl(ex_rois, gt_rois):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "    \n",
    "    gt_rois_np = gt_rois.numpy()\n",
    "    targets = bbox_transform(ex_rois, torch.from_numpy(gt_rois_np[:, :4])).numpy()\n",
    "    return targets\n",
    "\n",
    "def compute_targets_ptl(ex_rois, gt_rois, labels):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    targets = bbox_transform(ex_rois, gt_rois)\n",
    "    # Normalize targets by a precomputed mean and stdev (optional)\n",
    "    # targets = ((targets - targets.new(BBOX_NORMALIZE_MEANS)) / targets.new(BBOX_NORMALIZE_STDS))\n",
    "    return torch.cat([labels.unsqueeze(1), targets], 1)\n",
    "\n",
    "def _get_bbox_regression_labels(bbox_target_data, num_classes):\n",
    "    \"\"\"\n",
    "    Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "    compact form N x (class, tx, ty, tw, th)\n",
    "    This function expands those targets into the 4-of-4*K representation used\n",
    "    by the network (i.e. only one class has non-zero targets).\n",
    "    Returns:\n",
    "        bbox_target (ndarray): N x 4K blob of regression targets\n",
    "        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n",
    "    \"\"\"\n",
    "    # Inputs are tensor\n",
    "\n",
    "    clss = bbox_target_data[:, 0]\n",
    "    bbox_targets = clss.new_zeros(clss.numel(), 4 * num_classes)\n",
    "    bbox_inside_weights = clss.new_zeros(bbox_targets.shape)\n",
    "    inds = (clss > 0).nonzero().view(-1)\n",
    "    if inds.numel() > 0:\n",
    "        clss = clss[inds].contiguous().view(-1, 1)\n",
    "        dim1_inds = inds.unsqueeze(1).expand(inds.size(0), 4)\n",
    "        dim2_inds = torch.cat([4 * clss, 4 * clss + 1, 4 * clss + 2, 4 * clss + 3], 1).long()\n",
    "        test = bbox_target_data[inds][:, 1:]\n",
    "        bbox_targets[dim1_inds, dim2_inds] = test\n",
    "        bbox_inside_weights[dim1_inds, dim2_inds] = bbox_targets.new(BBOX_INSIDE_WEIGHTS).view(-1, 4).expand_as(dim1_inds)\n",
    "    # return None\n",
    "    return bbox_targets, bbox_inside_weights\n",
    "\n",
    "def bbox_transform_inv(boxes, deltas):\n",
    "    # Input should be both tensor or both Variable and on the same device\n",
    "    if len(boxes) == 0:\n",
    "        return deltas.detach() * 0\n",
    "    boxes = boxes.to(device)\n",
    "    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "    dx = deltas[:, 0::4]\n",
    "    dy = deltas[:, 1::4]\n",
    "    dw = deltas[:, 2::4]\n",
    "    dh = deltas[:, 3::4]\n",
    "\n",
    "    pred_ctr_x = dx * widths.unsqueeze(1) + ctr_x.unsqueeze(1)\n",
    "    pred_ctr_y = dy * heights.unsqueeze(1) + ctr_y.unsqueeze(1)\n",
    "    pred_w = torch.exp(dw) * widths.unsqueeze(1)\n",
    "    pred_h = torch.exp(dh) * heights.unsqueeze(1)\n",
    "\n",
    "    pred_boxes = torch.cat(\\\n",
    "      [_.unsqueeze(2) for _ in [pred_ctr_x - 0.5 * pred_w,\\\n",
    "                                pred_ctr_y - 0.5 * pred_h,\\\n",
    "                                pred_ctr_x + 0.5 * pred_w,\\\n",
    "                                pred_ctr_y + 0.5 * pred_h]], 2).view(len(boxes), -1)\n",
    "\n",
    "    return pred_boxes\n",
    "\n",
    "def clip_boxes(boxes, im_shape):\n",
    "    \"\"\"\n",
    "  Clip boxes to image boundaries.\n",
    "  boxes must be tensor or Variable, im_shape can be anything but Variable\n",
    "  \"\"\"\n",
    "\n",
    "    if not hasattr(boxes, 'data'):\n",
    "        boxes_ = boxes.numpy()\n",
    "\n",
    "    boxes = boxes.view(boxes.size(0), -1, 4)\n",
    "    boxes = torch.stack(\\\n",
    "      [boxes[:,:,0].clamp(0, im_shape[1] - 1),\n",
    "       boxes[:,:,1].clamp(0, im_shape[0] - 1),\n",
    "       boxes[:,:,2].clamp(0, im_shape[1] - 1),\n",
    "       boxes[:,:,3].clamp(0, im_shape[0] - 1)], 2).view(boxes.size(0), -1)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def clip_boxes_batch(boxes, im_shape, batch_size):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    \"\"\"\n",
    "    num_rois = boxes.size(1)\n",
    "\n",
    "    boxes[boxes < 0] = 0\n",
    "    # batch_x = (im_shape[:,0]-1).view(batch_size, 1).expand(batch_size, num_rois)\n",
    "    # batch_y = (im_shape[:,1]-1).view(batch_size, 1).expand(batch_size, num_rois)\n",
    "\n",
    "    batch_x = im_shape[:, 1] - 1\n",
    "    batch_y = im_shape[:, 0] - 1\n",
    "\n",
    "    boxes[:,:,0][boxes[:,:,0] > batch_x] = batch_x\n",
    "    boxes[:,:,1][boxes[:,:,1] > batch_y] = batch_y\n",
    "    boxes[:,:,2][boxes[:,:,2] > batch_x] = batch_x\n",
    "    boxes[:,:,3][boxes[:,:,3] > batch_y] = batch_y\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def fix_sample_regions(fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image):\n",
    "    # Small modification to the original version where we ensure a fixed number of regions are sampled\n",
    "    if fg_inds.numel() == 0 and bg_inds.numel() == 0:\n",
    "        to_replace = all_rois.size(0) < rois_per_image\n",
    "        bg_inds = torch.from_numpy(npr.choice(np.arange(0, all_rois.size(0)), size=int(rois_per_image), replace=to_replace)).long()\n",
    "        fg_rois_per_image = 0\n",
    "    elif fg_inds.numel() > 0 and bg_inds.numel() > 0:\n",
    "        fg_rois_per_image = min(fg_rois_per_image, fg_inds.numel())\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(fg_rois_per_image),\n",
    "                replace=False)).long().to(gt_boxes.device)]\n",
    "        bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "        to_replace = bg_inds.numel() < bg_rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(bg_rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "    elif fg_inds.numel() > 0:\n",
    "        to_replace = fg_inds.numel() < rois_per_image\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = rois_per_image\n",
    "    elif bg_inds.numel() > 0:\n",
    "        to_replace = bg_inds.numel() < rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = 0\n",
    "\n",
    "    return fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image\n",
    "\n",
    "def unmap(data, count, inds, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of\n",
    "  size count) \"\"\"\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count, ), dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds] = data\n",
    "    else:\n",
    "        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms, RoIAlign\n",
    "\n",
    "hidden_dim = 64\n",
    "num_of_class = 8\n",
    "\n",
    "im_size = 320\n",
    "bs = 1\n",
    "\n",
    "# Thresholds\n",
    "FG_THRESH = 0.5 # Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n",
    "# Overlap threshold for a ROI to be considered background (class = 0 if overlap in [LO, HI))\n",
    "BG_THRESH_HI = 0.5\n",
    "BG_THRESH_LO = 0.1\n",
    "\n",
    "PRE_NMS_TOPN = 12000\n",
    "POST_NMS_TOPN = 2000\n",
    "NMS_THRESH = 0.7\n",
    "\n",
    "POSITIVE_OVERLAP = 0.7\n",
    "NEGATIVE_OVERLAP = 0.3\n",
    "CLOBBER_POSITIVES = False\n",
    "RPN_BS = 8\n",
    "FG_FRACTION = 0.5\n",
    "RPN_POSITIVE_WEIGHT = -1.0\n",
    "POOLING_SIZE = 7\n",
    "\n",
    "class faster_R_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The main Faster R-CNN network used for this project.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(faster_R_CNN, self).__init__()\n",
    "        self.feat_stride = [16,]\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        self._layers = {}\n",
    "        self._act_summaries = {}\n",
    "        self._score_summaries = {}\n",
    "        self._event_summaries = {}\n",
    "        self._image_gt_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "        self._fc_channels = RPN_BS*RPN_BS*25\n",
    "        self._net_conv_channels = 1024\n",
    "\n",
    "        # This results in num_anchors = 9\n",
    "        anchor_scales = (8, 16, 32)\n",
    "        anchor_ratios = (0.5, 1, 2)\n",
    "        self.n_anchors = len(anchor_scales) * len(anchor_ratios)  \n",
    "        \n",
    "        # HeadNet: Generating a series of Feature maps from the input image\n",
    "        \n",
    "        # Current Size: 3 x h x w\n",
    "        self.head_conv1 = nn.Conv2d(3,  hidden_dim,  kernel_size=4, stride=2, padding=1)\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_batch_norm1 = nn.BatchNorm2d(hidden_dim)\n",
    "        # Current Size: 64 x h/2 x w/2 \n",
    "        self.head_relu1 = nn.ReLU()\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_pool1 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        # Current Size: 64 x h/4 x w/4\n",
    "        self.head_layer1 = nn.Conv2d(hidden_dim,  hidden_dim*4,  kernel_size=3, padding=1)\n",
    "        self.head_relu2 = nn.ReLU()\n",
    "        # Current Size: 256 x h/4 x w/4\n",
    "        self.head_layer2 = nn.Conv2d(hidden_dim*4,  hidden_dim*8,  kernel_size=3, padding=1)\n",
    "        self.head_pool2 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        self.head_relu3 = nn.ReLU()\n",
    "        # Current Size: 512 x h/8 x w/8\n",
    "        self.head_layer3 = nn.Conv2d(hidden_dim*8,  hidden_dim*16,  kernel_size=3, padding=1)\n",
    "        self.head_pool3 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        self.head_relu4 = nn.ReLU()\n",
    "        # Current Size: 1024 x h/16 x w/16\n",
    "        \n",
    "        # Region Proposal Network\n",
    "        self.rpn_net = nn.Conv2d(self._net_conv_channels, 512 , kernel_size=3, padding=1)\n",
    "        self.rpn_cls_score_net = nn.Conv2d(512, self.n_anchors*2, [1,1])\n",
    "        self.rpn_bbox_pred_net = nn.Conv2d(512, self.n_anchors*4, [1,1])\n",
    "\n",
    "        # Classification Network\n",
    "        # [256, 256, 5, 5]\n",
    "        self.cls_score_net = nn.Linear(self._fc_channels, RPN_BS*num_of_class)     \n",
    "        self.bbox_pred_net = nn.Linear(self._fc_channels, RPN_BS*num_of_class*4)\n",
    "\n",
    "    def head_net(self):\n",
    "        return nn.Sequential(\n",
    "            self.head_conv1,\n",
    "            self.head_batch_norm1,\n",
    "            self.head_relu1,\n",
    "            self.head_pool1,\n",
    "            self.head_layer1,\n",
    "            self.head_relu2,\n",
    "            self.head_layer2,\n",
    "            self.head_pool2,\n",
    "            self.head_relu3,\n",
    "            self.head_layer3,\n",
    "            self.head_pool3,\n",
    "            self.head_relu4\n",
    "        )\n",
    "\n",
    "    def fc7(self):\n",
    "        return nn.Sequential(\n",
    "            # Current Size: n x 1024 x 7 x 7\n",
    "            # nn.Conv2d(self._predictions[\"rois\"].size(0), self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1024, self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.AvgPool2d([3,3], 1)\n",
    "            # Current Size: n x 4096 x 3 x 3\n",
    "        ) # 256\n",
    "        \n",
    "\n",
    "    def proposal_layer(self, cls_prob, bbox_pred, anchors, n_anchors):\n",
    "        '''\n",
    "        Prunes no. of boxes using NMS based on fg scores and transforms bbox using regression coeff\n",
    "        bbox_pred: bs * h * w * (num_anchors*4)  \n",
    "        '''\n",
    "        \n",
    "        # Get the scores and bounding boxes\n",
    "        scores = cls_prob[:, :, :, n_anchors:]\n",
    "        rpn_bbox_pred = bbox_pred.view((-1, 4))\n",
    "        # rpn_bbox_pred = bbox_pred.view(bs, -1, 4) # for batch\n",
    "        scores = scores.contiguous().view(-1, 1)\n",
    "        # scores = scores.reshape(bs, -1)  # for batch\n",
    "        proposals = bbox_transform_inv(anchors, rpn_bbox_pred) # shift boxes based on prediction\n",
    "\n",
    "        # TODO: Anchors need to have 3 dimensions, currently only have 2 (Error: too many indices for tensor of dimension 2)\n",
    "        # proposals = bbox_transform_inv_batch(anchors, rpn_bbox_pred, bs) # for batch\n",
    "        proposals = clip_boxes(proposals, self._im_info[:2])\n",
    "        # proposals = clip_boxes_batch(proposals, self._im_info[:2], bs) # for batch\n",
    "\n",
    "        # NMS Selection, should include in final\n",
    "        \n",
    "        # Pick the top region proposals\n",
    "        scores, order = scores.view(-1).sort(descending=True)\n",
    "        if PRE_NMS_TOPN > 0:\n",
    "            order = order[:PRE_NMS_TOPN]\n",
    "            scores = scores[:PRE_NMS_TOPN].view(-1, 1)\n",
    "        proposals = proposals[order.data, :]\n",
    "\n",
    "        # Non-maximal suppression\n",
    "        keep = nms(proposals, scores.squeeze(1), NMS_THRESH)\n",
    "\n",
    "        # Pick the top region proposals after NMS\n",
    "        if POST_NMS_TOPN > 0:\n",
    "            keep = keep[:POST_NMS_TOPN]\n",
    "        proposals = proposals[keep, :]\n",
    "        scores = scores[keep, ]\n",
    "\n",
    "        # Only support single image as input\n",
    "        batch_inds = proposals.new_zeros(proposals.size(0), 1)\n",
    "        blob = torch.cat((batch_inds, proposals), 1)\n",
    " \n",
    "        # For batch (NMS)\n",
    "        # scores_keep = scores\n",
    "        # proposals_keep = proposals\n",
    "        # _, order = torch.sort(scores_keep, 1, True)\n",
    "\n",
    "        # output = scores.new(bs, post_nms_topN, 5).zero_()\n",
    "        # for i in range(bs):\n",
    "        #     # # 3. remove predicted boxes with either height or width < threshold\n",
    "        #     # # (NOTE: convert min_size to input image scale stored in im_info[2])\n",
    "        #     proposals_single = proposals_keep[i]\n",
    "        #     scores_single = scores_keep[i]\n",
    "\n",
    "        #     # # 4. sort all (proposal, score) pairs by score from highest to lowest\n",
    "        #     # # 5. take top pre_nms_topN (e.g. 6000)\n",
    "        #     order_single = order[i]\n",
    "\n",
    "        #     if pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel():\n",
    "        #         order_single = order_single[:pre_nms_topN]\n",
    "\n",
    "        #     proposals_single = proposals_single[order_single, :]\n",
    "        #     scores_single = scores_single[order_single].view(-1,1)\n",
    "\n",
    "        #     # 6. apply nms (e.g. threshold = 0.7)\n",
    "        #     # 7. take after_nms_topN (e.g. 300)\n",
    "        #     # 8. return the top proposals (-> RoIs top)\n",
    "\n",
    "        #     keep_idx_i = nms(torch.cat((proposals_single, scores_single), 1), nms_thresh, force_cpu=not cfg.USE_GPU_NMS)\n",
    "        #     keep_idx_i = keep_idx_i.long().view(-1)\n",
    "\n",
    "        #     if post_nms_topN > 0:\n",
    "        #         keep_idx_i = keep_idx_i[:post_nms_topN]\n",
    "        #     proposals_single = proposals_single[keep_idx_i, :]\n",
    "        #     scores_single = scores_single[keep_idx_i, :]\n",
    "\n",
    "        #     # padding 0 at the end.\n",
    "        #     num_proposal = proposals_single.size(0)\n",
    "        #     output[i,:,0] = i\n",
    "        #     output[i,:num_proposal,1:] = proposals_single\n",
    "        \n",
    "        return blob, scores\n",
    "\n",
    "    def anchor_target_layer(self, rpn_cls_score, gt_boxes, all_anchors, im_info=[320, 320, 1]):\n",
    "        '''\n",
    "        ### Parameters ###\n",
    "        rpn_cls_score: Class scores generated by the Region Proposal Network\n",
    "        gt_boxes: Ground Truth boxes\n",
    "        all_anchors: Anchor boxes generated by the anchor generation layer\n",
    "        \n",
    "        ### Fixed Parameters ###\n",
    "        im_info: Image dimensions\n",
    "        num_anchors: Number of different Anchor boxes used. By default, it is set to 9 here.\n",
    "\n",
    "        ### Additional information ###\n",
    "        POSITIVE_OVERLAP:       Threshold used to select if an anchor box is a good foreground box (Default: 0.7)\n",
    "\n",
    "        NEGATIVE_OVERLAP:       If the max overlap of a anchor from a ground truth box is lower than this thershold, it is marked as background. \n",
    "                                Boxes whose overlap is > than NEGATIVE_OVERLAP but < POSITIVE_OVERLAP are marked \n",
    "                                “don’t care”. (Default: 0.3)\n",
    "        \n",
    "        CLOBBER_POSITIVES:      If a particular anchor is satisfied by both the positive and the negative conditions,\n",
    "                                and if this value is set to False, then set the anchor to a negative example.\n",
    "                                Else, set the anchor to a positive example.\n",
    "        \n",
    "        RPN_BS:                 Total number of background and foreground anchors. (Default: 256)\n",
    "        \n",
    "        FG_FRACTION:            Fraction of the batch size that is foreground anchors (Default: 0.5). \n",
    "                                If the number of foreground anchors found is larger than RPN_BS * FG_FRACTION, \n",
    "                                the excess (indices are selected randomly) is marked “don’t care”.\n",
    "                            \n",
    "        RPN_POSITIVE_WEIGHT:    Using this value:\n",
    "                                Positive RPN examples are given a weight of RPN_POSITIVE_WEIGHT * 1 / num_of_positives\n",
    "                                Negative RPN examples are given a weight of (1 - RPN_POSITIVE_WEIGHT)\n",
    "                                Set to -1 by default, which will ensure uniform example weighting.\n",
    "        '''\n",
    "\n",
    "        # Map of shape (..., H, W)\n",
    "        height, width = rpn_cls_score.shape[1:3]\n",
    "\n",
    "        # Only keep anchors that are completely inside the image\n",
    "        inds_inside = np.where(\n",
    "            (all_anchors[:, 0] >= 0) &\n",
    "            (all_anchors[:, 1] >= 0) &\n",
    "            (all_anchors[:, 2] < im_info[1]) &  # width\n",
    "            (all_anchors[:, 3] < im_info[0])  # height\n",
    "        )[0]\n",
    "        anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "        # Label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        labels = np.empty((len(inds_inside), ), dtype=np.float32)\n",
    "        labels.fill(-1)\n",
    "\n",
    "        # BUG: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        # Overlaps between the Anchors and the Ground Truth boxes\n",
    "        overlaps = bbox_overlaps(\n",
    "            np.ascontiguousarray(anchors, dtype=np.float),\n",
    "            np.ascontiguousarray(gt_boxes, dtype=np.float))\n",
    "        argmax_overlaps = overlaps.argmax(axis=1)\n",
    "        max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "        gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
    "        gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n",
    "        gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Positives clobber Negatives\"\n",
    "        if not CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Foreground label: for each Ground Truth box, anchor with highest overlap\n",
    "        labels[gt_argmax_overlaps] = 1\n",
    "\n",
    "        # Foreground label: above threshold IOU\n",
    "        labels[max_overlaps >= POSITIVE_OVERLAP] = 1\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Negatives clobber Positives\"\n",
    "        if CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Subsample positive labels if we have too many\n",
    "        num_fg = int(FG_FRACTION * RPN_BS)\n",
    "        fg_inds = np.where(labels == 1)[0]\n",
    "        if len(fg_inds) > num_fg:\n",
    "            disable_inds = npr.choice(fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        # Subsample negative labels if we have too many\n",
    "        num_bg = RPN_BS - np.sum(labels == 1)\n",
    "        bg_inds = np.where(labels == 0)[0]\n",
    "        if len(bg_inds) > num_bg:\n",
    "            disable_inds = npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        bbox_targets = compute_targets_atl(anchors, gt_boxes[argmax_overlaps, :])\n",
    "        \n",
    "        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        # Only the positive ones have regression targets\n",
    "        bbox_inside_weights[labels == 1, :] = np.array((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "        labels = labels.numpy()\n",
    "        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        if RPN_POSITIVE_WEIGHT < 0:\n",
    "            # Uniform weighting of examples (given non-uniform sampling)\n",
    "            num_examples = np.sum(labels >= 0)\n",
    "            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "        else:\n",
    "            assert ((RPN_POSITIVE_WEIGHT > 0) &\n",
    "                    (RPN_POSITIVE_WEIGHT < 1))\n",
    "            positive_weights = (\n",
    "                RPN_POSITIVE_WEIGHT / np.sum(labels == 1))\n",
    "            negative_weights = (\n",
    "                (1.0 - RPN_POSITIVE_WEIGHT) / np.sum(labels == 0))\n",
    "        bbox_outside_weights[labels == 1, :] = positive_weights\n",
    "        bbox_outside_weights[labels == 0, :] = negative_weights\n",
    "\n",
    "        # Map up to original set of anchors\n",
    "        total_anchors = all_anchors.shape[0]\n",
    "        labels = unmap(labels, total_anchors, inds_inside, fill=-1)\n",
    "\n",
    "        bbox_targets = unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n",
    "        bbox_inside_weights = unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n",
    "        bbox_outside_weights = unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n",
    "\n",
    "        # Labels\n",
    "        labels = labels.reshape((1, height, width, self.n_anchors)).transpose(0, 3, 1, 2)\n",
    "        labels = labels.reshape((1, 1, self.n_anchors * height, width))\n",
    "        rpn_labels = labels\n",
    "        \n",
    "        # Bounding boxes\n",
    "        bbox_targets = bbox_targets.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_targets = bbox_targets\n",
    "        bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_inside_weights = bbox_inside_weights\n",
    "        bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_outside_weights = bbox_outside_weights\n",
    "\n",
    "        # Re-shape for future use\n",
    "        rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\n",
    "        rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_labels = rpn_labels.long()\n",
    "\n",
    "        # Data storing\n",
    "        self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "        self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "        self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "        self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "\n",
    "        for k in self._anchor_targets.keys():\n",
    "            self._score_summaries[k] = self._anchor_targets[k]\n",
    "\n",
    "        return rpn_labels\n",
    "        \n",
    "    def proposal_target_layer(self, proposed_rois, proposed_roi_scores, gt_boxes):\n",
    "        '''\n",
    "        1. Calculate overlap between ROI and GT boxes\n",
    "        2. Select promising ROIs by comparing against threshold(s)\n",
    "        3. Compute bounding box target regression targets and get bounding box regression labels\n",
    "        '''\n",
    "        # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        # print(\"gt_boxes: \", gt_boxes)\n",
    "        num_images = 1\n",
    "        rois_per_image = RPN_BS / num_images\n",
    "        # print(\"rois per image\", rois_per_image)\n",
    "        fg_rois_per_image = int(round(FG_FRACTION * rois_per_image))\n",
    "\n",
    "        # Sample rois with classification labels and bounding box regression targets\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "        overlaps = bbox_overlaps(proposed_rois[:, 1:5].data, gt_boxes[:, :4].data)\n",
    "        max_overlaps, gt_assignment = overlaps.max(1)\n",
    "        labels = gt_boxes[gt_assignment, [4]]\n",
    "\n",
    "        # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "        fg_inds = (max_overlaps >= FG_THRESH).nonzero().view(-1)\n",
    "        \n",
    "        # Guard against the case when an image has fewer than fg_rois_per_image\n",
    "        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "        bg_inds = ((max_overlaps < BG_THRESH_HI) + (max_overlaps >= BG_THRESH_LO) == 2).nonzero().view(-1)\n",
    "\n",
    "        # Ensure a fixed number of regions are sampled (optional?)\n",
    "        fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image = fix_sample_regions(fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image)\n",
    "        \n",
    "        # The indices that we're selecting (both fg and bg)\n",
    "        keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "        # Select sampled values from various arrays:\n",
    "        labels = labels[keep_inds].contiguous()\n",
    "\n",
    "        # Clamp labels for the background RoIs to 0\n",
    "        labels[int(fg_rois_per_image):] = 0\n",
    "        rois_final = proposed_rois[keep_inds].contiguous()\n",
    "        roi_scores_final = proposed_roi_scores[keep_inds].contiguous()\n",
    "        # Compute bounding box target regression targets\n",
    "        bbox_target_data = compute_targets_ptl(rois_final[:, 1:5].data, gt_boxes[gt_assignment[keep_inds]][:, :4].data, labels.data)\n",
    "        bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(bbox_target_data, num_of_class)\n",
    "\n",
    "        # Reshape tensors\n",
    "        rois_final = rois_final.view(-1, 5)\n",
    "        roi_scores_final = roi_scores_final.view(-1)\n",
    "        labels = labels.view(-1, 1)\n",
    "        bbox_targets = bbox_targets.view(-1, num_of_class * 4)\n",
    "        bbox_inside_weights = bbox_inside_weights.view(-1, num_of_class * 4)\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "\n",
    "        self._proposal_targets['rois'] = rois_final\n",
    "        self._proposal_targets['labels'] = labels.long()\n",
    "        self._proposal_targets['bbox_targets'] = bbox_targets\n",
    "        self._proposal_targets['bbox_inside_weights'] = bbox_inside_weights\n",
    "        self._proposal_targets['bbox_outside_weights'] = bbox_outside_weights\n",
    "\n",
    "        return rois_final, roi_scores_final \n",
    "\n",
    "    def region_proposal(self, net_conv, bb, anchors):\n",
    "        \"\"\"\n",
    "        Input: features from head network, bounding boxes, anchors generated\n",
    "        Output: rois\n",
    "        1. Proposal Layer\n",
    "        2. Anchor Target Layer\n",
    "        3. Compute RPN Loss\n",
    "        4. Proposal Target Layer\n",
    "        \n",
    "        Features -> class probabilities of anchors(fg or bg) and bbox coeff of anchors (to adjust them)\n",
    "        \"\"\"\n",
    "                                       \n",
    "        # TODO: Note down dimensions of features for each function\n",
    "        rpn = F.relu(self.rpn_net(net_conv))\n",
    "        rpn_cls_score = self.rpn_cls_score_net(rpn)\n",
    "                                       \n",
    "        rpn_cls_score_reshape = rpn_cls_score.view(1, 2, -1, rpn_cls_score.size()[-1]) # batch * 2 * (num_anchors*h) * w\n",
    "        # rpn_cls_score_reshape = rpn_cls_score.view(bs, 2, -1, rpn_cls_score.size()[-1]) # for batch\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, dim=1)\n",
    "\n",
    "        # Move channel to the last dimenstion, to fit the input of python functions\n",
    "        rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score).permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score = rpn_cls_score.permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score_reshape = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous() # batch * (num_anchors*h) * w * 2\n",
    "        rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(-1, 2), 1)[1]\n",
    "        # rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(bs, -1, 2), 1)[1]  # for batch (not sure)\n",
    "\n",
    "        rpn_bbox_pred = self.rpn_bbox_pred_net(rpn)\n",
    "        rpn_bbox_pred = rpn_bbox_pred.permute(0, 2, 3, 1).contiguous()  # batch * h * w * (num_anchors*4)                  \n",
    "\n",
    "        if self.mode == 'TRAIN':\n",
    "            rois, roi_scores = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, n_anchors=self.n_anchors)\n",
    "            rpn_labels = self.anchor_target_layer(rpn_cls_score, gt_boxes=bb, all_anchors=anchors)\n",
    "            rois, _ = self.proposal_target_layer(rois, roi_scores, gt_boxes=bb)\n",
    "        else:\n",
    "            rois, _ = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, n_anchors=self.n_anchors)\n",
    "        \n",
    "        self._predictions[\"rpn_cls_score\"] = rpn_cls_score\n",
    "        self._predictions[\"rpn_cls_score_reshape\"] = rpn_cls_score_reshape\n",
    "        self._predictions[\"rpn_cls_prob\"] = rpn_cls_prob\n",
    "        self._predictions[\"rpn_cls_pred\"] = rpn_cls_pred\n",
    "        self._predictions[\"rpn_bbox_pred\"] = rpn_bbox_pred\n",
    "        self._predictions[\"rois\"] = rois\n",
    "        \n",
    "        return rois\n",
    "    \n",
    "    def roi_align_layer(self, bottom, rois):\n",
    "        return RoIAlign((POOLING_SIZE, POOLING_SIZE), 1.0 / 16.0, 0)(bottom, rois)\n",
    "    \n",
    "    def _smooth_l1_loss(self, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n",
    "        sigma_2 = sigma**2\n",
    "        box_diff = bbox_pred - bbox_targets\n",
    "        in_box_diff = bbox_inside_weights * box_diff\n",
    "        abs_in_box_diff = torch.abs(in_box_diff)\n",
    "        smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
    "        in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
    "                      + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "        out_loss_box = bbox_outside_weights * in_loss_box\n",
    "        loss_box = out_loss_box\n",
    "        for i in sorted(dim, reverse=True):\n",
    "            loss_box = loss_box.sum(i)\n",
    "        loss_box = loss_box.mean()\n",
    "        return loss_box\n",
    "\n",
    "    def region_classification(self, fc7):\n",
    "        \"\"\"\n",
    "        cls_score\n",
    "            Linear layer (fc7 channels, num_classes)\n",
    "            torch max\n",
    "            softmax\n",
    "        bbox_pred\n",
    "            Linear layer (fc7 channels, num_classes*4)\n",
    "        \"\"\"\n",
    "        self.cls_score_net.to(device)\n",
    "        self.bbox_pred_net.to(device)\n",
    "        cls_score = self.cls_score_net(fc7)\n",
    "        cls_score = cls_score.view(-1, num_of_class) # Class scores for each anchor\n",
    "        cls_pred = torch.max(cls_score, 1)[1]\n",
    "        cls_prob = F.softmax(cls_score, dim=0)\n",
    "        bbox_pred = self.bbox_pred_net(fc7)\n",
    "        # print(\"cls score: \", cls_score.shape)\n",
    "        # print(\"bbox pred: \", bbox_pred.shape)\n",
    "        \n",
    "        self._predictions[\"cls_score\"] = cls_score\n",
    "        self._predictions[\"cls_pred\"] = cls_pred\n",
    "        self._predictions[\"cls_prob\"] = cls_prob\n",
    "        self._predictions[\"bbox_pred\"] = bbox_pred\n",
    "\n",
    "        return cls_prob, bbox_pred\n",
    "\n",
    "    def add_losses(self, sigma_rpn=3.0):\n",
    "                                       \n",
    "        # RPN, class loss\n",
    "        rpn_cls_score = self._predictions['rpn_cls_score_reshape'].view(-1, 2).to(device)\n",
    "        rpn_label = self._anchor_targets['rpn_labels'].view(-1).to(device)\n",
    "        rpn_select = (rpn_label.data != -1).nonzero().view(-1)\n",
    "        rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\n",
    "        rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\n",
    "        rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "\n",
    "        # RPN, bbox loss\n",
    "        rpn_bbox_pred = self._predictions['rpn_bbox_pred'].to(device)\n",
    "        rpn_bbox_targets = self._anchor_targets['rpn_bbox_targets'].to(device)\n",
    "        rpn_bbox_inside_weights = self._anchor_targets['rpn_bbox_inside_weights'].to(device)\n",
    "        rpn_bbox_outside_weights = self._anchor_targets['rpn_bbox_outside_weights'].to(device)\n",
    "        rpn_loss_box = self._smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights, sigma=sigma_rpn, dim=[1, 2, 3])\n",
    "\n",
    "        # RCNN, class loss\n",
    "        cls_score = self._predictions[\"cls_score\"].to(device)\n",
    "        # print(\"label: \", self._proposal_targets[\"labels\"].shape)\n",
    "        label = self._proposal_targets[\"labels\"].view(-1).to(device)\n",
    "        # print(\"cls_score: \", cls_score.shape)\n",
    "        # print(\"label: \", label.shape)\n",
    "        cross_entropy = F.cross_entropy(cls_score.view(-1, num_of_class), label)\n",
    "\n",
    "        # RCNN, bbox loss\n",
    "        bbox_pred = self._predictions['bbox_pred'].to(device)\n",
    "        bbox_pred = bbox_pred.view(RPN_BS, -1)\n",
    "        bbox_targets = self._proposal_targets['bbox_targets'].to(device)\n",
    "        bbox_inside_weights = self._proposal_targets['bbox_inside_weights'].to(device)\n",
    "        bbox_outside_weights = self._proposal_targets['bbox_outside_weights'].to(device)\n",
    "        # print(\"bbox_pred: \", bbox_pred.shape)\n",
    "        # print(\"bbox_targets: \", bbox_targets.shape)\n",
    "\n",
    "        loss_box = self._smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n",
    "\n",
    "        self._losses['cross_entropy'] = cross_entropy\n",
    "        self._losses['loss_box'] = loss_box\n",
    "        self._losses['rpn_cross_entropy'] = rpn_cross_entropy\n",
    "        self._losses['rpn_loss_box'] = rpn_loss_box\n",
    "\n",
    "        loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n",
    "        self._losses['total_loss'] = loss\n",
    "\n",
    "        for k in self._losses.keys():\n",
    "            self._event_summaries[k] = self._losses[k]\n",
    "\n",
    "        return loss                                       \n",
    "\n",
    "    def forward(self, x, bb, im_info=[320, 320, 1], train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mode = \"TRAIN\"\n",
    "        else:\n",
    "            self.mode = \"TEST\"\n",
    "        # Store image information\n",
    "        self._im_info = im_info\n",
    "                                       \n",
    "        # Pass the image through the Backbone ConvNet to generate the series of Feature maps\n",
    "        head_conv_net = self.head_net()\n",
    "        output_head = head_conv_net(x) # current: [1, 1024, 154, 154]\n",
    "        anchors, length = generate_anchors(output_head.size(2), output_head.size(3))\n",
    "        anchors = torch.from_numpy(anchors)\n",
    "        \n",
    "        # print(\"Output_head: \", output_head.shape) # [1,1024,20,20]\n",
    "        rois = self.region_proposal(output_head, bb, anchors)\n",
    "        # print(\"ROIS: \", rois.shape) # [RPN_BS, 5]\n",
    "        pool5 = self.roi_align_layer(output_head, rois)\n",
    "        # print(\"POOL5\", pool5.shape) # [RPN_BS, 1024, 7, 7]\n",
    "        fc7 = self.fc7()\n",
    "        fc7.to(device)\n",
    "        fc7_out = fc7(pool5)\n",
    "        # print(\"fc7: \", fc7_out.shape) # [RPN_BS, RPN_BS, 5, 5]\n",
    "        fc7_out = fc7_out.view(-1)\n",
    "        # print(\"fc7: \", fc7_out.shape) # [RPN_BS * 5*5] 6400\n",
    "        n_rois = rois.shape[0]\n",
    "        self.cls_score_net = nn.Linear(self._fc_channels, n_rois*num_of_class)\n",
    "        self.bbox_pred_net = nn.Linear(self._fc_channels, n_rois*num_of_class*4)\n",
    "        cls_prob, bbox_pred = self.region_classification(fc7_out)\n",
    "        # bbox_pred [RPN_BS, 28]\n",
    "        # bbox_targets [RPN_BS, 28]\n",
    "        # label [RPN_BS]\n",
    "        # cls_score [RPN_BS, 7]\n",
    "        # print(\"bbox_pred: \", bbox_pred)\n",
    "        if self.mode == 'TEST':\n",
    "            print(\"n_rois: \", n_rois)\n",
    "            # print(\"bbox_pred shape: \", bbox_pred.shape) # [512] -> n_rois * n_class * 4\n",
    "            # stds = bbox_pred.data.new((0.1, 0.1, 0.2, 0.2)).repeat(num_of_class).expand_as(bbox_pred)\n",
    "            # means = bbox_pred.data.new((0.0, 0.0, 0.0, 0.0)).repeat(num_of_class).expand_as(bbox_pred)\n",
    "            # self._predictions[\"bbox_pred\"] = bbox_pred.mul(stds).add(means)\n",
    "            return self._predictions[\"cls_score\"], self._predictions[\"cls_pred\"], self._predictions[\"cls_prob\"], self._predictions[\"bbox_pred\"], self._predictions[\"rois\"]\n",
    "            # return bbox_pred.mul(stds).add(means)\n",
    "        else:\n",
    "            loss = self.add_losses()\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster_R_CNN(\n",
      "  (head_conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (head_batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head_relu1): ReLU()\n",
      "  (head_pool1): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_layer1): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_relu2): ReLU()\n",
      "  (head_layer2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool2): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_relu3): ReLU()\n",
      "  (head_layer3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool3): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_relu4): ReLU()\n",
      "  (rpn_net): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rpn_cls_score_net): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rpn_bbox_pred_net): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (cls_score_net): Linear(in_features=1600, out_features=64, bias=True)\n",
      "  (bbox_pred_net): Linear(in_features=1600, out_features=256, bias=True)\n",
      ")\n",
      "There are 11309878 (11.31 million) parameters in this neural network\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "net = faster_R_CNN()\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "print(utils.display_num_param(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chinhui\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:251: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\chinhui\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:252: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rois:  tensor([[  0.0000,  97.0668,  70.4045, 272.7678, 319.0000],\n",
      "        [  0.0000,   0.0000,   0.0000, 141.5709, 194.9471],\n",
      "        [  0.0000,  33.7375,  76.3666, 176.8177, 226.7715],\n",
      "        [  0.0000,   0.0000,   0.0000, 129.7246, 315.9519],\n",
      "        [  0.0000,  21.1558,  62.0350, 156.9142, 211.9326],\n",
      "        [  0.0000, 199.3733,   0.0000, 239.0373,  69.8098],\n",
      "        [  0.0000,   3.7970,  76.7752, 140.9794, 226.5036],\n",
      "        [  0.0000,  69.7140,   0.0000, 245.6659, 311.9166]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 301.5217,   0.0000, 301.5217, 319.0000],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000, 319.0000],\n",
      "        [  0.0000, 265.9122,   0.0000, 265.9122, 319.0000],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000,   0.0000],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000,   0.0000],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000, 319.0000,   0.0000],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000,   0.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  49.7077, 110.1146,  99.1932, 210.7475],\n",
      "        [  0.0000, 159.5107, 202.2893, 203.1008, 319.0000],\n",
      "        [  0.0000, 159.5107, 202.2893, 203.1008, 319.0000],\n",
      "        [  0.0000,  47.4170,  95.6997,  97.5596, 193.9921],\n",
      "        [  0.0000,  47.4170,  95.6997,  97.5596, 193.9921],\n",
      "        [  0.0000,  49.7077, 110.1146,  99.1932, 210.7475],\n",
      "        [  0.0000,  52.3919, 125.0006, 100.7318, 227.9485],\n",
      "        [  0.0000,  47.4170,  95.6997,  97.5596, 193.9921]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0., 319.,   0., 319., 319.],\n",
      "        [  0.,   0., 319., 319., 319.],\n",
      "        [  0., 319.,   0., 319., 319.],\n",
      "        [  0.,   0.,   0., 319.,   0.],\n",
      "        [  0., 319.,   0., 319., 319.],\n",
      "        [  0., 319.,   0., 319., 319.],\n",
      "        [  0.,   0., 319., 319., 319.],\n",
      "        [  0., 319.,   0., 319., 319.]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   0.0000, 120.9190,  54.0359, 246.4336],\n",
      "        [  0.0000,   0.0000, 148.0641,  78.5883, 255.0974],\n",
      "        [  0.0000,   0.0000,  91.4496,  50.3678, 262.5945],\n",
      "        [  0.0000,   0.0000, 148.8776,  57.0682, 274.8623],\n",
      "        [  0.0000,   0.0000, 147.9265,  36.8426, 282.3464],\n",
      "        [  0.0000,   0.0000, 129.3588,  89.0681, 241.9772],\n",
      "        [  0.0000, 137.2510, 139.5999, 233.7213, 199.9556],\n",
      "        [  0.0000,   0.0000, 123.8914,  50.3343, 294.6281]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  72.1258, 126.5104, 117.6871, 181.6060],\n",
      "        [  0.0000,  11.6787, 259.2043,  93.5396, 312.0486],\n",
      "        [  0.0000,  74.9919, 138.2021, 116.4984, 194.5411],\n",
      "        [  0.0000, 159.9111, 178.8920, 212.8145, 230.5842],\n",
      "        [  0.0000,  62.9922, 120.6910, 120.1949, 173.5485],\n",
      "        [  0.0000,  47.1884, 259.0411,  99.2217, 309.8763],\n",
      "        [  0.0000,  27.7224, 259.5814, 108.5051, 312.7515],\n",
      "        [  0.0000,  29.8354, 258.0319,  82.6424, 307.6504]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 319.0000,   0.0000, 319.0000, 319.0000],\n",
      "        [  0.0000,  57.9957, 251.6514, 128.1840, 319.0000],\n",
      "        [  0.0000,  99.3998,   0.0000, 167.4865,   0.0000],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000, 319.0000],\n",
      "        [  0.0000,  61.6216,  39.5238, 148.5442, 163.7975],\n",
      "        [  0.0000,  62.3178, 119.7328, 152.9696, 158.8125],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000, 319.0000],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000, 285.4756]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 216.9213,  36.0434, 319.0000, 192.8416],\n",
      "        [  0.0000, 170.0663,   1.8624, 275.9336, 168.5294],\n",
      "        [  0.0000, 159.5591, 113.8217, 239.4089, 319.0000],\n",
      "        [  0.0000, 163.3838,   0.0000, 319.0000, 133.6724],\n",
      "        [  0.0000, 201.2236,  19.2810, 309.7570, 180.2932],\n",
      "        [  0.0000, 152.4933, 169.5594, 224.2209, 263.1782],\n",
      "        [  0.0000, 186.1225,   0.0000, 291.0975, 152.7673],\n",
      "        [  0.0000, 217.2497,   2.5029, 319.0000, 164.8915]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  68.1814, 207.5153,  95.0772, 282.9471],\n",
      "        [  0.0000,  35.5622, 225.4617, 101.7635, 295.5015],\n",
      "        [  0.0000,  34.7662, 196.6319, 101.1459, 267.9040],\n",
      "        [  0.0000,  85.1792, 236.4112, 151.1258, 304.9836],\n",
      "        [  0.0000,  47.3138, 189.4425, 111.2019, 274.9359],\n",
      "        [  0.0000,  35.2845, 210.5428, 101.6225, 280.9814],\n",
      "        [  0.0000,  52.0049, 223.3487, 118.2354, 292.7200],\n",
      "        [  0.0000,  63.9795, 233.6483, 127.7084, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  52.5377,  92.5033, 107.9377, 149.3377],\n",
      "        [  0.0000,  91.8495, 195.8922, 134.7323, 239.3560],\n",
      "        [  0.0000,  66.8200, 201.8866, 126.6164, 245.2953],\n",
      "        [  0.0000,  44.5845,  95.2861,  92.9667, 138.8267],\n",
      "        [  0.0000,   0.0000, 196.5036,  54.4311, 242.5693],\n",
      "        [  0.0000,  86.2021, 208.0515, 126.0140, 250.3328],\n",
      "        [  0.0000, 202.1821, 280.0598, 219.4802, 319.0000],\n",
      "        [  0.0000,  92.5019, 207.6150, 129.7593, 255.7531]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  39.3248, 244.1792,  93.0063, 295.6624],\n",
      "        [  0.0000,  37.8543, 254.2332, 120.1177, 300.9935],\n",
      "        [  0.0000,  40.4440, 260.7620,  93.2180, 311.5433],\n",
      "        [  0.0000,  54.6505, 221.9754,  89.1096, 298.6571],\n",
      "        [  0.0000,  38.5861, 227.8194,  92.8237, 279.7937],\n",
      "        [  0.0000, 160.8304, 254.5771, 219.4807, 298.2058],\n",
      "        [  0.0000,  20.9686, 253.6557, 102.2840, 300.2655],\n",
      "        [  0.0000, 165.6209, 261.3731, 195.6986, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  80.9447,  40.6104, 188.4910, 250.0659],\n",
      "        [  0.0000,  91.3083,   0.0000, 215.4088, 198.5959],\n",
      "        [  0.0000, 133.8156, 146.0836, 245.8516, 319.0000],\n",
      "        [  0.0000, 120.2983,  93.3059, 236.5213, 282.2984],\n",
      "        [  0.0000,  78.2880, 103.4115, 221.9629, 319.0000],\n",
      "        [  0.0000, 184.6859, 179.3595, 225.1030, 212.3396],\n",
      "        [  0.0000,  46.9998,   0.0000, 187.8989, 175.6864],\n",
      "        [  0.0000, 176.2990, 138.6669, 211.9253, 166.7604]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 214.8132, 150.7459, 259.3395, 195.1811],\n",
      "        [  0.0000,  78.6171,  44.9450, 123.3612, 104.8081],\n",
      "        [  0.0000, 165.5634, 135.7860, 211.8385, 181.1786],\n",
      "        [  0.0000, 221.6765, 178.7239, 288.1875, 236.0394],\n",
      "        [  0.0000, 175.4476, 131.6312, 242.9577, 188.3812],\n",
      "        [  0.0000, 159.1076, 131.4047, 226.8136, 188.2813],\n",
      "        [  0.0000,  82.0028,  68.0912, 133.6091, 117.9949],\n",
      "        [  0.0000,  75.2455,  58.0608, 151.8534, 110.5407]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 210.5638, 109.6190, 266.2707, 159.7343],\n",
      "        [  0.0000, 210.5638, 109.6190, 266.2707, 159.7343],\n",
      "        [  0.0000, 210.5638, 109.6190, 266.2707, 159.7343],\n",
      "        [  0.0000, 222.9149, 136.3979, 261.7712, 201.5600],\n",
      "        [  0.0000, 206.8721, 159.7967, 271.4884, 203.3133],\n",
      "        [  0.0000, 206.8721, 159.7967, 271.4884, 203.3133],\n",
      "        [  0.0000, 223.7920, 154.7792, 262.4851, 218.6569],\n",
      "        [  0.0000, 223.7920, 154.7792, 262.4851, 218.6569]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  38.8449, 120.6484, 104.5866, 178.5365],\n",
      "        [  0.0000, 319.0000,   0.0000, 319.0000, 319.0000],\n",
      "        [  0.0000,   0.0000,   0.0000, 319.0000,   0.0000],\n",
      "        [  0.0000,  49.1728, 120.7468, 123.8422, 287.5051],\n",
      "        [  0.0000,   7.3662,  40.1964,  47.8090, 112.6021],\n",
      "        [  0.0000,  39.5253,  73.2491, 104.7965, 132.0994],\n",
      "        [  0.0000, 186.4093, 273.0347, 273.1735, 319.0000],\n",
      "        [  0.0000, 184.5674, 217.5381, 303.6610, 237.8015]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 164.5490, 106.1967, 278.1224, 319.0000],\n",
      "        [  0.0000, 153.0366, 126.5955, 286.7313, 263.8698],\n",
      "        [  0.0000, 123.5178,   0.0000, 221.9059, 233.1307],\n",
      "        [  0.0000, 135.8354,  19.3139, 284.4663, 183.7611],\n",
      "        [  0.0000,  62.8781,   0.0000, 236.7588, 213.1495],\n",
      "        [  0.0000, 139.6967,  15.3665, 236.3065, 247.6814],\n",
      "        [  0.0000, 122.1038,  37.4372, 222.4999, 282.8326],\n",
      "        [  0.0000, 111.9079,  94.5354, 256.4432, 218.0419]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  80.8816,   0.0000, 177.3388, 108.0621],\n",
      "        [  0.0000,  86.8356,   5.5469, 146.3709,  82.9997],\n",
      "        [  0.0000,  85.8944,  21.0768, 144.6523,  99.6874],\n",
      "        [  0.0000,  86.8356,   5.5469, 146.3709,  82.9997],\n",
      "        [  0.0000,  83.0319,  35.7897, 140.8176, 116.8895],\n",
      "        [  0.0000,  86.8356,   5.5469, 146.3709,  82.9997],\n",
      "        [  0.0000,  64.8337,   0.0000, 159.5680, 106.0663],\n",
      "        [  0.0000,  83.0319,  35.7897, 140.8176, 116.8895]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 133.3238, 223.6034, 182.1975, 319.0000],\n",
      "        [  0.0000, 144.1756, 229.8165, 187.2506, 319.0000],\n",
      "        [  0.0000, 129.0047, 236.2412, 175.2527, 319.0000],\n",
      "        [  0.0000, 144.1756, 229.8165, 187.2506, 319.0000],\n",
      "        [  0.0000, 133.3238, 223.6034, 182.1975, 319.0000],\n",
      "        [  0.0000, 129.0047, 236.2412, 175.2527, 319.0000],\n",
      "        [  0.0000, 133.3238, 223.6034, 182.1975, 319.0000],\n",
      "        [  0.0000, 144.1756, 229.8165, 187.2506, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  27.2238,  25.3302, 170.7668, 319.0000],\n",
      "        [  0.0000,   8.8194, 145.7875,  74.7570, 216.9632],\n",
      "        [  0.0000,  48.8745, 107.3938, 196.9599, 319.0000],\n",
      "        [  0.0000,   8.1271, 161.4686,  74.0915, 232.0820],\n",
      "        [  0.0000,  27.2238,  25.3302, 170.7668, 319.0000],\n",
      "        [  0.0000, 176.0463, 114.2038, 228.2949, 228.9353],\n",
      "        [  0.0000,   8.1271, 161.4686,  74.0915, 232.0820],\n",
      "        [  0.0000,   8.1271, 161.4686,  74.0915, 232.0820]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  63.5438, 223.0919, 136.6615, 291.3745],\n",
      "        [  0.0000, 157.4634, 186.3005, 211.3734, 300.2532],\n",
      "        [  0.0000,  60.7413, 238.2644, 129.4558, 292.0421],\n",
      "        [  0.0000, 150.4942, 156.4222, 197.9517, 290.3974],\n",
      "        [  0.0000,  73.3869, 163.5222, 124.5855, 288.3536],\n",
      "        [  0.0000,  58.5117, 208.8479, 137.6014, 319.0000],\n",
      "        [  0.0000,  43.9276, 205.5061, 122.8043, 319.0000],\n",
      "        [  0.0000, 134.4634, 160.5139, 213.6277, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 231.6730,  94.9993, 285.0308, 186.9750],\n",
      "        [  0.0000, 156.0049,  22.1864, 218.0961, 107.9672],\n",
      "        [  0.0000,  60.8673,  76.6966, 155.9969, 215.5719],\n",
      "        [  0.0000, 200.1474, 169.6068, 247.8510, 286.8278],\n",
      "        [  0.0000, 134.5964,  53.2091, 183.1904, 175.6022],\n",
      "        [  0.0000, 172.9550,  39.9488, 234.8879, 123.7538],\n",
      "        [  0.0000,  85.5527, 117.0679, 133.4124, 238.4425],\n",
      "        [  0.0000, 131.7232,  10.4832, 190.4131, 108.9059]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 181.8478,  74.8215, 269.4281, 209.6266],\n",
      "        [  0.0000,   0.0000,  92.4220, 162.1378, 314.9138],\n",
      "        [  0.0000, 202.5452,  60.5234, 271.9559, 155.3977],\n",
      "        [  0.0000,  44.2067,  34.9659, 195.0659, 295.7646],\n",
      "        [  0.0000, 192.4930, 187.7999, 252.1310, 296.5161],\n",
      "        [  0.0000, 201.2658,  89.9721, 268.6451, 185.4623],\n",
      "        [  0.0000,  30.3629,   8.2080, 178.7437, 258.3863],\n",
      "        [  0.0000,  27.5504,  65.6941, 181.1726, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 143.6798,  37.3449, 247.6027, 319.0000],\n",
      "        [  0.0000, 115.9996,  57.0938, 219.3864, 319.0000],\n",
      "        [  0.0000, 118.2194,   1.2250, 246.3304, 289.0623],\n",
      "        [  0.0000,  88.7258,  97.9980, 229.3022, 294.1119],\n",
      "        [  0.0000,  90.3920,  25.4106, 319.0000, 288.2414],\n",
      "        [  0.0000, 103.7500,  37.6467, 235.6307, 201.8013],\n",
      "        [  0.0000,  96.4712, 142.5818, 244.1858, 319.0000],\n",
      "        [  0.0000, 119.9771,   0.0000, 256.4385, 217.2211]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[0.0000e+00, 2.7641e-01, 1.1899e+02, 1.0956e+02, 3.1250e+02],\n",
      "        [0.0000e+00, 1.3446e+02, 1.2986e+02, 2.2441e+02, 2.8812e+02],\n",
      "        [0.0000e+00, 3.4151e+01, 1.3227e+02, 1.2932e+02, 2.9501e+02],\n",
      "        [0.0000e+00, 1.1472e+02, 1.2137e+02, 2.0596e+02, 2.9208e+02],\n",
      "        [0.0000e+00, 1.3341e+02, 1.5886e+02, 2.2523e+02, 3.1836e+02],\n",
      "        [0.0000e+00, 1.5960e+01, 1.0846e+02, 1.1145e+02, 2.5034e+02],\n",
      "        [0.0000e+00, 2.2198e+01, 1.3241e+02, 1.1884e+02, 2.6654e+02],\n",
      "        [0.0000e+00, 1.0629e+02, 1.5324e+02, 2.0984e+02, 3.1900e+02]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  98.8074,   0.0000, 188.9347, 319.0000],\n",
      "        [  0.0000,  71.2008,  67.7701, 113.2179, 107.9578],\n",
      "        [  0.0000,  59.3123,  74.5104, 100.9342, 121.8470],\n",
      "        [  0.0000,  65.0298, 110.3357, 185.1655, 319.0000],\n",
      "        [  0.0000,  49.1583,  52.0026, 173.1466, 301.7050],\n",
      "        [  0.0000,  79.4153,  53.8216, 203.5706, 305.7863],\n",
      "        [  0.0000,  87.2077,   0.0000, 202.0810, 270.4045],\n",
      "        [  0.0000,  58.6982,  58.1527, 100.5199, 104.4866]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  70.2941,  78.1337, 113.4860, 115.5891],\n",
      "        [  0.0000,  52.5702,  25.0271, 178.1854, 295.1368],\n",
      "        [  0.0000,  61.4429,  80.4580, 169.5973, 319.0000],\n",
      "        [  0.0000,  71.0310, 117.9957, 190.7802, 319.0000],\n",
      "        [  0.0000,  71.0310, 117.9957, 190.7802, 319.0000],\n",
      "        [  0.0000,  61.4429,  80.4580, 169.5973, 319.0000],\n",
      "        [  0.0000,  70.2941,  78.1337, 113.4860, 115.5891],\n",
      "        [  0.0000,  52.5702,  25.0271, 178.1854, 295.1368]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 145.7522,   0.0000, 207.8309, 125.7561],\n",
      "        [  0.0000, 159.7752, 233.8841, 221.3136, 319.0000],\n",
      "        [  0.0000, 104.2971,   0.0000, 228.5776, 232.7108],\n",
      "        [  0.0000, 145.7522,   0.0000, 207.8309, 125.7561],\n",
      "        [  0.0000, 145.7522,   0.0000, 207.8309, 125.7561],\n",
      "        [  0.0000,  64.4162,   5.7664, 274.7861, 214.2228],\n",
      "        [  0.0000, 145.7522,   0.0000, 207.8309, 125.7561],\n",
      "        [  0.0000,  91.1765,   0.0000, 207.4243, 283.4943]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  57.5494, 198.6591, 122.1568, 271.5915],\n",
      "        [  0.0000, 153.8046, 207.5752, 216.9831, 284.4922],\n",
      "        [  0.0000,  57.7233, 183.4974, 121.9056, 255.4575],\n",
      "        [  0.0000,  73.1490, 181.3803, 137.9851, 255.7314],\n",
      "        [  0.0000,  62.4155, 176.1582, 150.4413, 319.0000],\n",
      "        [  0.0000, 138.6626, 265.6619, 203.1692, 319.0000],\n",
      "        [  0.0000,  72.6876, 212.0323, 138.3972, 288.7968],\n",
      "        [  0.0000,  62.2452, 126.8345, 148.8746, 291.1967]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 162.5323, 219.4521, 195.7499, 284.7064],\n",
      "        [  0.0000,  41.3260, 121.2359, 203.0193, 319.0000],\n",
      "        [  0.0000, 173.4099, 173.7002, 202.8025, 222.1368],\n",
      "        [  0.0000,  68.6574, 127.7796, 107.1054, 194.6776],\n",
      "        [  0.0000,  46.4943, 217.4558,  83.1861, 275.5899],\n",
      "        [  0.0000, 158.5268, 173.9544, 188.0604, 223.8824],\n",
      "        [  0.0000,  44.0357, 230.7066,  81.4030, 286.1250],\n",
      "        [  0.0000,  68.4653, 111.9321, 107.2110, 178.2270]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 104.3388,  93.0169, 192.0937, 279.3329],\n",
      "        [  0.0000,  30.1520,  83.7624, 196.2975, 319.0000],\n",
      "        [  0.0000, 119.1436,  70.5941, 207.8287, 261.9016],\n",
      "        [  0.0000,  96.2280,  77.4217, 319.0000, 268.7888],\n",
      "        [  0.0000, 104.8318,   0.0000, 221.4463, 309.0093],\n",
      "        [  0.0000,   0.0000, 138.9690, 137.5882, 319.0000],\n",
      "        [  0.0000,  17.5643, 149.5561, 182.3953, 319.0000],\n",
      "        [  0.0000, 104.2536,  89.0786, 239.0595, 230.7516]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  64.9388, 163.0674, 106.3267, 232.2895],\n",
      "        [  0.0000,  65.2323, 179.1866, 106.4813, 248.7527],\n",
      "        [  0.0000, 151.8808, 216.0779, 199.8251, 273.9940],\n",
      "        [  0.0000,  59.0546, 189.6690, 120.9585, 252.8365],\n",
      "        [  0.0000,  59.0186, 173.6526, 120.7941, 236.9265],\n",
      "        [  0.0000, 140.8618, 205.3409, 218.8841, 319.0000],\n",
      "        [  0.0000,  42.9289, 173.6072, 104.1188, 236.9656],\n",
      "        [  0.0000, 134.0221, 214.4072, 184.3848, 274.5241]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 173.7353, 120.5328, 210.1816, 172.2668],\n",
      "        [  0.0000,  42.4882, 244.9664, 107.4362, 312.5957],\n",
      "        [  0.0000,  10.8081, 124.7982,  74.6243, 189.4377],\n",
      "        [  0.0000, 155.8783, 247.2328, 218.4109, 314.2349],\n",
      "        [  0.0000,  40.8577, 227.5286, 105.5401, 299.0966],\n",
      "        [  0.0000, 170.4033, 133.3648, 207.9788, 183.1915],\n",
      "        [  0.0000,  98.9214, 106.6062, 157.8518, 174.8177],\n",
      "        [  0.0000,  38.7835, 132.4942,  77.0587, 197.0244]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 117.8505, 168.4701, 175.1103, 227.9593],\n",
      "        [  0.0000,  26.6290, 268.3181,  91.4398, 319.0000],\n",
      "        [  0.0000,  42.5615, 268.1591, 107.4236, 319.0000],\n",
      "        [  0.0000, 117.8505, 168.4701, 175.1103, 227.9593],\n",
      "        [  0.0000,  42.5615, 268.1591, 107.4236, 319.0000],\n",
      "        [  0.0000,  42.5615, 268.1591, 107.4236, 319.0000],\n",
      "        [  0.0000,  42.5615, 268.1591, 107.4236, 319.0000],\n",
      "        [  0.0000, 112.2205, 186.1110, 172.7468, 246.4415]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  46.2959, 236.5030, 106.6265, 306.2635],\n",
      "        [  0.0000, 124.6634, 200.4622, 185.3038, 282.8401],\n",
      "        [  0.0000, 125.1434, 216.1319, 185.7890, 296.0320],\n",
      "        [  0.0000,  31.1580, 220.6295,  90.8922, 291.3644],\n",
      "        [  0.0000, 125.9150, 233.0126, 186.8451, 307.4413],\n",
      "        [  0.0000,  30.7995, 236.4375,  90.7220, 306.9695],\n",
      "        [  0.0000,  41.6843, 219.6817,  85.7765, 281.6928],\n",
      "        [  0.0000,  48.6805, 239.4987,  89.5970, 307.1646]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 178.1473,   0.0000, 225.6275,  90.1665],\n",
      "        [  0.0000, 178.1473,   0.0000, 225.6275,  90.1665],\n",
      "        [  0.0000, 178.1473,   0.0000, 225.6275,  90.1665],\n",
      "        [  0.0000, 178.1473,   0.0000, 225.6275,  90.1665],\n",
      "        [  0.0000, 151.8204,   0.0000, 199.3905,  87.8342],\n",
      "        [  0.0000, 151.8204,   0.0000, 199.3905,  87.8342],\n",
      "        [  0.0000, 178.1473,   0.0000, 225.6275,  90.1665],\n",
      "        [  0.0000, 151.8204,   0.0000, 199.3905,  87.8342]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  59.2688,  88.7122, 161.3088, 284.1442],\n",
      "        [  0.0000,  59.2688,  88.7122, 161.3088, 284.1442],\n",
      "        [  0.0000,  59.2688,  88.7122, 161.3088, 284.1442],\n",
      "        [  0.0000,  39.2019, 100.1252, 125.4469, 319.0000],\n",
      "        [  0.0000,  55.2623,  98.2117, 140.2254, 319.0000],\n",
      "        [  0.0000,  59.2688,  88.7122, 161.3088, 284.1442],\n",
      "        [  0.0000,  55.2623,  98.2117, 140.2254, 319.0000],\n",
      "        [  0.0000,  59.2688,  88.7122, 161.3088, 284.1442]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 117.6290, 214.8060, 157.5692, 286.7564],\n",
      "        [  0.0000, 119.2872, 219.8249, 184.2235, 284.4363],\n",
      "        [  0.0000,  64.1250, 228.9921, 132.9824, 287.7536],\n",
      "        [  0.0000,  68.5056, 247.7623, 135.6658, 308.0894],\n",
      "        [  0.0000,  47.6002, 233.5045, 128.9343, 319.0000],\n",
      "        [  0.0000, 106.2459, 204.9688, 170.1999, 268.5118],\n",
      "        [  0.0000, 106.0329, 220.7358, 170.3307, 283.2034],\n",
      "        [  0.0000, 121.6414, 236.5719, 185.6666, 299.5981]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  59.3092, 159.9644, 132.1208, 226.2708],\n",
      "        [  0.0000, 104.8096, 268.3531, 170.6528, 319.0000],\n",
      "        [  0.0000,  55.0464, 141.8907, 121.5878, 204.8772],\n",
      "        [  0.0000, 149.6590, 181.5681, 226.1225, 248.0676],\n",
      "        [  0.0000,  73.5901, 151.2381, 112.3531, 214.2267],\n",
      "        [  0.0000,  40.4589, 267.8699, 106.2439, 319.0000],\n",
      "        [  0.0000,  24.2871, 267.6641,  90.1025, 319.0000],\n",
      "        [  0.0000,  72.7351, 134.9009, 111.6031, 202.3537]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  87.0316,  35.5969, 275.0099, 271.9520],\n",
      "        [  0.0000,  93.0520, 101.8616, 244.5508, 319.0000],\n",
      "        [  0.0000, 111.5751, 191.2131, 146.1069, 223.6581],\n",
      "        [  0.0000, 118.4115, 107.8563, 223.7193, 319.0000],\n",
      "        [  0.0000,  67.3967,   0.0000, 232.1033, 319.0000],\n",
      "        [  0.0000, 105.2444,   0.0000, 270.4181, 319.0000],\n",
      "        [  0.0000,  27.6275,   0.0000, 277.2574, 319.0000],\n",
      "        [  0.0000,  27.7504,  72.1209, 228.9936, 304.1836]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  28.4075,  59.6272, 245.3661, 307.0714],\n",
      "        [  0.0000,   0.0000, 103.9006, 210.1526, 284.8545],\n",
      "        [  0.0000,   9.5526, 140.6570, 220.4335, 319.0000],\n",
      "        [  0.0000,  68.0171,  73.6590, 232.9888, 319.0000],\n",
      "        [  0.0000,   0.0000,  40.0833, 198.9750, 319.0000],\n",
      "        [  0.0000,  53.7961, 128.8809, 250.4835, 319.0000],\n",
      "        [  0.0000, 100.7835, 148.0117, 234.1254, 302.2656],\n",
      "        [  0.0000, 107.1901, 100.4167, 241.5042, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   0.0000, 110.2420, 115.1631, 251.8513],\n",
      "        [  0.0000, 133.6196, 275.9285, 169.9420, 319.0000],\n",
      "        [  0.0000,  26.8502, 160.1910, 130.6646, 319.0000],\n",
      "        [  0.0000,  18.2620, 114.6701, 102.4070, 287.5317],\n",
      "        [  0.0000, 112.7249,  57.6534, 250.0590, 319.0000],\n",
      "        [  0.0000, 129.4697,   0.0000, 207.8635, 319.0000],\n",
      "        [  0.0000, 101.9001, 114.1481, 236.7802, 319.0000],\n",
      "        [  0.0000,  34.1910, 114.8933, 126.9906, 312.6162]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   0.0000,  52.8049, 140.5862, 319.0000],\n",
      "        [  0.0000, 105.1828,  25.7049, 200.0969, 258.6380],\n",
      "        [  0.0000, 120.7178,  21.5223, 217.5508, 259.8744],\n",
      "        [  0.0000, 101.5935,  63.2248, 251.6490, 281.7582],\n",
      "        [  0.0000,  43.0629,   0.0000, 138.3426, 223.0409],\n",
      "        [  0.0000,  55.5462,  12.9816, 150.3105, 245.0159],\n",
      "        [  0.0000,  40.8756,  92.8841, 134.8929, 319.0000],\n",
      "        [  0.0000,  25.4459, 114.7987, 150.8932, 283.6675]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 139.9450, 261.0013, 202.0357, 319.0000],\n",
      "        [  0.0000, 124.8338, 245.1517, 185.8807, 302.6341],\n",
      "        [  0.0000,  69.8542, 179.5056, 107.6739, 265.0647],\n",
      "        [  0.0000,  62.1292, 217.0515, 122.6836, 279.6610],\n",
      "        [  0.0000, 135.5946, 228.2529, 219.2265, 319.0000],\n",
      "        [  0.0000,  64.0638, 201.4923, 122.5550, 265.3897],\n",
      "        [  0.0000, 121.8119, 244.8001, 202.8450, 319.0000],\n",
      "        [  0.0000, 140.7639, 245.3905, 201.8540, 303.0624]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 116.9474, 163.7086, 288.7851, 285.6456],\n",
      "        [  0.0000,   0.0000, 152.7333, 267.6608, 319.0000],\n",
      "        [  0.0000,  95.3153, 117.4207, 270.3764, 249.6621],\n",
      "        [  0.0000, 159.9754, 126.8269, 274.1136, 319.0000],\n",
      "        [  0.0000,  84.7563,  70.1167, 239.0959, 319.0000],\n",
      "        [  0.0000, 100.5876, 147.2019, 271.9452, 268.2529],\n",
      "        [  0.0000,  38.1522,  33.8913, 208.3956, 149.7417],\n",
      "        [  0.0000, 123.3456, 134.2016, 296.3615, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  40.3001,  96.0093, 192.7276, 319.0000],\n",
      "        [  0.0000,  25.3771,  28.4796, 177.3782, 319.0000],\n",
      "        [  0.0000,  99.5409, 272.8423, 135.9395, 319.0000],\n",
      "        [  0.0000,  25.3771,  28.4796, 177.3782, 319.0000],\n",
      "        [  0.0000,  25.3771,  28.4796, 177.3782, 319.0000],\n",
      "        [  0.0000,  40.3001,  96.0093, 192.7276, 319.0000],\n",
      "        [  0.0000,  40.3001,  96.0093, 192.7276, 319.0000],\n",
      "        [  0.0000,  25.3771,  28.4796, 177.3782, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  36.5805, 150.5323, 164.0524, 302.4056],\n",
      "        [  0.0000, 129.9236,  58.3793, 258.0550, 205.8994],\n",
      "        [  0.0000, 138.8145,   0.0000, 319.0000, 205.7860],\n",
      "        [  0.0000, 107.0013,   0.0000, 258.2178, 288.4722],\n",
      "        [  0.0000,  55.5263, 162.9274, 224.1347, 276.6799],\n",
      "        [  0.0000,  58.0624,   0.0000, 273.5293, 254.5653],\n",
      "        [  0.0000,  37.5224,  28.3746, 319.0000, 216.6049],\n",
      "        [  0.0000, 104.6411, 130.1255, 221.8710, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   0.0000,  18.2977, 244.2346, 230.3123],\n",
      "        [  0.0000, 163.1214, 255.9229, 203.9308, 311.3990],\n",
      "        [  0.0000, 172.2250,   0.0000, 277.5072, 286.5323],\n",
      "        [  0.0000,   0.0000,   0.0000, 155.5449, 263.6337],\n",
      "        [  0.0000, 109.1045,  91.0563, 319.0000, 291.9230],\n",
      "        [  0.0000,  77.3644, 100.9603, 279.4257, 319.0000],\n",
      "        [  0.0000, 221.5503,   9.8630, 281.0845,  72.1064],\n",
      "        [  0.0000, 248.2230,   6.3476, 283.0406,  95.2193]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  33.0693,   0.0000, 191.7860, 188.7093],\n",
      "        [  0.0000,   0.0000,  83.8928, 319.0000, 285.3270],\n",
      "        [  0.0000, 112.0291,   0.0000, 261.2360, 200.9014],\n",
      "        [  0.0000,   0.0000,   0.0000, 311.8206, 198.1653],\n",
      "        [  0.0000,  69.2303,  23.3788, 231.5472, 150.0956],\n",
      "        [  0.0000,  68.7777,  69.7344, 203.9836, 319.0000],\n",
      "        [  0.0000,  79.5699,  74.9663, 231.0928, 224.4437],\n",
      "        [  0.0000,  24.6381,  43.0552, 242.4637, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  27.9421,  99.8900, 181.7813, 319.0000],\n",
      "        [  0.0000,  71.8548,  59.2270, 227.6078, 319.0000],\n",
      "        [  0.0000,  43.8615, 147.7549, 197.7176, 319.0000],\n",
      "        [  0.0000,  88.9447, 111.6310, 244.3100, 319.0000],\n",
      "        [  0.0000,   6.0512, 167.4621, 238.0059, 319.0000],\n",
      "        [  0.0000,   5.9195,  87.4084, 238.1837, 319.0000],\n",
      "        [  0.0000,  75.4757, 147.4167, 229.5674, 319.0000],\n",
      "        [  0.0000,  56.6610, 128.0182,  93.0264, 194.4124]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 203.8977,   0.0000, 284.1577,  75.9160],\n",
      "        [  0.0000, 203.8977,   0.0000, 284.1577,  75.9160],\n",
      "        [  0.0000, 190.3834,   0.0000, 268.2077,  88.9038],\n",
      "        [  0.0000, 203.1635,   0.0000, 284.9781, 112.0921],\n",
      "        [  0.0000, 203.8977,   0.0000, 284.1577,  75.9160],\n",
      "        [  0.0000, 203.1635,   0.0000, 284.9781, 112.0921],\n",
      "        [  0.0000, 190.3834,   0.0000, 268.2077,  88.9038],\n",
      "        [  0.0000, 203.1635,   0.0000, 284.9781, 112.0921]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  72.3557, 161.6929, 138.6790, 254.9528],\n",
      "        [  0.0000, 133.6671, 239.8696, 223.4847, 319.0000],\n",
      "        [  0.0000, 164.1134, 235.7481, 252.0710, 319.0000],\n",
      "        [  0.0000, 168.7492, 227.8816, 234.4826, 316.4467],\n",
      "        [  0.0000, 152.9883, 258.0203, 220.1523, 319.0000],\n",
      "        [  0.0000, 130.5287, 256.9225, 214.5838, 319.0000],\n",
      "        [  0.0000, 148.7511, 220.7765, 237.6389, 319.0000],\n",
      "        [  0.0000,  89.1726, 177.1779, 154.8547, 273.3382]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  39.3226, 212.3156,  77.8828, 283.3139],\n",
      "        [  0.0000,  75.3688, 157.2079, 138.8164, 222.3919],\n",
      "        [  0.0000,  91.4114, 156.9803, 155.3207, 225.4227],\n",
      "        [  0.0000,  39.3988, 196.5621,  77.9749, 267.7101],\n",
      "        [  0.0000,  75.3688, 157.2079, 138.8164, 222.3919],\n",
      "        [  0.0000,  75.3688, 157.2079, 138.8164, 222.3919],\n",
      "        [  0.0000,  91.4114, 156.9803, 155.3207, 225.4227],\n",
      "        [  0.0000, 101.9562, 144.9627, 140.5277, 214.1398]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 146.1483, 132.9379, 227.0697, 298.3644],\n",
      "        [  0.0000, 130.4989, 103.0401, 212.1197, 270.5995],\n",
      "        [  0.0000,  90.0812, 126.3331, 237.3128, 319.0000],\n",
      "        [  0.0000, 116.2965, 104.5810, 196.2386, 274.8645],\n",
      "        [  0.0000, 162.7426,   0.0000, 319.0000, 226.1378],\n",
      "        [  0.0000, 118.0898, 118.8886, 269.2197, 319.0000],\n",
      "        [  0.0000, 149.1475,   0.0000, 301.1064, 275.3000],\n",
      "        [  0.0000,  91.3859,  46.5423, 319.0000, 230.0633]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  54.6406, 166.0583,  93.7487, 238.2281],\n",
      "        [  0.0000, 142.3292, 188.6590, 178.8706, 248.3440],\n",
      "        [  0.0000,  54.6321, 182.0500,  93.7489, 254.2303],\n",
      "        [  0.0000, 135.0738, 179.8724, 187.8068, 251.8882],\n",
      "        [  0.0000,  93.1548, 255.7730, 154.3381, 319.0000],\n",
      "        [  0.0000,  43.0638, 173.1470, 106.9538, 238.5444],\n",
      "        [  0.0000, 148.9195, 177.8494, 203.0364, 254.9577],\n",
      "        [  0.0000,  59.1118, 173.1851, 123.0870, 238.2041]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 188.9711, 210.1672, 261.4005, 319.0000],\n",
      "        [  0.0000, 210.5825, 143.8174, 265.1764, 213.2510],\n",
      "        [  0.0000, 227.9100,  50.7804, 319.0000, 117.9326],\n",
      "        [  0.0000, 191.2169, 216.3463, 228.7422, 285.4012],\n",
      "        [  0.0000, 142.3410,  73.8724, 200.4244, 139.3122],\n",
      "        [  0.0000, 157.9478, 159.7129, 213.9508, 232.6322],\n",
      "        [  0.0000, 225.8829,  45.3014, 263.9848, 117.0316],\n",
      "        [  0.0000,  98.9001, 146.8246, 152.3742, 212.7403]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 211.0740,  69.0016, 250.5246, 132.3504],\n",
      "        [  0.0000, 210.3342,  85.1945, 249.9065, 146.3533],\n",
      "        [  0.0000, 192.0350, 180.6901, 231.7084, 239.7712],\n",
      "        [  0.0000, 210.3342,  85.1945, 249.9065, 146.3533],\n",
      "        [  0.0000, 192.0350, 180.6901, 231.7084, 239.7712],\n",
      "        [  0.0000, 192.0350, 180.6901, 231.7084, 239.7712],\n",
      "        [  0.0000, 210.3342,  85.1945, 249.9065, 146.3533],\n",
      "        [  0.0000, 192.0350, 180.6901, 231.7084, 239.7712]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 129.6157, 180.0911, 187.8191, 260.3994],\n",
      "        [  0.0000,  42.8890, 189.6481, 105.7067, 257.5603],\n",
      "        [  0.0000,  52.9970, 183.1695,  93.1015, 250.9467],\n",
      "        [  0.0000,  36.7826, 183.0348,  76.9789, 250.2980],\n",
      "        [  0.0000, 125.3452, 196.1708, 183.6199, 282.1865],\n",
      "        [  0.0000,  36.8572, 199.0038,  77.0068, 266.4574],\n",
      "        [  0.0000,  26.5598, 173.6463,  89.2334, 242.3566],\n",
      "        [  0.0000,  42.6526, 173.6194, 105.4725, 241.8155]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  82.0725,  79.5489, 250.4111, 319.0000],\n",
      "        [  0.0000,  16.8821,   2.1781, 186.9569, 319.0000],\n",
      "        [  0.0000,  69.6407,   0.0000, 217.3023, 319.0000],\n",
      "        [  0.0000,  50.5121,  14.7104, 229.7377, 273.1924],\n",
      "        [  0.0000,  37.7039, 113.0140, 228.0669, 319.0000],\n",
      "        [  0.0000,   0.0000,  92.6041, 252.7278, 266.9569],\n",
      "        [  0.0000,  26.8098, 154.6104, 210.1896, 319.0000],\n",
      "        [  0.0000,  46.7702, 168.9547, 259.2056, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  53.6916,  68.9977, 266.5412, 319.0000],\n",
      "        [  0.0000,   0.0000,  28.0842, 184.7946, 286.1025],\n",
      "        [  0.0000, 149.9456, 243.3684, 189.0670, 293.6060],\n",
      "        [  0.0000,  71.5453,  82.3787, 216.5364, 319.0000],\n",
      "        [  0.0000, 163.3598, 244.8806, 204.0560, 285.2370],\n",
      "        [  0.0000,   0.0000,   0.0000, 213.3560, 247.1177],\n",
      "        [  0.0000,   0.0000,  94.2386, 192.0034, 319.0000],\n",
      "        [  0.0000,   9.7748,  45.3745, 241.4851, 296.1302]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 120.4624,  67.8679, 251.4530, 238.6469],\n",
      "        [  0.0000,  86.7440,  71.0100, 220.6016, 233.4290],\n",
      "        [  0.0000,   0.0000,  72.9684, 288.1248, 253.5520],\n",
      "        [  0.0000,  71.3339,  57.4383, 205.5500, 214.4645],\n",
      "        [  0.0000,  55.9623, 105.2496, 189.7928, 262.6703],\n",
      "        [  0.0000,  77.3255,   0.0000, 233.3253, 282.6891],\n",
      "        [  0.0000,  90.3620,  57.6680, 319.0000, 239.0869],\n",
      "        [  0.0000,  87.2628,  41.2782, 221.4684, 198.7514]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 111.8651, 130.1763, 188.3471, 287.8663],\n",
      "        [  0.0000, 141.3005,  93.9926, 298.3530, 205.7762],\n",
      "        [  0.0000, 105.9236, 111.8460, 237.6151, 292.9825],\n",
      "        [  0.0000,  74.4617, 110.7348, 205.3678, 294.5979],\n",
      "        [  0.0000, 156.0256, 110.4628, 313.8338, 220.8876],\n",
      "        [  0.0000, 138.7858,  61.8698, 269.3575, 247.8027],\n",
      "        [  0.0000, 175.5842,  98.7603, 251.7948, 256.1852],\n",
      "        [  0.0000,  97.3018, 145.5485, 250.8919, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  66.1742,  81.7703, 219.1759, 319.0000],\n",
      "        [  0.0000,   0.0000, 110.4687, 156.1091, 319.0000],\n",
      "        [  0.0000,  98.4061,  80.9715, 250.7808, 319.0000],\n",
      "        [  0.0000,  34.5076,  81.1735, 187.2177, 319.0000],\n",
      "        [  0.0000,  91.2058,  99.6069, 224.4252, 272.9752],\n",
      "        [  0.0000,  30.0594,  74.4929, 160.1732, 272.2488],\n",
      "        [  0.0000,  82.1559,  33.7525, 235.1519, 319.0000],\n",
      "        [  0.0000,  85.1389, 136.5256, 233.9707, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   9.9480,  68.1206, 272.4368, 299.2318],\n",
      "        [  0.0000, 170.4530, 172.7887, 229.9077, 319.0000],\n",
      "        [  0.0000, 134.2146,  97.3202, 205.3811, 158.4198],\n",
      "        [  0.0000, 163.5592, 188.0190, 236.8875, 248.0002],\n",
      "        [  0.0000,   0.0000,  39.6090, 231.6001, 274.5344],\n",
      "        [  0.0000,   9.1293,  17.5873, 271.7906, 254.1354],\n",
      "        [  0.0000, 161.4463, 166.7661, 236.9798, 226.0746],\n",
      "        [  0.0000,   0.0000,  79.1995, 213.0770, 311.0175]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  84.6259, 203.7202, 116.5548, 256.3976],\n",
      "        [  0.0000,  77.4992,  62.0628, 142.3423, 125.0276],\n",
      "        [  0.0000, 107.3904, 169.2863, 173.3801, 230.4819],\n",
      "        [  0.0000, 144.6235, 239.5037, 219.7866, 319.0000],\n",
      "        [  0.0000,  77.5332,  46.2123, 142.3470, 109.2114],\n",
      "        [  0.0000, 218.4065, 135.5836, 284.7777, 196.5220],\n",
      "        [  0.0000,  61.2956,  46.3951, 126.1911, 109.2730],\n",
      "        [  0.0000, 103.4413, 212.5711, 134.0871, 261.3430]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   0.0000, 123.1181, 189.6412, 319.0000],\n",
      "        [  0.0000,  67.7538,   7.8893, 206.2012, 281.2494],\n",
      "        [  0.0000,   0.0000,  48.6777, 164.7076, 310.0623],\n",
      "        [  0.0000,   0.0000,   6.3319, 131.1766, 319.0000],\n",
      "        [  0.0000,   0.0000,  84.3642, 245.3187, 252.5325],\n",
      "        [  0.0000, 172.2502, 194.7232, 225.2003, 264.3253],\n",
      "        [  0.0000, 159.3429, 101.0204, 212.2754, 172.9318],\n",
      "        [  0.0000, 157.6176, 182.0262, 211.2815, 253.0129]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 219.9283, 156.4351, 272.8032, 234.3163],\n",
      "        [  0.0000, 128.6395,   0.0000, 205.4925, 100.2145],\n",
      "        [  0.0000, 199.2252, 142.0064, 254.0827, 216.7726],\n",
      "        [  0.0000, 151.4967,  16.7791, 207.1425,  90.3812],\n",
      "        [  0.0000, 149.0519,   3.0769, 206.3220,  74.4817],\n",
      "        [  0.0000, 130.6298, 118.4229, 189.7524, 187.4273],\n",
      "        [  0.0000, 142.5983, 125.3363, 177.6188, 183.3698],\n",
      "        [  0.0000, 133.7767, 132.0259, 190.9131, 203.5402]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  39.5433, 212.5444,  94.1898, 292.3042],\n",
      "        [  0.0000,  53.5991,  56.4789, 212.4606, 302.8517],\n",
      "        [  0.0000, 151.9915, 179.1405, 183.1007, 218.8233],\n",
      "        [  0.0000,  77.3988,  94.2486, 204.9464, 319.0000],\n",
      "        [  0.0000,  46.1630,  84.3465, 178.0359, 313.1703],\n",
      "        [  0.0000,  75.3521,  58.8111, 187.0933, 284.8915],\n",
      "        [  0.0000,  51.1739, 104.0611,  84.3204, 151.6334],\n",
      "        [  0.0000,  51.6311, 249.4136,  84.7077, 295.9463]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  61.2891, 184.1619,  97.0636, 242.2018],\n",
      "        [  0.0000, 129.5189, 135.1983, 187.4632, 212.8748],\n",
      "        [  0.0000, 228.6360, 145.9288, 282.7211, 235.6567],\n",
      "        [  0.0000, 183.0292, 251.9119, 234.1065, 319.0000],\n",
      "        [  0.0000, 227.8119, 129.3109, 319.0000, 242.8854],\n",
      "        [  0.0000, 141.2469,   0.0000, 222.1615, 107.2953],\n",
      "        [  0.0000, 125.5360,   0.0000, 206.6173, 106.7745],\n",
      "        [  0.0000, 217.1416, 118.6325, 298.2800, 274.1337]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   0.0000, 108.9241, 195.8079, 319.0000],\n",
      "        [  0.0000,  49.9715,  86.7111, 197.8763, 315.5518],\n",
      "        [  0.0000, 143.9169, 253.9125, 178.1126, 307.1428],\n",
      "        [  0.0000,  32.7620,  57.9774, 245.4614, 319.0000],\n",
      "        [  0.0000, 176.5435, 194.1946, 210.5366, 245.5067],\n",
      "        [  0.0000, 112.0325, 287.3444, 146.2016, 319.0000],\n",
      "        [  0.0000, 161.3774, 195.3422, 194.9673, 244.4644],\n",
      "        [  0.0000,  32.6685,  61.7354, 167.0907, 313.6729]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  44.3365,  85.5590, 104.6497, 165.8805],\n",
      "        [  0.0000, 189.7708, 154.5888, 224.9531, 207.7416],\n",
      "        [  0.0000,  59.3322,  67.7699,  95.6972, 128.4954],\n",
      "        [  0.0000,  44.2837,  71.3210, 105.1844, 148.1138],\n",
      "        [  0.0000, 173.4368, 153.6925, 208.7788, 207.7327],\n",
      "        [  0.0000, 171.2915, 130.3628, 230.9103, 216.7187],\n",
      "        [  0.0000, 171.5488, 141.7032, 229.5154, 237.8638],\n",
      "        [  0.0000, 203.7341, 128.5703, 262.5388, 218.7857]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  18.2247, 118.1665, 162.4056, 283.2985],\n",
      "        [  0.0000,   0.0000, 168.8056, 271.8034, 319.0000],\n",
      "        [  0.0000,  81.9420,  77.3850, 222.4917, 319.0000],\n",
      "        [  0.0000,   0.0000,  72.0470, 209.6862, 252.4260],\n",
      "        [  0.0000,  48.6318, 109.2177, 240.9127, 319.0000],\n",
      "        [  0.0000,   0.0000, 103.3827, 276.0236, 282.4622],\n",
      "        [  0.0000,  39.7738, 141.4777, 319.0000, 311.9152],\n",
      "        [  0.0000,   0.0000, 120.0966, 209.5707, 300.5380]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,   0.0000,  90.8071, 182.1534, 319.0000],\n",
      "        [  0.0000,   0.0000,  11.2952, 166.4617, 319.0000],\n",
      "        [  0.0000,   0.0000,  90.8071, 182.1534, 319.0000],\n",
      "        [  0.0000,   0.0000,  11.2952, 166.4617, 319.0000],\n",
      "        [  0.0000,  36.7929, 120.7127, 174.2491, 319.0000],\n",
      "        [  0.0000,   0.0000,  90.8071, 182.1534, 319.0000],\n",
      "        [  0.0000,   0.0000,  90.8071, 182.1534, 319.0000],\n",
      "        [  0.0000, 171.6808, 167.5489, 207.3615, 215.2795]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 168.9649, 213.2896, 227.4455, 295.0955],\n",
      "        [  0.0000, 234.7476, 213.8644, 270.3347, 259.7516],\n",
      "        [  0.0000, 230.9917, 176.0206, 287.3792, 267.9794],\n",
      "        [  0.0000, 169.7596, 114.6637, 206.1411, 168.5307],\n",
      "        [  0.0000, 170.0523, 132.0238, 206.1808, 183.5568],\n",
      "        [  0.0000, 168.4452, 228.1828, 226.4983, 311.8839],\n",
      "        [  0.0000, 184.1234, 210.8989, 242.5256, 296.1626],\n",
      "        [  0.0000, 184.5538, 195.7013, 243.2290, 279.3276]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 120.2046, 123.4354, 216.8349, 319.0000],\n",
      "        [  0.0000, 104.6090, 107.9079, 201.5180, 308.6787],\n",
      "        [  0.0000,  82.1241, 133.4695, 204.8077, 319.0000],\n",
      "        [  0.0000, 113.8648, 173.4735, 215.3637, 319.0000],\n",
      "        [  0.0000,   0.0000, 112.8956, 115.1620, 262.0734],\n",
      "        [  0.0000, 114.7906,  85.7276, 236.4439, 303.0419],\n",
      "        [  0.0000,  20.6850, 134.7328, 110.8045, 312.8149],\n",
      "        [  0.0000,  12.3342, 115.1405,  96.8693, 282.8422]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 114.9924,  49.4733, 206.3548, 249.5467],\n",
      "        [  0.0000, 100.6963,  65.4767, 190.8960, 259.9017],\n",
      "        [  0.0000, 114.9924,  49.4733, 206.3548, 249.5467],\n",
      "        [  0.0000,  86.6736,  49.3384, 175.7657, 237.7373],\n",
      "        [  0.0000, 114.9924,  49.4733, 206.3548, 249.5467],\n",
      "        [  0.0000, 114.9924,  49.4733, 206.3548, 249.5467],\n",
      "        [  0.0000, 114.9924,  49.4733, 206.3548, 249.5467],\n",
      "        [  0.0000,  85.4470,  97.6096, 174.7556, 288.8038]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  62.9813, 137.8658, 213.8814, 248.4536],\n",
      "        [  0.0000,  46.6706, 119.8208, 200.2591, 229.2503],\n",
      "        [  0.0000,  70.3859, 114.5575, 236.2844, 275.9252],\n",
      "        [  0.0000, 119.4927,  41.0592, 207.9789, 247.5407],\n",
      "        [  0.0000, 103.4277, 114.5706, 267.6104, 274.9051],\n",
      "        [  0.0000,  79.4044, 123.6846, 228.5102, 235.4390],\n",
      "        [  0.0000,  73.4267, 107.3405, 215.2772, 319.0000],\n",
      "        [  0.0000, 104.2635, 137.7807, 192.1904, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 135.4811, 144.2177, 173.6657, 209.1203],\n",
      "        [  0.0000,  64.6623, 245.0884, 118.9418, 311.0582],\n",
      "        [  0.0000, 135.4309, 126.9668, 173.7091, 191.2803],\n",
      "        [  0.0000,  54.2453, 105.8017, 201.6662, 319.0000],\n",
      "        [  0.0000,  71.2593, 255.3355, 109.4995, 319.0000],\n",
      "        [  0.0000,   0.0000,  72.6915, 259.4785, 319.0000],\n",
      "        [  0.0000,  71.3130, 240.3673, 109.4926, 305.4827],\n",
      "        [  0.0000,  65.1207, 262.0140, 118.4536, 319.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 103.3703, 178.1683, 141.4738, 245.1870],\n",
      "        [  0.0000, 111.8480, 180.5920, 167.6199, 246.0697],\n",
      "        [  0.0000, 227.2360, 158.1091, 274.2194, 224.9083],\n",
      "        [  0.0000, 112.1240,  52.9117, 167.5610, 118.3720],\n",
      "        [  0.0000, 111.8860, 197.0846, 166.8272, 263.0546],\n",
      "        [  0.0000, 230.6371, 147.1174, 269.3108, 207.4996],\n",
      "        [  0.0000, 162.1311, 202.8562, 212.1875, 269.0160],\n",
      "        [  0.0000, 210.4067, 155.7587, 259.2878, 222.3433]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000, 183.1333,  62.2493, 220.8763, 126.3713],\n",
      "        [  0.0000, 193.1022, 169.8500, 244.0822, 236.7453],\n",
      "        [  0.0000, 150.9972, 137.6712, 188.9083, 200.2180],\n",
      "        [  0.0000, 151.0331, 202.3226, 188.9921, 265.5023],\n",
      "        [  0.0000, 151.1003, 123.9784, 188.9570, 187.2052],\n",
      "        [  0.0000, 175.6538,  68.8290, 230.9028, 135.0137],\n",
      "        [  0.0000, 198.9839, 168.8582, 236.9004, 231.0551],\n",
      "        [  0.0000, 193.4756, 154.0574, 244.3151, 220.5349]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  53.3612,   9.5551, 203.1537, 299.1857],\n",
      "        [  0.0000, 167.6211, 180.0958, 205.0634, 246.1673],\n",
      "        [  0.0000, 167.4772, 194.3478, 204.9270, 259.8387],\n",
      "        [  0.0000,  53.3612,   9.5551, 203.1537, 299.1857],\n",
      "        [  0.0000,  71.6575,  69.5371, 109.2608, 137.1226],\n",
      "        [  0.0000, 167.4772, 194.3478, 204.9270, 259.8387],\n",
      "        [  0.0000, 167.6211, 180.0958, 205.0634, 246.1673],\n",
      "        [  0.0000,  53.3612,   9.5551, 203.1537, 299.1857]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "rois:  tensor([[  0.0000,  95.1568, 117.5662, 146.1888, 187.8934],\n",
      "        [  0.0000,  78.8082, 114.8147, 132.5464, 183.7991],\n",
      "        [  0.0000, 159.4278, 197.5458, 210.5853, 267.4724],\n",
      "        [  0.0000,  41.8450, 129.8539, 151.4673, 191.3065],\n",
      "        [  0.0000,  47.3594, 221.7739, 130.8353, 319.0000],\n",
      "        [  0.0000, 143.1027, 182.5070, 193.3206, 253.2876],\n",
      "        [  0.0000,  61.2538, 223.2444, 122.1108, 288.8941],\n",
      "        [  0.0000, 143.2217, 197.6924, 194.1326, 268.0225]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "epoch= 0 \t time= 0.3906945029894511 min \t lr= 0.005 \t loss= 22.62091377046373\n"
     ]
    }
   ],
   "source": [
    "# Set initial learning rate and Optimizer\n",
    "init_lr = 0.005\n",
    "EPOCHS = 1\n",
    "\n",
    "# Set training variables\n",
    "running_loss = 0\n",
    "num_batches = 0\n",
    "start = time.time()\n",
    "\n",
    "# Training process\n",
    "for epoch in range(EPOCHS):\n",
    "    # learning rate strategy : divide the learning rate by 1.5 every 10 epochs\n",
    "    if epoch%10==0 and epoch>10: \n",
    "        lr = lr / 1.5\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = init_lr)\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        batch_images, batch_bboxes = data[0], data[1]\n",
    "        batch_images = batch_images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = net(batch_images, batch_bboxes)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', init_lr  ,'\\t loss=', total_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "\n",
    "torch.save(net, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[125.3968,  66.7725, 142.9365,  92.2751,   2.0000],\n",
      "        [111.5079,  94.9206, 128.2540, 119.0476,   1.0000],\n",
      "        [151.5873,  16.8254, 268.0952, 127.6190,   7.0000],\n",
      "        [154.0476, 145.9259, 229.6825, 241.6931,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[ 94.5238, 203.4921, 134.5238, 255.9788,   3.0000],\n",
      "        [141.5079, 171.7460, 177.6984, 220.0000,   2.0000],\n",
      "        [190.7143, 208.5714, 225.9524, 255.9788,   2.0000],\n",
      "        [149.7619, 243.7037, 181.5079, 286.0317,   1.0000],\n",
      "        [ 61.8254,  16.4021, 275.7936, 170.4762,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[ 33.4127, 202.5397,  76.8254, 259.3651,   2.0000],\n",
      "        [147.0635, 185.2910, 190.0794, 242.6455,   3.0000],\n",
      "        [ 79.3651, 200.9524, 115.7143, 252.6984,   2.0000],\n",
      "        [109.4444, 193.1217, 147.8571, 248.7831,   2.0000],\n",
      "        [ 67.6984,  48.2540, 208.0159, 395.2381,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[154.4444,  60.1058, 203.1746, 126.2434,   1.0000],\n",
      "        [178.2540,  66.4550, 226.9841, 132.5926,   1.0000],\n",
      "        [143.9683, 125.9259, 196.5079, 196.9312,   2.0000],\n",
      "        [183.8095, 192.5926, 236.1905, 257.5662,   2.0000],\n",
      "        [205.6349, 124.7619, 270.3968, 208.2540,   3.0000],\n",
      "        [139.8413, 182.2222, 188.8889, 241.6931,   3.0000],\n",
      "        [185.0794, 221.9048, 239.3651, 278.2010,   4.0000],\n",
      "        [105.7143, 145.5026, 165.4762, 224.8677,   4.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[153.5714,  26.8783, 179.1270,  63.7037,   1.0000],\n",
      "        [233.2540,  91.5344, 273.3333, 141.3757,   4.0000],\n",
      "        [137.4603,  76.7196, 301.9841, 319.8942,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[119.9206, 107.9365, 242.9365, 262.6455,   5.0000],\n",
      "        [256.3492, 205.6085, 293.1746, 251.9577,   4.0000],\n",
      "        [188.1746, 211.8518, 210.7143, 243.3862,   1.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[ 96.2698, 117.7778, 228.8889, 265.6085,   5.0000],\n",
      "        [167.6190, 208.9947, 193.7302, 244.7619,   1.0000],\n",
      "        [245.5556, 192.8042, 282.1429, 240.9524,   4.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[143.2540, 124.6561, 183.8095, 173.6508,   2.0000],\n",
      "        [ 59.7619,  35.4497, 191.3492, 411.9577,   7.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[186.7460,  59.3651, 228.9683, 123.5979,   1.0000],\n",
      "        [202.6190,  74.4974, 244.8413, 138.7301,   1.0000],\n",
      "        [222.3016, 135.8730, 268.4127, 213.9683,   3.0000],\n",
      "        [198.6508, 193.5450, 243.5714, 266.1376,   2.0000],\n",
      "        [173.1746, 117.5661, 216.1111, 189.9471,   2.0000],\n",
      "        [153.0159, 180.8466, 207.1429, 244.7619,   3.0000],\n",
      "        [195.0000, 232.4868, 243.7302, 289.5238,   4.0000],\n",
      "        [123.4127, 128.3598, 185.7936, 215.8730,   4.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[  3.3333, 111.7460, 108.1746, 370.7936,   7.0000],\n",
      "        [119.4444, 144.6561, 200.0794, 360.7407,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[ 99.8413, 115.6614, 138.6508, 164.6561,   1.0000],\n",
      "        [ 72.3016, 210.8995, 115.7143, 265.5027,   2.0000],\n",
      "        [147.4603, 195.3439, 197.1429, 262.2222,   4.0000],\n",
      "        [ 78.5714, 306.7725, 123.2540, 364.1270,   3.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[112.6190, 162.9630, 138.7301, 198.0952,   1.0000],\n",
      "        [ 91.5079,  52.1693, 207.6190, 246.9841,   5.0000],\n",
      "        [131.1905, 257.7778, 162.9365, 298.2010,   4.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[ 55.1587, 241.5873,  94.8413, 295.0265,   2.0000],\n",
      "        [ 71.4286, 301.6931, 114.8413, 362.9630,   3.0000],\n",
      "        [ 15.0794, 332.3810,  57.6190, 391.4286,   1.0000],\n",
      "        [ 30.4762,  19.3651, 239.9206, 306.1376,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[130.4762, 224.8677, 156.8254, 265.5027,   1.0000],\n",
      "        [103.0159, 178.8360, 132.4603, 224.8677,   3.0000],\n",
      "        [167.2222,  65.6085, 207.1429, 120.6349,   4.0000],\n",
      "        [ 73.9683,  12.1693, 259.7619, 319.8942,   7.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[123.2540, 120.7407, 172.0635, 185.2910,   1.0000],\n",
      "        [ 33.4127, 191.9577,  91.9048, 267.1958,   2.0000],\n",
      "        [144.9206, 255.4497, 200.4762, 330.6878,   2.0000]])]\n",
      "n_rois:  1244\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[166.1905,  92.0635, 210.0000, 148.5714,   3.0000],\n",
      "        [203.4921, 143.0688, 255.6349, 208.9947,   4.0000],\n",
      "        [ 68.9683,  38.2011, 255.6349, 275.3439,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[183.0952, 134.6032, 218.8095, 182.7513,   3.0000],\n",
      "        [177.1429, 229.1005, 212.1429, 275.1323,   2.0000],\n",
      "        [ 68.5714,  86.5609, 205.1587, 262.5397,   5.0000],\n",
      "        [129.3651,   1.5873, 316.2698, 234.3915,   7.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[105.9524,  74.7090, 152.6190, 137.2487,   1.0000],\n",
      "        [227.8571,  39.8942, 273.0952, 110.5820,   3.0000],\n",
      "        [130.3968, 158.8360, 179.8413, 231.5344,   2.0000],\n",
      "        [236.7460, 167.3016, 280.3174, 236.0847,   3.0000],\n",
      "        [172.6984, 202.0106, 219.7619, 273.4391,   2.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[215.9524,  58.6243, 246.2698,  92.8042,   3.0000],\n",
      "        [231.2698, 178.6243, 261.1905, 218.5185,   1.0000],\n",
      "        [232.9365, 226.0317, 274.0476, 278.4127,   4.0000],\n",
      "        [155.1587,  36.7196, 301.6667, 295.7672,   7.0000],\n",
      "        [ 78.8889,  71.1111, 194.5238, 246.2434,   5.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[142.3810, 184.8677, 168.1746, 221.1640,   1.0000],\n",
      "        [181.8254, 147.8307, 221.7460, 200.3175,   3.0000],\n",
      "        [ 89.8413, 113.9683, 208.4127, 306.8783,   5.0000],\n",
      "        [169.4444, 109.8413, 319.9206, 292.2751,   7.0000]])]\n",
      "n_rois:  1252\n",
      "predictions:  {'10c': [array([], shape=(0, 1), dtype=float32)], '20c': [array([], shape=(0, 1), dtype=float32)], '50c': [array([], shape=(0, 1), dtype=float32)], '$1': [array([], shape=(0, 1), dtype=float32)], '$2': [array([], shape=(0, 1), dtype=float32)], '$5': [array([], shape=(0, 1), dtype=float32)], '$10': [array([], shape=(0, 1), dtype=float32)]}\n",
      "batch_bbox:  [tensor([[179.1270, 132.6984, 233.2540, 206.7725,   3.0000],\n",
      "        [145.3968, 209.9471, 199.5238, 283.7037,   4.0000],\n",
      "        [ 59.7619,  18.2011, 284.5238, 293.6508,   5.0000]])]\n",
      "test error  =  0.0 percent\n"
     ]
    }
   ],
   "source": [
    "<<<<<<< local\n",
    "def eval_on_test_set(test_loader):\n",
    "    # net = faster_R_CNN()\n",
    "    # net.to(device)\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    TEST_NMS = 0.3\n",
    "    thresh = 0.3\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    for data in test_loader:\n",
    "        with torch.no_grad():\n",
    "            batch_images, batch_bboxes = data[0], data[1]\n",
    "            batch_images = batch_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cls_score, cls_pred, cls_prob, bbox_pred, rois = net(batch_images, batch_bboxes, train_flag=False)\n",
    "\n",
    "            boxes = rois[:, 1:5]\n",
    "            n_anchors = int(bbox_pred.shape[0] / (num_of_class * 4))\n",
    "            scores = cls_score.view(n_anchors, -1).cpu()\n",
    "            box_deltas = bbox_pred.view(n_anchors, -1)\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas)\n",
    "            pred_boxes = clip_boxes(pred_boxes, [320,320])\n",
    "            predictions = {}\n",
    "            # error = utils.get_error( scores , minibatch_label)\n",
    "            # skip j = 0, because it's the background class\n",
    "            for j in range(1, num_of_class):\n",
    "                inds = np.where(scores[:, j] > thresh)[0]\n",
    "                cls_scores = scores[inds, j].cpu()\n",
    "                cls_boxes = boxes[inds, j * 4:(j + 1) * 4].cpu()\n",
    "                cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n",
    "                .astype(np.float32, copy=False)\n",
    "                keep = nms(\n",
    "                    torch.from_numpy(cls_boxes), torch.from_numpy(cls_scores),\n",
    "                    TEST_NMS).numpy() if cls_dets.size > 0 else []\n",
    "                cls_dets = cls_dets[keep, :]\n",
    "                label = index_label_dict[j]\n",
    "                if label in predictions.keys():\n",
    "                    predictions[label].append(cls_dets)\n",
    "                else:\n",
    "                    predictions[label] = [cls_dets]\n",
    "            print(\"predictions: \", predictions)\n",
    "            print(\"batch_bbox: \", batch_bboxes)\n",
    "            # running_error += error.item()\n",
    "\n",
    "            num_batches+=1\n",
    "\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'test error  = ', total_error*100 ,'percent')\n",
    "device = torch.device('cuda')\n",
    "net = torch.load(\"model\")\n",
    "net.to(device)\n",
    "=======\n",
    "def eval_on_test_set(test_loader):\n",
    "    # net = faster_R_CNN()\n",
    "    # net.to(device)\n",
    "    device = torch.device('cuda')\n",
    "    net = torch.load(\"model\")\n",
    "    net.to(device)\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    TEST_NMS = 0.3\n",
    "    thresh = 0.3\n",
    "\n",
    "    for data in test_loader:\n",
    "        with torch.no_grad():\n",
    "            batch_images, batch_bboxes = data[0], data[1]\n",
    "            batch_images = batch_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cls_score, cls_pred, cls_prob, bbox_pred = net(batch_images, batch_bboxes, train_flag=False)\n",
    "\n",
    "            n_anchors = int(bbox_pred.shape[0] / (num_of_class * 4))\n",
    "            scores = cls_score.view(n_anchors, -1).cpu().numpy()\n",
    "            boxes = bbox_pred.view(n_anchors, -1).cpu().numpy()\n",
    "            predictions = {}\n",
    "            # error = utils.get_error( scores , minibatch_label)\n",
    "            # skip j = 0, because it's the background class\n",
    "            for j in range(1, num_of_class):\n",
    "                inds = np.where(scores[:, j] > thresh)[0]\n",
    "                cls_scores = scores[inds, j]\n",
    "                cls_boxes = boxes[inds, j * 4:(j + 1) * 4]\n",
    "                cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n",
    "                .astype(np.float32, copy=False)\n",
    "                keep = nms(\n",
    "                    torch.from_numpy(cls_boxes), torch.from_numpy(cls_scores),\n",
    "                    TEST_NMS).numpy() if cls_dets.size > 0 else []\n",
    "                cls_dets = cls_dets[keep, :]\n",
    "                label = index_label_dict[j]\n",
    "                if label in predictions.keys():\n",
    "                    predictions[label].append(cls_dets)\n",
    "                else:\n",
    "                    predictions[label] = [cls_dets]\n",
    "            print(\"predictions: \", predictions)\n",
    "            print(\"batch_bbox: \", batch_bboxes)\n",
    "            # running_error += error.item()\n",
    "\n",
    "            num_batches+=1\n",
    "\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'test error  = ', total_error*100 ,'percent')\n",
    "\n",
    ">>>>>>> remote\n",
    "eval_on_test_set(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb3f132b8f46ef83ff81b7067b37036dbb49ec9e6fa8b80a12ce6c1c87b906f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
