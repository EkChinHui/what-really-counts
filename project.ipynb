{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Really Counts: A Cash Recognization System\n",
    "\n",
    "---\n",
    "\n",
    "**Group Members**:\n",
    "- Ek Chin Hui ()\n",
    "- Lee Hyung Woon (A0218475X)\n",
    "- Toh Hai Jie Joey (A0205141Y)\n",
    "\n",
    "---\n",
    "\n",
    "Our project, named **What Really Counts**, is a Cash Recognization System for the Visually Impaired in Singapore. In Singapore, the disabled community face many challenges in their daily lives, and this is especially so for those who are hampered by visual impairments. One such challenge they face is cash payment, as they need to identify the correct combinations of bills and coins. Hence, our aim was to contruct a system that can help them overcome these challenges by employing a deep learning-based Object Detection model using Convolutional Neural Networks (CNN) - in particular, the Faster R-CNN model. \n",
    "\n",
    "**What Really Counts** is an architecture that detects and analyzes given images of Singapore Currencies (bills and/or coins), and is primarily designed to assist the visually impaired in identifying the correct combinations of bills and coins. The model uses CNNs to perform image classification on the objects detected in a given input image, through which we can ascertain the exact number and type of bills / coins present in the image, allowing us to calculate and return the sum of the currency to the user.\n",
    "\n",
    "For this project, we will gather and pre-process our own dataset, and then move onto training and testing of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used\n",
    "\n",
    "The following are modules that we used for this project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection & Preprocessing\n",
    "\n",
    "For this project, we collected data by taking pictures...\n",
    "\n",
    "As an initial proof of concept, we decided to focus on the 7 most common classes of currencies in Singapore: $10, $5, $2, $1, 50c, 20c, 10c...\n",
    "\n",
    "After we gathered the data, we did data cleaning, augmentation, labelling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10c': 1, '20c': 2, '50c': 3, '$1': 4, '$2': 5, '$5': 6, '$10': 7, 'background': 0}\n",
      "{1: '10c', 2: '20c', 3: '50c', 4: '$1', 5: '$2', 6: '$5', 7: '$10', 0: 'background'}\n"
     ]
    }
   ],
   "source": [
    "# Set class labels\n",
    "\n",
    "coin_labels = ('10c', '20c', '50c', '$1', '$2', '$5', '$10')\n",
    "\n",
    "label_index_dict = {k:v+1 for v, k in enumerate(coin_labels)}\n",
    "label_index_dict['background'] = 0\n",
    "print(label_index_dict)\n",
    "\n",
    "index_label_dict = {v+1:k for v, k in enumerate(coin_labels)}\n",
    "index_label_dict[0] = 'background'\n",
    "print(index_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"\n",
    "    Function to convert XML data of a single image into an object.\n",
    "    The object contains Bbox parameters and the corresponding label for each Bbox.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the XML file into a tree structure\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    height = float(root.find('size').find('height').text)\n",
    "    width = float(root.find('size').find('width').text)\n",
    "\n",
    "    # Set initial lists\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    # difficulties = list()\n",
    "\n",
    "    # Loop over each Bbox found in the XML file\n",
    "    for object in root.iter('object'):\n",
    "        # Convert Bbox co-ordinates\n",
    "        bbox = object.find('bndbox')\n",
    "        # xmin = int(bbox.find('xmin').text) - 1\n",
    "        # ymin = int(bbox.find('ymin').text) - 1\n",
    "        # xmax = int(bbox.find('xmax').text) - 1\n",
    "        # ymax = int(bbox.find('ymax').text) - 1\n",
    "        xmin = float(bbox.find('xmin').text)/width\n",
    "        ymin = float(bbox.find('ymin').text)/height\n",
    "        xmax = float(bbox.find('xmax').text)/width\n",
    "        ymax = float(bbox.find('ymax').text)/height\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert Bbox Label\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_index_dict:\n",
    "            continue\n",
    "        labels.append(label_index_dict[label])\n",
    "\n",
    "        # Convert Bbox Difficulty\n",
    "        # difficult = int(object.find('difficult').text == '1')\n",
    "        # difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "def xml_to_json(files):\n",
    "    \"\"\"\n",
    "    Function to convert image and XML data into two separate JSON objects.\n",
    "    One object for images, and another object for Bbox co-ordinate values and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise lists\n",
    "    images_list = [] \n",
    "    objects_list = []\n",
    "    # files = [file for sublist in files for file in sublist]\n",
    "    # Set up two JSON files to be written\n",
    "    images_file = open(\"TRAIN_images.json\", 'w')\n",
    "    objects_file = open(\"TRAIN_objects.json\", 'w')\n",
    "    \n",
    "    # Iterate through each XML-Image pair\n",
    "    for file in files:\n",
    "    \n",
    "        # Add each image file path into the images list\n",
    "        file_path = os.path.splitext(file)[0]   \n",
    "        images_list.append(file_path + \".jpg\")\n",
    "        \n",
    "        # Add each XML object into the objects list\n",
    "        xml_dict = parse_annotation(file)\n",
    "        objects_list.append(xml_dict)\n",
    "    \n",
    "    # Write each list into the corresponding JSON files\n",
    "    json.dump(images_list, images_file)\n",
    "    json.dump(objects_list, objects_file)\n",
    "\n",
    "CH_FILES = glob(r'dataset/ch dataset/*.xml')\n",
    "JY_FILES = glob(r'dataset/joey dataset/*.xml')\n",
    "HW_FILES = glob(r'dataset/hw dataset/*.xml')\n",
    "# xml_to_json(CH_FILES, HW_FILES)\n",
    "xml_to_json(CH_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, boxes, dims = (320, 320), return_percent_coords = False):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Resize image. For the SSD300, resize to (300, 300).\n",
    "    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n",
    "    you may choose to retain them.\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resize image\n",
    "    new_image = FT.resize(image, dims)\n",
    "\n",
    "    # Resize Bboxes\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
    "    new_boxes = boxes / old_dims  # percent coordinates\n",
    "\n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, boxes, labels):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    Apply the transformations above.\n",
    "    :param image: image, a PIL Image\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
    "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
    "    \"\"\"\n",
    "\n",
    "    # Mean and Standard deviation used for the base VGG from torchvision\n",
    "    # See: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "\n",
    "    # Resize image - this also converts absolute boundary coordinates to their fractional form\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims = (320, 320))\n",
    "\n",
    "    # Convert PIL image to Torch tensor\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "\n",
    "    # Normalize by mean and standard deviation of ImageNet data that the base torchvision VGG was trained on\n",
    "    new_image = FT.normalize(new_image, mean = mean, std = std)\n",
    "\n",
    "    return new_image, new_boxes, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader.\n",
    "    This class is primarily used to create batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_folder, split):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param split: split, one of 'TRAIN' or 'TEST'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.split = split.upper()\n",
    "        assert self.split in {'TRAIN', 'TEST'}\n",
    "\n",
    "        # Read from the JSON files\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        # Number of images must match the number of objects containing the Bboxes for each image\n",
    "        assert len(self.images) == len(self.objects)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # Read image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensor = transforms.Resize(size = (320, 320))(image_tensor)\n",
    "\n",
    "        # Read objects in this image (Bboxes, labels)\n",
    "        objects = self.objects[i]\n",
    "        box = torch.FloatTensor(objects['boxes']) # (n_objects, 4xy values and 1 )\n",
    "        label = torch.LongTensor(objects['labels']) # (n_objects)\n",
    "\n",
    "        # Apply transformations\n",
    "        image, box, label = transform(image, box, label)\n",
    "        box_and_label = torch.cat([box, label.unsqueeze(1)], 1)\n",
    "        return image_tensor, box_and_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "\n",
    "        images = torch.stack(images, dim = 0)\n",
    "        return images, boxes  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import math\n",
    "\n",
    "color_maps = ['r', 'g', 'b', 'y', 'm', 'w', 'k', 'c']\n",
    "bs = 1\n",
    "dataset = PascalVOCDataset(\".\", \"TRAIN\")\n",
    "length = len(dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [math.floor(0.8*length), math.ceil(0.2*length)])\n",
    "train_dataloader = DataLoader(train_data, batch_size = bs, shuffle = True, collate_fn = dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size = bs, shuffle = True, collate_fn = dataset.collate_fn)\n",
    "\n",
    "\n",
    "# for data in train_dataloader:\n",
    "#     print(data)\n",
    "#     break\n",
    "# TODO: Currently, the object itself is being resized. \n",
    "# Need to instead use the resize() method to resize the bounding boxes instead?\n",
    "# This should ideally be done in the Faster R-CNN network itself\n",
    "# We'll need to ROI-Pool both the Bounding box Coordinates and the ROI itself\n",
    "\n",
    "# Visualise data\n",
    "# for data in train_dataloader:\n",
    "#     count = 1\n",
    "#     # if count == 1:\n",
    "#     #     print(data[0])\n",
    "#     #     print(data[0].size())\n",
    "#     #     print(data[0].type())\n",
    "#     count = count + 1\n",
    "#     for batch in range(bs):\n",
    "#         img = data[0][batch]\n",
    "#         boxes = data[1][batch]\n",
    "# #         labels = data[2][batch].tolist()\n",
    "# #         named_labels = [index_label_dict[label] for label in labels]\n",
    "#         plt.imshow(transforms.ToPILImage()(img))\n",
    "#         ax = plt.gca()\n",
    "#         labelled = set()\n",
    "#         for i, box in enumerate(boxes):\n",
    "#             w,h = box[2] - box[0], box[3] - box[1]\n",
    "#             x,y = box[0].item(), box[1].item()\n",
    "#             x = [x, x + w, x + w, x, x]\n",
    "#             y = [y, y, y + h, y + h, y]\n",
    "#             label = int(box[4].item())\n",
    "#             if label not in labelled:\n",
    "#                 plt.plot(x,y, color = color_maps[label], label = index_label_dict[label])\n",
    "#                 labelled.add(label)\n",
    "#             else:\n",
    "#                 plt.plot(x,y, color = color_maps[label])\n",
    "#             plt.legend(loc = 'best')\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n",
    "BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n",
    "BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "def pascal2yolo(anchor):\n",
    "    \"\"\"\n",
    "    Transforms anchor coordinates of the form [xmin, ymin, xmax, ymax] to [x_center, y_center, width, height]\n",
    "    \"\"\"\n",
    "    # TODO: +1? for width and height\n",
    "    width = anchor[2] - anchor[0] + 1\n",
    "    height = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + (width-1)/2 \n",
    "    y_ctr = anchor[1] + (height-1)/2\n",
    "    return width, height, x_ctr, y_ctr\n",
    "\n",
    "def yolo2pascal(width, height, x_ctr, y_ctr):\n",
    "    \"\"\"\n",
    "    Transforms anchor coordinates of the form [x_center, y_center, width, height] to [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    width = width[:, np.newaxis]\n",
    "    height = height[:, np.newaxis]\n",
    "    anchors = np.hstack((x_ctr - 0.5 * (width - 1), y_ctr - 0.5 * (height - 1),\n",
    "                         x_ctr + 0.5 * (width - 1), y_ctr + 0.5 * (height - 1)))\n",
    "    return anchors\n",
    "\n",
    "def generate_ratio_anchors(anchor, ratios=(0.5,1,2)):\n",
    "    \"\"\"\n",
    "    Generate anchors based on given width:height ratio\n",
    "    \"\"\"\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor)\n",
    "    size = w*h\n",
    "    size_ratios = size / ratios\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    hs = np.round(ws * ratios)\n",
    "    anchors = yolo2pascal(ws, hs, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_scale_anchors(anchor, scales=np.array((8,16,32))):\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor) \n",
    "    scaled_w = w * scales\n",
    "    scaled_h = h * scales\n",
    "    anchors = yolo2pascal(scaled_w, scaled_h, x_ctr, y_ctr)\n",
    "    return anchors\n",
    "\n",
    "def generate_anchors(height, width, aspect_ratio=np.array((0.5,1,2)), stride_length=16, scales=np.array((8,16,32))):\n",
    "    # Generate anchors of differing scale and aspect ratios\n",
    "    # return anchors and anchor length\n",
    "    base_anchor = pascal2yolo([0,0,15,15]) # 16, 16, 7.5, 7.5\n",
    "    ratio_anchors = generate_ratio_anchors(base_anchor, ratios=aspect_ratio)\n",
    "    anchors = np.vstack([\n",
    "        generate_scale_anchors(ratio_anchors[i, :], scales)\n",
    "        for i in range(ratio_anchors.shape[0])\n",
    "    ])\n",
    "    A = anchors.shape[0]\n",
    "    shift_x = np.arange(0, width) * stride_length\n",
    "    shift_y = np.arange(0, height) * stride_length\n",
    "    # Shift each ratio and \n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n",
    "                        shift_y.ravel())).transpose()\n",
    "    K = shifts.shape[0]\n",
    "    # width changes faster, so here it is H, W, C\n",
    "    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n",
    "    length = np.int32(anchors.shape[0])\n",
    "\n",
    "    return anchors, length\n",
    "\n",
    "def bbox_overlaps(boxes, query_boxes):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: (N, 4) ndarray or tensor or variable\n",
    "    query_boxes: (K, 4) ndarray or tensor or variable\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, np.ndarray):\n",
    "        boxes = torch.from_numpy(boxes)\n",
    "        query_boxes = torch.from_numpy(query_boxes)\n",
    "        out_fn = lambda x: x.numpy()  # If input is ndarray, turn the overlaps back to ndarray when return\n",
    "    else:\n",
    "        out_fn = lambda x: x\n",
    "\n",
    "    boxes = boxes.to(device)\n",
    "    query_boxes = query_boxes.to(device)\n",
    "    box_areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    query_areas = (query_boxes[:, 2] - query_boxes[:, 0] + 1) * (query_boxes[:, 3] - query_boxes[:, 1] + 1)\n",
    "\n",
    "    iw = (torch.min(boxes[:, 2:3], query_boxes[:, 2:3].t()) - torch.max(boxes[:, 0:1], query_boxes[:, 0:1].t()) + 1).clamp(min=0)\n",
    "    ih = (torch.min(boxes[:, 3:4], query_boxes[:, 3:4].t()) - torch.max(boxes[:, 1:2], query_boxes[:, 1:2].t()) + 1).clamp(min=0)\n",
    "    ua = box_areas.view(-1, 1) + query_areas.view(1, -1) - iw * ih\n",
    "    overlaps = iw * ih / ua\n",
    "    return out_fn(overlaps.cpu())\n",
    "\n",
    "def bbox_transform(ex_rois, gt_rois):\n",
    "    ex_rois = ex_rois.to(device)\n",
    "    gt_rois = gt_rois.to(device)\n",
    "    \n",
    "    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n",
    "    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n",
    "\n",
    "    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n",
    "    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "    targets_dw = torch.log(gt_widths / ex_widths)\n",
    "    targets_dh = torch.log(gt_heights / ex_heights)\n",
    "\n",
    "    targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), 1)\n",
    "    return targets.cpu()\n",
    "\n",
    "def compute_targets_atl(ex_rois, gt_rois):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "    \n",
    "    gt_rois_np = gt_rois.numpy()\n",
    "    targets = bbox_transform(ex_rois, torch.from_numpy(gt_rois_np[:, :4])).numpy()\n",
    "    return targets\n",
    "\n",
    "def compute_targets_ptl(ex_rois, gt_rois, labels):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    targets = bbox_transform(ex_rois, gt_rois)\n",
    "    # Normalize targets by a precomputed mean and stdev (optional)\n",
    "    # targets = ((targets - targets.new(BBOX_NORMALIZE_MEANS)) / targets.new(BBOX_NORMALIZE_STDS))\n",
    "    return torch.cat([labels.unsqueeze(1), targets], 1)\n",
    "\n",
    "def _get_bbox_regression_labels(bbox_target_data, num_classes):\n",
    "    \"\"\"\n",
    "    Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "    compact form N x (class, tx, ty, tw, th)\n",
    "    This function expands those targets into the 4-of-4*K representation used\n",
    "    by the network (i.e. only one class has non-zero targets).\n",
    "    Returns:\n",
    "        bbox_target (ndarray): N x 4K blob of regression targets\n",
    "        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n",
    "    \"\"\"\n",
    "    # Inputs are tensor\n",
    "\n",
    "    clss = bbox_target_data[:, 0]\n",
    "    bbox_targets = clss.new_zeros(clss.numel(), 4 * num_classes)\n",
    "    bbox_inside_weights = clss.new_zeros(bbox_targets.shape)\n",
    "    inds = (clss > 0).nonzero().view(-1)\n",
    "    if inds.numel() > 0:\n",
    "        clss = clss[inds].contiguous().view(-1, 1)\n",
    "        dim1_inds = inds.unsqueeze(1).expand(inds.size(0), 4)\n",
    "        dim2_inds = torch.cat([4 * clss, 4 * clss + 1, 4 * clss + 2, 4 * clss + 3], 1).long()\n",
    "        test = bbox_target_data[inds][:, 1:]\n",
    "        bbox_targets[dim1_inds, dim2_inds] = test\n",
    "        bbox_inside_weights[dim1_inds, dim2_inds] = bbox_targets.new(BBOX_INSIDE_WEIGHTS).view(-1, 4).expand_as(dim1_inds)\n",
    "    # return None\n",
    "    return bbox_targets, bbox_inside_weights\n",
    "\n",
    "def bbox_transform_inv(boxes, deltas):\n",
    "    # Input should be both tensor or both Variable and on the same device\n",
    "    if len(boxes) == 0:\n",
    "        return deltas.detach() * 0\n",
    "    boxes = boxes.to(device)\n",
    "    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "    dx = deltas[:, 0::4]\n",
    "    dy = deltas[:, 1::4]\n",
    "    dw = deltas[:, 2::4]\n",
    "    dh = deltas[:, 3::4]\n",
    "\n",
    "    pred_ctr_x = dx * widths.unsqueeze(1) + ctr_x.unsqueeze(1)\n",
    "    pred_ctr_y = dy * heights.unsqueeze(1) + ctr_y.unsqueeze(1)\n",
    "    pred_w = torch.exp(dw) * widths.unsqueeze(1)\n",
    "    pred_h = torch.exp(dh) * heights.unsqueeze(1)\n",
    "\n",
    "    pred_boxes = torch.cat(\\\n",
    "      [_.unsqueeze(2) for _ in [pred_ctr_x - 0.5 * pred_w,\\\n",
    "                                pred_ctr_y - 0.5 * pred_h,\\\n",
    "                                pred_ctr_x + 0.5 * pred_w,\\\n",
    "                                pred_ctr_y + 0.5 * pred_h]], 2).view(len(boxes), -1)\n",
    "\n",
    "    return pred_boxes\n",
    "\n",
    "def clip_boxes(boxes, im_shape):\n",
    "    \"\"\"\n",
    "  Clip boxes to image boundaries.\n",
    "  boxes must be tensor or Variable, im_shape can be anything but Variable\n",
    "  \"\"\"\n",
    "\n",
    "    if not hasattr(boxes, 'data'):\n",
    "        boxes_ = boxes.numpy()\n",
    "\n",
    "    boxes = boxes.view(boxes.size(0), -1, 4)\n",
    "    boxes = torch.stack(\\\n",
    "      [boxes[:,:,0].clamp(0, im_shape[1] - 1),\n",
    "       boxes[:,:,1].clamp(0, im_shape[0] - 1),\n",
    "       boxes[:,:,2].clamp(0, im_shape[1] - 1),\n",
    "       boxes[:,:,3].clamp(0, im_shape[0] - 1)], 2).view(boxes.size(0), -1)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def clip_boxes_batch(boxes, im_shape, batch_size):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    \"\"\"\n",
    "    num_rois = boxes.size(1)\n",
    "\n",
    "    boxes[boxes < 0] = 0\n",
    "    # batch_x = (im_shape[:,0]-1).view(batch_size, 1).expand(batch_size, num_rois)\n",
    "    # batch_y = (im_shape[:,1]-1).view(batch_size, 1).expand(batch_size, num_rois)\n",
    "\n",
    "    batch_x = im_shape[:, 1] - 1\n",
    "    batch_y = im_shape[:, 0] - 1\n",
    "\n",
    "    boxes[:,:,0][boxes[:,:,0] > batch_x] = batch_x\n",
    "    boxes[:,:,1][boxes[:,:,1] > batch_y] = batch_y\n",
    "    boxes[:,:,2][boxes[:,:,2] > batch_x] = batch_x\n",
    "    boxes[:,:,3][boxes[:,:,3] > batch_y] = batch_y\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def fix_sample_regions(fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image):\n",
    "    # Small modification to the original version where we ensure a fixed number of regions are sampled\n",
    "    if fg_inds.numel() == 0 and bg_inds.numel() == 0:\n",
    "        to_replace = all_rois.size(0) < rois_per_image\n",
    "        bg_inds = torch.from_numpy(npr.choice(np.arange(0, all_rois.size(0)), size=int(rois_per_image), replace=to_replace)).long()\n",
    "        fg_rois_per_image = 0\n",
    "    elif fg_inds.numel() > 0 and bg_inds.numel() > 0:\n",
    "        fg_rois_per_image = min(fg_rois_per_image, fg_inds.numel())\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(fg_rois_per_image),\n",
    "                replace=False)).long().to(gt_boxes.device)]\n",
    "        bg_rois_per_image = rois_per_image - fg_rois_per_image\n",
    "        to_replace = bg_inds.numel() < bg_rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(bg_rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "    elif fg_inds.numel() > 0:\n",
    "        to_replace = fg_inds.numel() < rois_per_image\n",
    "        fg_inds = fg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, fg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = rois_per_image\n",
    "    elif bg_inds.numel() > 0:\n",
    "        to_replace = bg_inds.numel() < rois_per_image\n",
    "        bg_inds = bg_inds[torch.from_numpy(\n",
    "            npr.choice(\n",
    "                np.arange(0, bg_inds.numel()),\n",
    "                size=int(rois_per_image),\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\n",
    "        fg_rois_per_image = 0\n",
    "\n",
    "    return fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image\n",
    "\n",
    "def unmap(data, count, inds, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of\n",
    "  size count) \"\"\"\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count, ), dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds] = data\n",
    "    else:\n",
    "        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\n",
    "        ret.fill(fill)\n",
    "        ret[inds, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms, RoIAlign\n",
    "\n",
    "hidden_dim = 64\n",
    "nb_objects = 3 # TODO: How about cases where images have different number of objects?\n",
    "num_of_class = 8\n",
    "\n",
    "im_size = 320\n",
    "bs = 1\n",
    "\n",
    "# Thresholds\n",
    "FG_THRESH = 0.5 # Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n",
    "# Overlap threshold for a ROI to be considered background (class = 0 if overlap in [LO, HI))\n",
    "BG_THRESH_HI = 0.5\n",
    "BG_THRESH_LO = 0.1\n",
    "\n",
    "PRE_NMS_TOPN = 12000\n",
    "POST_NMS_TOPN = 2000\n",
    "NMS_THRESH = 0.7\n",
    "\n",
    "POSITIVE_OVERLAP = 0.7\n",
    "NEGATIVE_OVERLAP = 0.3\n",
    "CLOBBER_POSITIVES = False\n",
    "RPN_BS = 16\n",
    "FG_FRACTION = 0.5\n",
    "RPN_POSITIVE_WEIGHT = -1.0\n",
    "POOLING_SIZE = 7\n",
    "\n",
    "class faster_R_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The main Faster R-CNN network used for this project.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(faster_R_CNN, self).__init__()\n",
    "        self.feat_stride = [16,]\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        self._layers = {}\n",
    "        self._act_summaries = {}\n",
    "        self._score_summaries = {}\n",
    "        self._event_summaries = {}\n",
    "        self._image_gt_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "        self._fc_channels = RPN_BS*RPN_BS*25\n",
    "        self._net_conv_channels = 1024\n",
    "\n",
    "        # This results in num_anchors = 9\n",
    "        anchor_scales = (8, 16, 32)\n",
    "        anchor_ratios = (0.5, 1, 2)\n",
    "        self.n_anchors = len(anchor_scales) * len(anchor_ratios)  \n",
    "        \n",
    "        # HeadNet: Generating a series of Feature maps from the input image\n",
    "        \n",
    "        # Current Size: 3 x h x w\n",
    "        self.head_conv1 = nn.Conv2d(3,  hidden_dim,  kernel_size=4, stride=2, padding=1)\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_batch_norm1 = nn.BatchNorm2d(hidden_dim)\n",
    "        # Current Size: 64 x h/2 x w/2 \n",
    "        self.head_relu1 = nn.ReLU()\n",
    "        # Current Size: 64 x h/2 x w/2\n",
    "        self.head_pool1 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        # Current Size: 64 x h/4 x w/4\n",
    "        self.head_layer1 = nn.Conv2d(hidden_dim,  hidden_dim*4,  kernel_size=3, padding=1)\n",
    "        self.head_relu2 = nn.ReLU()\n",
    "        # Current Size: 256 x h/4 x w/4\n",
    "        self.head_layer2 = nn.Conv2d(hidden_dim*4,  hidden_dim*8,  kernel_size=3, padding=1)\n",
    "        self.head_pool2 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        self.head_relu3 = nn.ReLU()\n",
    "        # Current Size: 512 x h/8 x w/8\n",
    "        self.head_layer3 = nn.Conv2d(hidden_dim*8,  hidden_dim*16,  kernel_size=3, padding=1)\n",
    "        self.head_pool3 = nn.MaxPool2d([3,3], padding=1, stride=2)\n",
    "        self.head_relu4 = nn.ReLU()\n",
    "        # Current Size: 1024 x h/16 x w/16\n",
    "        \n",
    "        # Region Proposal Network\n",
    "        self.rpn_net = nn.Conv2d(self._net_conv_channels, 512 , kernel_size=3, padding=1)\n",
    "        self.rpn_cls_score_net = nn.Conv2d(512, self.n_anchors*2, [1,1])\n",
    "        self.rpn_bbox_pred_net = nn.Conv2d(512, self.n_anchors*4, [1,1])\n",
    "\n",
    "        # Classification Network\n",
    "        # [256, 256, 5, 5]\n",
    "        self.cls_score_net = nn.Linear(self._fc_channels, RPN_BS*num_of_class)     \n",
    "        self.bbox_pred_net = nn.Linear(self._fc_channels, RPN_BS*num_of_class*4)\n",
    "\n",
    "    def head_net(self):\n",
    "        return nn.Sequential(\n",
    "            self.head_conv1,\n",
    "            self.head_batch_norm1,\n",
    "            self.head_relu1,\n",
    "            self.head_pool1,\n",
    "            self.head_layer1,\n",
    "            self.head_relu2,\n",
    "            self.head_layer2,\n",
    "            self.head_pool2,\n",
    "            self.head_relu3,\n",
    "            self.head_layer3,\n",
    "            self.head_pool3,\n",
    "            self.head_relu4\n",
    "        )\n",
    "\n",
    "    def fc7(self):\n",
    "        return nn.Sequential(\n",
    "            # Current Size: n x 1024 x 7 x 7\n",
    "            # nn.Conv2d(self._predictions[\"rois\"].size(0), self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1024, self._predictions[\"rois\"].size(0),  kernel_size=3, padding=1),\n",
    "            nn.AvgPool2d([3,3], 1)\n",
    "            # Current Size: n x 4096 x 3 x 3\n",
    "        ) # 256\n",
    "        \n",
    "\n",
    "    def proposal_layer(self, cls_prob, bbox_pred, anchors, n_anchors):\n",
    "        '''\n",
    "        Prunes no. of boxes using NMS based on fg scores and transforms bbox using regression coeff\n",
    "        bbox_pred: bs * h * w * (num_anchors*4)  \n",
    "        '''\n",
    "        \n",
    "        # Get the scores and bounding boxes\n",
    "        scores = cls_prob[:, :, :, n_anchors:]\n",
    "        rpn_bbox_pred = bbox_pred.view((-1, 4))\n",
    "        # rpn_bbox_pred = bbox_pred.view(bs, -1, 4) # for batch\n",
    "        scores = scores.contiguous().view(-1, 1)\n",
    "        # scores = scores.reshape(bs, -1)  # for batch\n",
    "        proposals = bbox_transform_inv(anchors, rpn_bbox_pred) # shift boxes based on prediction\n",
    "\n",
    "        # TODO: Anchors need to have 3 dimensions, currently only have 2 (Error: too many indices for tensor of dimension 2)\n",
    "        # proposals = bbox_transform_inv_batch(anchors, rpn_bbox_pred, bs) # for batch\n",
    "        proposals = clip_boxes(proposals, self._im_info[:2])\n",
    "        # proposals = clip_boxes_batch(proposals, self._im_info[:2], bs) # for batch\n",
    "\n",
    "        # NMS Selection, should include in final\n",
    "        \n",
    "        # Pick the top region proposals\n",
    "        scores, order = scores.view(-1).sort(descending=True)\n",
    "        if PRE_NMS_TOPN > 0:\n",
    "            order = order[:PRE_NMS_TOPN]\n",
    "            scores = scores[:PRE_NMS_TOPN].view(-1, 1)\n",
    "        proposals = proposals[order.data, :]\n",
    "\n",
    "        # Non-maximal suppression\n",
    "        keep = nms(proposals, scores.squeeze(1), NMS_THRESH)\n",
    "\n",
    "        # Pick the top region proposals after NMS\n",
    "        if POST_NMS_TOPN > 0:\n",
    "            keep = keep[:POST_NMS_TOPN]\n",
    "        proposals = proposals[keep, :]\n",
    "        scores = scores[keep, ]\n",
    "\n",
    "        # Only support single image as input\n",
    "        batch_inds = proposals.new_zeros(proposals.size(0), 1)\n",
    "        blob = torch.cat((batch_inds, proposals), 1)\n",
    " \n",
    "        # For batch (NMS)\n",
    "        # scores_keep = scores\n",
    "        # proposals_keep = proposals\n",
    "        # _, order = torch.sort(scores_keep, 1, True)\n",
    "\n",
    "        # output = scores.new(bs, post_nms_topN, 5).zero_()\n",
    "        # for i in range(bs):\n",
    "        #     # # 3. remove predicted boxes with either height or width < threshold\n",
    "        #     # # (NOTE: convert min_size to input image scale stored in im_info[2])\n",
    "        #     proposals_single = proposals_keep[i]\n",
    "        #     scores_single = scores_keep[i]\n",
    "\n",
    "        #     # # 4. sort all (proposal, score) pairs by score from highest to lowest\n",
    "        #     # # 5. take top pre_nms_topN (e.g. 6000)\n",
    "        #     order_single = order[i]\n",
    "\n",
    "        #     if pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel():\n",
    "        #         order_single = order_single[:pre_nms_topN]\n",
    "\n",
    "        #     proposals_single = proposals_single[order_single, :]\n",
    "        #     scores_single = scores_single[order_single].view(-1,1)\n",
    "\n",
    "        #     # 6. apply nms (e.g. threshold = 0.7)\n",
    "        #     # 7. take after_nms_topN (e.g. 300)\n",
    "        #     # 8. return the top proposals (-> RoIs top)\n",
    "\n",
    "        #     keep_idx_i = nms(torch.cat((proposals_single, scores_single), 1), nms_thresh, force_cpu=not cfg.USE_GPU_NMS)\n",
    "        #     keep_idx_i = keep_idx_i.long().view(-1)\n",
    "\n",
    "        #     if post_nms_topN > 0:\n",
    "        #         keep_idx_i = keep_idx_i[:post_nms_topN]\n",
    "        #     proposals_single = proposals_single[keep_idx_i, :]\n",
    "        #     scores_single = scores_single[keep_idx_i, :]\n",
    "\n",
    "        #     # padding 0 at the end.\n",
    "        #     num_proposal = proposals_single.size(0)\n",
    "        #     output[i,:,0] = i\n",
    "        #     output[i,:num_proposal,1:] = proposals_single\n",
    "        \n",
    "        return blob, scores\n",
    "\n",
    "    def anchor_target_layer(self, rpn_cls_score, gt_boxes, all_anchors, im_info=[320, 320, 1]):\n",
    "        '''\n",
    "        ### Parameters ###\n",
    "        rpn_cls_score: Class scores generated by the Region Proposal Network\n",
    "        gt_boxes: Ground Truth boxes\n",
    "        all_anchors: Anchor boxes generated by the anchor generation layer\n",
    "        \n",
    "        ### Fixed Parameters ###\n",
    "        im_info: Image dimensions\n",
    "        num_anchors: Number of different Anchor boxes used. By default, it is set to 9 here.\n",
    "\n",
    "        ### Additional information ###\n",
    "        POSITIVE_OVERLAP:       Threshold used to select if an anchor box is a good foreground box (Default: 0.7)\n",
    "\n",
    "        NEGATIVE_OVERLAP:       If the max overlap of a anchor from a ground truth box is lower than this thershold, it is marked as background. \n",
    "                                Boxes whose overlap is > than NEGATIVE_OVERLAP but < POSITIVE_OVERLAP are marked \n",
    "                                “don’t care”. (Default: 0.3)\n",
    "        \n",
    "        CLOBBER_POSITIVES:      If a particular anchor is satisfied by both the positive and the negative conditions,\n",
    "                                and if this value is set to False, then set the anchor to a negative example.\n",
    "                                Else, set the anchor to a positive example.\n",
    "        \n",
    "        RPN_BS:                 Total number of background and foreground anchors. (Default: 256)\n",
    "        \n",
    "        FG_FRACTION:            Fraction of the batch size that is foreground anchors (Default: 0.5). \n",
    "                                If the number of foreground anchors found is larger than RPN_BS * FG_FRACTION, \n",
    "                                the excess (indices are selected randomly) is marked “don’t care”.\n",
    "                            \n",
    "        RPN_POSITIVE_WEIGHT:    Using this value:\n",
    "                                Positive RPN examples are given a weight of RPN_POSITIVE_WEIGHT * 1 / num_of_positives\n",
    "                                Negative RPN examples are given a weight of (1 - RPN_POSITIVE_WEIGHT)\n",
    "                                Set to -1 by default, which will ensure uniform example weighting.\n",
    "        '''\n",
    "\n",
    "        # Map of shape (..., H, W)\n",
    "        height, width = rpn_cls_score.shape[1:3]\n",
    "\n",
    "        # Only keep anchors that are completely inside the image\n",
    "        inds_inside = np.where(\n",
    "            (all_anchors[:, 0] >= 0) &\n",
    "            (all_anchors[:, 1] >= 0) &\n",
    "            (all_anchors[:, 2] < im_info[1]) &  # width\n",
    "            (all_anchors[:, 3] < im_info[0])  # height\n",
    "        )[0]\n",
    "        anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "        # Label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        labels = np.empty((len(inds_inside), ), dtype=np.float32)\n",
    "        labels.fill(-1)\n",
    "\n",
    "        # BUG: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "        # Overlaps between the Anchors and the Ground Truth boxes\n",
    "        overlaps = bbox_overlaps(\n",
    "            np.ascontiguousarray(anchors, dtype=np.float),\n",
    "            np.ascontiguousarray(gt_boxes, dtype=np.float))\n",
    "        argmax_overlaps = overlaps.argmax(axis=1)\n",
    "        max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
    "        gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
    "        gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n",
    "        gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Positives clobber Negatives\"\n",
    "        if not CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Foreground label: for each Ground Truth box, anchor with highest overlap\n",
    "        labels[gt_argmax_overlaps] = 1\n",
    "\n",
    "        # Foreground label: above threshold IOU\n",
    "        labels[max_overlaps >= POSITIVE_OVERLAP] = 1\n",
    "\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\n",
    "        # \"Negatives clobber Positives\"\n",
    "        if CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        # Subsample positive labels if we have too many\n",
    "        num_fg = int(FG_FRACTION * RPN_BS)\n",
    "        fg_inds = np.where(labels == 1)[0]\n",
    "        if len(fg_inds) > num_fg:\n",
    "            disable_inds = npr.choice(fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        # Subsample negative labels if we have too many\n",
    "        num_bg = RPN_BS - np.sum(labels == 1)\n",
    "        bg_inds = np.where(labels == 0)[0]\n",
    "        if len(bg_inds) > num_bg:\n",
    "            disable_inds = npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
    "            labels[disable_inds] = -1\n",
    "\n",
    "        bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        bbox_targets = compute_targets_atl(anchors, gt_boxes[argmax_overlaps, :])\n",
    "        \n",
    "        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        # Only the positive ones have regression targets\n",
    "        bbox_inside_weights[labels == 1, :] = np.array((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "        labels = labels.numpy()\n",
    "        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
    "        if RPN_POSITIVE_WEIGHT < 0:\n",
    "            # Uniform weighting of examples (given non-uniform sampling)\n",
    "            num_examples = np.sum(labels >= 0)\n",
    "            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n",
    "        else:\n",
    "            assert ((RPN_POSITIVE_WEIGHT > 0) &\n",
    "                    (RPN_POSITIVE_WEIGHT < 1))\n",
    "            positive_weights = (\n",
    "                RPN_POSITIVE_WEIGHT / np.sum(labels == 1))\n",
    "            negative_weights = (\n",
    "                (1.0 - RPN_POSITIVE_WEIGHT) / np.sum(labels == 0))\n",
    "        bbox_outside_weights[labels == 1, :] = positive_weights\n",
    "        bbox_outside_weights[labels == 0, :] = negative_weights\n",
    "\n",
    "        # Map up to original set of anchors\n",
    "        total_anchors = all_anchors.shape[0]\n",
    "        labels = unmap(labels, total_anchors, inds_inside, fill=-1)\n",
    "\n",
    "        bbox_targets = unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n",
    "        bbox_inside_weights = unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n",
    "        bbox_outside_weights = unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n",
    "\n",
    "        # Labels\n",
    "        labels = labels.reshape((1, height, width, self.n_anchors)).transpose(0, 3, 1, 2)\n",
    "        labels = labels.reshape((1, 1, self.n_anchors * height, width))\n",
    "        rpn_labels = labels\n",
    "        \n",
    "        # Bounding boxes\n",
    "        bbox_targets = bbox_targets.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_targets = bbox_targets\n",
    "        bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_inside_weights = bbox_inside_weights\n",
    "        bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, self.n_anchors * 4))\n",
    "        rpn_bbox_outside_weights = bbox_outside_weights\n",
    "\n",
    "        # Re-shape for future use\n",
    "        rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\n",
    "        rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "        rpn_labels = rpn_labels.long()\n",
    "\n",
    "        # Data storing\n",
    "        self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "        self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "        self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "        self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "\n",
    "        for k in self._anchor_targets.keys():\n",
    "            self._score_summaries[k] = self._anchor_targets[k]\n",
    "\n",
    "        return rpn_labels\n",
    "        \n",
    "    def proposal_target_layer(self, proposed_rois, proposed_roi_scores, gt_boxes):\n",
    "        '''\n",
    "        1. Calculate overlap between ROI and GT boxes\n",
    "        2. Select promising ROIs by comparing against threshold(s)\n",
    "        3. Compute bounding box target regression targets and get bounding box regression labels\n",
    "        '''\n",
    "        # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\n",
    "\n",
    "        num_images = 1\n",
    "        rois_per_image = RPN_BS / num_images\n",
    "        # print(\"rois per image\", rois_per_image)\n",
    "        fg_rois_per_image = int(round(FG_FRACTION * rois_per_image))\n",
    "\n",
    "        # Sample rois with classification labels and bounding box regression targets\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "        overlaps = bbox_overlaps(proposed_rois[:, 1:5].data, gt_boxes[:, :4].data)\n",
    "        max_overlaps, gt_assignment = overlaps.max(1)\n",
    "        labels = gt_boxes[gt_assignment, [4]]\n",
    "\n",
    "        # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "        fg_inds = (max_overlaps >= FG_THRESH).nonzero().view(-1)\n",
    "        \n",
    "        # Guard against the case when an image has fewer than fg_rois_per_image\n",
    "        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "        bg_inds = ((max_overlaps < BG_THRESH_HI) + (max_overlaps >= BG_THRESH_LO) == 2).nonzero().view(-1)\n",
    "\n",
    "        # Ensure a fixed number of regions are sampled (optional?)\n",
    "        fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image = fix_sample_regions(fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image)\n",
    "        \n",
    "        # The indices that we're selecting (both fg and bg)\n",
    "        keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "        # Select sampled values from various arrays:\n",
    "        labels = labels[keep_inds].contiguous()\n",
    "\n",
    "        # Clamp labels for the background RoIs to 0\n",
    "        labels[int(fg_rois_per_image):] = 0\n",
    "        rois_final = proposed_rois[keep_inds].contiguous()\n",
    "        roi_scores_final = proposed_roi_scores[keep_inds].contiguous()\n",
    "        # Compute bounding box target regression targets\n",
    "        bbox_target_data = compute_targets_ptl(rois_final[:, 1:5].data, gt_boxes[gt_assignment[keep_inds]][:, :4].data, labels.data)\n",
    "        bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(bbox_target_data, num_of_class)\n",
    "\n",
    "        # Reshape tensors\n",
    "        rois_final = rois_final.view(-1, 5)\n",
    "        roi_scores_final = roi_scores_final.view(-1)\n",
    "        labels = labels.view(-1, 1)\n",
    "        bbox_targets = bbox_targets.view(-1, num_of_class * 4)\n",
    "        bbox_inside_weights = bbox_inside_weights.view(-1, num_of_class * 4)\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "\n",
    "        self._proposal_targets['rois'] = rois_final\n",
    "        self._proposal_targets['labels'] = labels.long()\n",
    "        self._proposal_targets['bbox_targets'] = bbox_targets\n",
    "        self._proposal_targets['bbox_inside_weights'] = bbox_inside_weights\n",
    "        self._proposal_targets['bbox_outside_weights'] = bbox_outside_weights\n",
    "\n",
    "        return rois_final, roi_scores_final \n",
    "\n",
    "    def region_proposal(self, net_conv, bb, anchors):\n",
    "        \"\"\"\n",
    "        Input: features from head network, bounding boxes, anchors generated\n",
    "        Output: rois\n",
    "        1. Proposal Layer\n",
    "        2. Anchor Target Layer\n",
    "        3. Compute RPN Loss\n",
    "        4. Proposal Target Layer\n",
    "        \n",
    "        Features -> class probabilities of anchors(fg or bg) and bbox coeff of anchors (to adjust them)\n",
    "        \"\"\"\n",
    "                                       \n",
    "        # TODO: Note down dimensions of features for each function\n",
    "        rpn = F.relu(self.rpn_net(net_conv))\n",
    "        rpn_cls_score = self.rpn_cls_score_net(rpn)\n",
    "                                       \n",
    "        rpn_cls_score_reshape = rpn_cls_score.view(1, 2, -1, rpn_cls_score.size()[-1]) # batch * 2 * (num_anchors*h) * w\n",
    "        # rpn_cls_score_reshape = rpn_cls_score.view(bs, 2, -1, rpn_cls_score.size()[-1]) # for batch\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, dim=1)\n",
    "\n",
    "        # Move channel to the last dimenstion, to fit the input of python functions\n",
    "        rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score).permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score = rpn_cls_score.permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\n",
    "        rpn_cls_score_reshape = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous() # batch * (num_anchors*h) * w * 2\n",
    "        rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(-1, 2), 1)[1]\n",
    "        # rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(bs, -1, 2), 1)[1]  # for batch (not sure)\n",
    "\n",
    "        rpn_bbox_pred = self.rpn_bbox_pred_net(rpn)\n",
    "        rpn_bbox_pred = rpn_bbox_pred.permute(0, 2, 3, 1).contiguous()  # batch * h * w * (num_anchors*4)                  \n",
    "\n",
    "        if self.mode == 'TRAIN':\n",
    "            rois, roi_scores = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, n_anchors=self.n_anchors)\n",
    "            rpn_labels = self.anchor_target_layer(rpn_cls_score, gt_boxes=bb, all_anchors=anchors)\n",
    "            rois, _ = self.proposal_target_layer(rois, roi_scores, gt_boxes=bb)\n",
    "        else:\n",
    "            rois, _ = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, n_anchors=self.n_anchors)\n",
    "        \n",
    "        self._predictions[\"rpn_cls_score\"] = rpn_cls_score\n",
    "        self._predictions[\"rpn_cls_score_reshape\"] = rpn_cls_score_reshape\n",
    "        self._predictions[\"rpn_cls_prob\"] = rpn_cls_prob\n",
    "        self._predictions[\"rpn_cls_pred\"] = rpn_cls_pred\n",
    "        self._predictions[\"rpn_bbox_pred\"] = rpn_bbox_pred\n",
    "        self._predictions[\"rois\"] = rois\n",
    "        \n",
    "        return rois\n",
    "    \n",
    "    def roi_align_layer(self, bottom, rois):\n",
    "        return RoIAlign((POOLING_SIZE, POOLING_SIZE), 1.0 / 16.0, 0)(bottom, rois)\n",
    "    \n",
    "    def _smooth_l1_loss(self, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n",
    "        sigma_2 = sigma**2\n",
    "        box_diff = bbox_pred - bbox_targets\n",
    "        in_box_diff = bbox_inside_weights * box_diff\n",
    "        abs_in_box_diff = torch.abs(in_box_diff)\n",
    "        smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
    "        in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
    "                      + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "        out_loss_box = bbox_outside_weights * in_loss_box\n",
    "        loss_box = out_loss_box\n",
    "        for i in sorted(dim, reverse=True):\n",
    "            loss_box = loss_box.sum(i)\n",
    "        loss_box = loss_box.mean()\n",
    "        return loss_box\n",
    "\n",
    "    def region_classification(self, fc7):\n",
    "        \"\"\"\n",
    "        cls_score\n",
    "            Linear layer (fc7 channels, num_classes)\n",
    "            torch max\n",
    "            softmax\n",
    "        bbox_pred\n",
    "            Linear layer (fc7 channels, num_classes*4)\n",
    "        \"\"\"\n",
    "        \n",
    "        cls_score = self.cls_score_net(fc7)\n",
    "        cls_pred = torch.max(cls_score, 0)[1]\n",
    "        cls_prob = F.softmax(cls_score, dim=0)\n",
    "        bbox_pred = self.bbox_pred_net(fc7)\n",
    "        # print(\"cls score: \", cls_score.shape)\n",
    "        # print(\"bbox pred: \", bbox_pred.shape)\n",
    "        # self.cls_score_net = nn.Linear(self._fc_channels, RPN_BS*num_of_class)     \n",
    "        # self.bbox_pred_net = nn.Linear(self._fc_channels, RPN_BS*num_of_class*4)\n",
    "        \n",
    "        self._predictions[\"cls_score\"] = cls_score\n",
    "        self._predictions[\"cls_pred\"] = cls_pred\n",
    "        self._predictions[\"cls_prob\"] = cls_prob\n",
    "        self._predictions[\"bbox_pred\"] = bbox_pred\n",
    "\n",
    "        return cls_prob, bbox_pred\n",
    "\n",
    "    def add_losses(self, sigma_rpn=3.0):\n",
    "                                       \n",
    "        # RPN, class loss\n",
    "        rpn_cls_score = self._predictions['rpn_cls_score_reshape'].view(-1, 2).to(device)\n",
    "        rpn_label = self._anchor_targets['rpn_labels'].view(-1).to(device)\n",
    "        rpn_select = (rpn_label.data != -1).nonzero().view(-1)\n",
    "        rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\n",
    "        rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\n",
    "        rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "\n",
    "        # RPN, bbox loss\n",
    "        rpn_bbox_pred = self._predictions['rpn_bbox_pred'].to(device)\n",
    "        rpn_bbox_targets = self._anchor_targets['rpn_bbox_targets'].to(device)\n",
    "        rpn_bbox_inside_weights = self._anchor_targets['rpn_bbox_inside_weights'].to(device)\n",
    "        rpn_bbox_outside_weights = self._anchor_targets['rpn_bbox_outside_weights'].to(device)\n",
    "        rpn_loss_box = self._smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights, sigma=sigma_rpn, dim=[1, 2, 3])\n",
    "\n",
    "        # RCNN, class loss\n",
    "        cls_score = self._predictions[\"cls_score\"].to(device)\n",
    "        # print(\"label: \", self._proposal_targets[\"labels\"].shape)\n",
    "        label = self._proposal_targets[\"labels\"].view(-1).to(device)\n",
    "        # print(\"cls_score: \", cls_score.shape)\n",
    "        # print(\"label: \", label.shape)\n",
    "        cross_entropy = F.cross_entropy(cls_score.view(-1, num_of_class), label)\n",
    "\n",
    "        # RCNN, bbox loss\n",
    "        bbox_pred = self._predictions['bbox_pred'].to(device)\n",
    "        bbox_pred = bbox_pred.view(RPN_BS, -1)\n",
    "        bbox_targets = self._proposal_targets['bbox_targets'].to(device)\n",
    "        bbox_inside_weights = self._proposal_targets['bbox_inside_weights'].to(device)\n",
    "        bbox_outside_weights = self._proposal_targets['bbox_outside_weights'].to(device)\n",
    "        # print(\"bbox_pred: \", bbox_pred.shape)\n",
    "        # print(\"bbox_targets: \", bbox_targets.shape)\n",
    "\n",
    "        loss_box = self._smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n",
    "\n",
    "        self._losses['cross_entropy'] = cross_entropy\n",
    "        self._losses['loss_box'] = loss_box\n",
    "        self._losses['rpn_cross_entropy'] = rpn_cross_entropy\n",
    "        self._losses['rpn_loss_box'] = rpn_loss_box\n",
    "\n",
    "        loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n",
    "        self._losses['total_loss'] = loss\n",
    "\n",
    "        for k in self._losses.keys():\n",
    "            self._event_summaries[k] = self._losses[k]\n",
    "\n",
    "        return loss                                       \n",
    "\n",
    "    def forward(self, x, bb, im_info=[320, 320, 1], train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mode = \"TRAIN\"\n",
    "        else:\n",
    "            self.mode = \"TEST\"\n",
    "        # Store image information\n",
    "        self._im_info = im_info\n",
    "                                       \n",
    "        # Pass the image through the Backbone ConvNet to generate the series of Feature maps\n",
    "        head_conv_net = self.head_net()\n",
    "        output_head = head_conv_net(x) # current: [1, 1024, 154, 154]\n",
    "        anchors, length = generate_anchors(output_head.size(2), output_head.size(3))\n",
    "        anchors = torch.from_numpy(anchors)\n",
    "        \n",
    "        # print(\"Output_head: \", output_head.shape) # [1,1024,20,20]\n",
    "        rois = self.region_proposal(output_head, bb, anchors)\n",
    "        # print(\"ROIS: \", rois.shape) # [RPN_BS, 5]\n",
    "        pool5 = self.roi_align_layer(output_head, rois)\n",
    "        # print(\"POOL5\", pool5.shape) # [RPN_BS, 1024, 7, 7]\n",
    "        fc7 = self.fc7()\n",
    "        fc7.to(device)\n",
    "        fc7_out = fc7(pool5)\n",
    "        # print(\"fc7: \", fc7_out.shape) # [RPN_BS, RPN_BS, 5, 5]\n",
    "        fc7_out = fc7_out.view(-1)\n",
    "        # print(\"fc7: \", fc7_out.shape) # [RPN_BS * 5*5] 6400\n",
    "        cls_prob, bbox_pred = self.region_classification(fc7_out)\n",
    "        # bbox_pred [RPN_BS, 28]\n",
    "        # bbox_targets [RPN_BS, 28]\n",
    "        # label [RPN_BS]\n",
    "        # cls_score [RPN_BS, 7]\n",
    "        if self.mode == 'TEST':\n",
    "            # print(\"bbox_pred shape: \", bbox_pred.shape) # [512] -> n_rois * n_class * 4\n",
    "            # stds = bbox_pred.data.new((0.1, 0.1, 0.2, 0.2)).repeat(num_of_class).expand_as(bbox_pred)\n",
    "            # means = bbox_pred.data.new((0.0, 0.0, 0.0, 0.0)).repeat(num_of_class).expand_as(bbox_pred)\n",
    "            # self._predictions[\"bbox_pred\"] = bbox_pred.mul(stds).add(means)\n",
    "            return self._predictions[\"cls_score\"], self._predictions[\"cls_pred\"], self._predictions[\"cls_prob\"], self._predictions[\"bbox_pred\"]\n",
    "            # return bbox_pred.mul(stds).add(means)\n",
    "        else:\n",
    "            loss = self.add_losses()\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster_R_CNN(\n",
      "  (head_conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (head_batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head_relu1): ReLU()\n",
      "  (head_pool1): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_layer1): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_relu2): ReLU()\n",
      "  (head_layer2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool2): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_relu3): ReLU()\n",
      "  (head_layer3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool3): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_relu4): ReLU()\n",
      "  (rpn_net): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rpn_cls_score_net): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rpn_bbox_pred_net): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (cls_score_net): Linear(in_features=6400, out_features=128, bias=True)\n",
      "  (bbox_pred_net): Linear(in_features=6400, out_features=512, bias=True)\n",
      ")\n",
      "There are 14894198 (14.89 million) parameters in this neural network\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "net = faster_R_CNN()\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "print(utils.display_num_param(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chinhui\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "C:\\Users\\chinhui\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:251: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\chinhui\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:252: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 0.2186763604482015 min \t lr= 0.005 \t loss= 208.01503584384918\n",
      "epoch= 1 \t time= 0.41534106334050497 min \t lr= 0.005 \t loss= 106.50501885414124\n",
      "epoch= 2 \t time= 0.6103721936543782 min \t lr= 0.005 \t loss= 72.62741790612539\n",
      "epoch= 3 \t time= 0.806049931049347 min \t lr= 0.005 \t loss= 55.50550888180733\n",
      "epoch= 4 \t time= 1.0004153887430827 min \t lr= 0.005 \t loss= 45.20339973926544\n",
      "epoch= 5 \t time= 1.1974747061729432 min \t lr= 0.005 \t loss= 38.28953875104586\n",
      "epoch= 6 \t time= 1.3915077328681946 min \t lr= 0.005 \t loss= 33.31671790821212\n",
      "epoch= 7 \t time= 1.5870546023050944 min \t lr= 0.005 \t loss= 29.5488575540483\n",
      "epoch= 8 \t time= 1.7827030301094056 min \t lr= 0.005 \t loss= 26.61102015376091\n",
      "epoch= 9 \t time= 1.976024572054545 min \t lr= 0.005 \t loss= 24.272585102915762\n",
      "epoch= 10 \t time= 2.17136093378067 min \t lr= 0.005 \t loss= 22.339035500992427\n",
      "epoch= 11 \t time= 2.3537198821703593 min \t lr= 0.005 \t loss= 20.74025352721413\n",
      "epoch= 12 \t time= 2.5449364384015403 min \t lr= 0.005 \t loss= 19.377846531455333\n",
      "epoch= 13 \t time= 2.7318047602971394 min \t lr= 0.005 \t loss= 18.19470886098487\n",
      "epoch= 14 \t time= 2.917921260992686 min \t lr= 0.005 \t loss= 17.17291440586249\n",
      "epoch= 15 \t time= 3.102605414390564 min \t lr= 0.005 \t loss= 16.283170493505896\n",
      "epoch= 16 \t time= 3.2909416516621905 min \t lr= 0.005 \t loss= 15.495099788553574\n",
      "epoch= 17 \t time= 3.480660065015157 min \t lr= 0.005 \t loss= 14.793107824855381\n",
      "epoch= 18 \t time= 3.6665205041567486 min \t lr= 0.005 \t loss= 14.17668221185082\n",
      "epoch= 19 \t time= 3.8528757810592653 min \t lr= 0.005 \t loss= 13.604017169773579\n"
     ]
    }
   ],
   "source": [
    "# Set initial learning rate and Optimizer\n",
    "init_lr = 0.005\n",
    "EPOCHS = 20\n",
    "\n",
    "# Set training variables\n",
    "running_loss = 0\n",
    "num_batches = 0\n",
    "start = time.time()\n",
    "\n",
    "# Training process\n",
    "for epoch in range(EPOCHS):\n",
    "    # learning rate strategy : divide the learning rate by 1.5 every 10 epochs\n",
    "    if epoch%10==0 and epoch>10: \n",
    "        lr = lr / 1.5\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = init_lr)\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        batch_images, batch_bboxes = data[0], data[1]\n",
    "        batch_images = batch_images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = net(batch_images, batch_bboxes)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', init_lr  ,'\\t loss=', total_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "\n",
    "torch.save(net, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch bbox:  [tensor([[0.0549, 0.0488, 0.0679, 0.0718, 1.0000],\n",
      "        [0.0323, 0.0194, 0.0441, 0.0407, 2.0000],\n",
      "        [0.0397, 0.0586, 0.0553, 0.0876, 3.0000],\n",
      "        [0.0276, 0.0605, 0.0396, 0.0813, 1.0000]])]\n",
      "bbox_pred:  tensor([ 7.3791e-03, -2.4011e-03,  1.3976e-02,  2.0960e-04,  1.6160e-02,\n",
      "         1.1153e-01, -9.1434e-02,  1.0842e-01,  9.0946e-02,  1.0241e-01,\n",
      "         8.5254e-02,  8.1421e-02, -2.6683e-02,  1.0023e-02,  2.6724e-03,\n",
      "         1.0803e-01,  1.2965e-01,  9.8793e-03,  5.9301e-02,  1.1154e-01,\n",
      "        -7.8297e-03,  1.6963e-03, -4.0475e-03, -7.6407e-03, -2.1700e-02,\n",
      "         1.1385e-02,  3.9772e-02, -1.6125e-02,  1.8882e-02, -3.2655e-03,\n",
      "         1.0149e-02,  1.1120e-02,  2.9649e-03, -4.4118e-03, -6.3656e-03,\n",
      "        -4.2921e-03,  1.8777e-01,  1.7510e-01,  1.6450e-01,  1.7652e-01,\n",
      "         9.3084e-02,  9.1461e-02,  8.0378e-02,  9.4074e-02, -5.0657e-02,\n",
      "        -9.6185e-02,  1.3524e-01,  1.1884e-01,  6.8169e-02, -2.7839e-02,\n",
      "         6.4985e-02,  2.6559e-02, -8.3764e-03,  1.0118e-02, -2.9734e-03,\n",
      "         3.0913e-03, -1.0722e-01, -6.8856e-02,  3.7492e-02, -2.7504e-02,\n",
      "        -1.4946e-02,  3.3234e-03,  5.6467e-03, -6.9270e-03,  1.4581e-02,\n",
      "        -5.8814e-03, -4.3712e-03, -9.8572e-03,  1.6085e-01,  1.4539e-01,\n",
      "         1.8759e-01,  1.7156e-01,  9.3541e-02,  8.3283e-02,  7.0776e-02,\n",
      "         7.3360e-02,  6.1626e-02,  1.0069e-01,  3.7260e-02, -1.0701e-01,\n",
      "         6.3307e-02,  1.2724e-02,  7.2802e-02, -1.5978e-02, -5.9742e-03,\n",
      "         6.1627e-03, -4.7727e-03,  3.5348e-03,  1.2338e-01,  1.0148e-01,\n",
      "         9.6714e-02,  1.1434e-01, -7.7943e-03,  7.3422e-03, -7.9083e-03,\n",
      "        -1.4988e-03,  1.0130e-02, -6.1672e-03,  6.8191e-03, -8.6426e-03,\n",
      "         3.3407e-01, -2.1595e-02,  2.9783e-01, -6.5804e-02,  6.7860e-02,\n",
      "         5.1210e-02,  4.4241e-02,  4.8678e-02,  1.4015e-02,  3.7407e-02,\n",
      "        -1.6579e-01,  1.0825e-01,  9.3807e-02, -3.4814e-02, -1.8754e-02,\n",
      "         1.1978e-01, -1.4475e-03, -6.0606e-04, -9.2987e-03,  1.1659e-03,\n",
      "         1.1909e-01,  1.0240e-01, -1.4332e-01,  6.6111e-02, -7.4327e-03,\n",
      "        -1.1428e-02, -1.7122e-02,  9.2625e-03,  5.4403e-03,  4.0452e-03,\n",
      "        -1.3595e-02, -2.3175e-03, -5.1797e-02, -6.2943e-02,  9.9628e-02,\n",
      "         1.0328e-01,  7.2519e-02,  9.9938e-02,  7.8999e-02,  8.5314e-02,\n",
      "         8.0104e-02,  1.7866e-02,  6.7532e-04,  4.9766e-02,  8.1919e-04,\n",
      "         5.6813e-02, -8.6097e-02,  5.8933e-02, -3.3074e-03, -6.8341e-03,\n",
      "         1.2012e-02, -1.0504e-02, -5.0643e-02,  1.7683e-01,  6.4693e-02,\n",
      "         1.6631e-01,  1.6452e-02,  3.3090e-03, -1.2199e-02,  1.8207e-02,\n",
      "        -5.6914e-03,  1.0925e-02,  8.3845e-03, -1.0795e-03,  7.1534e-02,\n",
      "         1.6741e-01, -1.5259e-01, -1.5846e-01,  6.1608e-02, -3.0875e-02,\n",
      "         3.0839e-02,  3.6957e-02, -6.2047e-02, -5.2600e-02,  1.2911e-01,\n",
      "         3.2859e-03,  6.0083e-02,  1.0861e-01,  6.0940e-03,  4.9291e-02,\n",
      "         6.0657e-03, -4.1229e-03, -6.7625e-03,  1.4548e-02,  8.8775e-04,\n",
      "         7.8894e-02,  8.9081e-02, -8.8875e-02,  1.3859e-03, -7.7052e-03,\n",
      "        -3.0764e-03,  1.1183e-02,  8.4976e-04,  3.3825e-03, -1.4738e-03,\n",
      "        -1.6567e-03, -3.2425e-02,  5.5228e-02, -2.1924e-01,  3.7578e-02,\n",
      "         9.5701e-02,  1.0146e-01,  5.1242e-02,  8.9766e-02, -9.6524e-02,\n",
      "        -1.2872e-01,  5.5206e-02, -1.6315e-01,  1.7013e-01,  1.3614e-01,\n",
      "        -7.3672e-02,  5.8387e-02, -8.5561e-03, -6.1414e-03, -8.3352e-03,\n",
      "        -2.8189e-03,  1.6247e-01, -3.4584e-02,  1.0475e-01,  1.0029e-01,\n",
      "         4.7256e-04, -1.1725e-02,  6.6855e-03,  8.8609e-03, -4.0069e-03,\n",
      "         6.8252e-03, -1.2530e-02,  5.1749e-03,  1.7868e-01,  1.6542e-01,\n",
      "         1.5758e-01,  1.7726e-01,  9.8017e-02,  8.0852e-02,  5.7943e-02,\n",
      "         4.7979e-02,  8.6736e-03, -3.7952e-03, -7.7504e-04, -4.0186e-03,\n",
      "        -1.2217e-02,  1.6204e-01, -2.5991e-02,  6.0030e-02, -1.1626e-02,\n",
      "         8.2405e-03, -1.2313e-02, -1.3099e-02,  1.4297e-01,  2.0061e-01,\n",
      "        -7.8808e-02, -1.2166e-01,  8.9054e-03,  3.8468e-04, -1.8478e-02,\n",
      "        -1.0810e-02, -1.0536e-02,  1.1679e-02,  1.3188e-02, -3.1133e-03,\n",
      "        -2.1283e-04, -5.7671e-03,  4.1082e-03, -7.3414e-03,  8.0038e-03,\n",
      "         1.2644e-02, -8.6296e-04, -8.2934e-03, -5.5231e-03, -1.1930e-02,\n",
      "         1.1685e-02,  4.7822e-03, -6.8669e-03, -6.1232e-03, -6.4633e-03,\n",
      "        -2.1953e-03, -5.8594e-03, -1.0593e-02,  2.0995e-03, -1.6992e-03,\n",
      "         5.0157e-03,  2.0098e-03, -9.7214e-03, -5.6202e-03,  1.3751e-02,\n",
      "        -1.6565e-02,  1.0756e-02, -1.8307e-03, -1.4739e-03, -1.7066e-02,\n",
      "        -4.0792e-03, -2.3526e-03,  5.1484e-03, -4.4902e-03, -1.1902e-02,\n",
      "         8.7045e-03,  1.9351e-04,  1.4068e-02, -9.0868e-03,  8.0012e-03,\n",
      "         3.4764e-04,  2.5781e-03,  6.9297e-03,  1.0506e-02,  1.6467e-02,\n",
      "         1.0004e-02, -1.9556e-03,  3.9635e-03, -1.2019e-02,  9.9541e-03,\n",
      "        -1.3512e-02, -7.0980e-03,  2.4306e-04, -3.3814e-03,  1.2956e-02,\n",
      "        -1.6893e-02,  3.9262e-04,  1.2372e-03,  4.6156e-03,  1.3618e-03,\n",
      "         9.4699e-03, -1.2409e-02,  8.2781e-03,  4.6790e-03, -7.0800e-03,\n",
      "         2.7862e-03,  1.3173e-02,  1.0951e-02,  4.9395e-04,  3.7756e-03,\n",
      "        -4.0650e-03, -8.9938e-03,  3.4300e-03, -1.0452e-02, -9.2137e-03,\n",
      "         6.0563e-03, -9.0848e-05, -1.1740e-02,  4.1658e-03, -8.5947e-04,\n",
      "        -3.6863e-03,  1.0782e-02, -6.9890e-03, -4.1013e-03,  4.0311e-03,\n",
      "        -1.1244e-02, -7.1088e-04, -9.3849e-03,  6.9083e-03, -3.0614e-03,\n",
      "        -9.1752e-03,  8.1978e-03, -1.7683e-04,  1.2870e-02,  8.0927e-04,\n",
      "        -7.4593e-03,  2.1472e-03,  1.8100e-03, -9.6066e-04,  4.6536e-03,\n",
      "        -5.3908e-03, -5.4655e-03,  1.3436e-02,  5.5874e-03,  7.4594e-03,\n",
      "         3.5812e-03,  3.1545e-03, -6.7347e-04, -1.1840e-02, -1.5420e-02,\n",
      "         4.8883e-03, -1.8707e-02,  7.3770e-03, -6.4520e-03,  9.7065e-03,\n",
      "        -5.6966e-03,  4.5453e-03,  4.9615e-03, -1.1724e-02, -7.7274e-03,\n",
      "         2.9808e-03,  4.9405e-03, -1.8661e-03,  1.1340e-03,  1.5203e-03,\n",
      "        -1.6227e-03, -9.0010e-03,  8.7509e-03,  2.8344e-03,  8.4176e-03,\n",
      "         6.1894e-03,  5.3533e-03, -4.4510e-03, -2.2766e-03,  7.8920e-03,\n",
      "         4.7015e-03,  3.4466e-03,  1.0758e-02, -4.3436e-03, -1.3548e-03,\n",
      "         1.0826e-02, -9.3978e-03, -1.1277e-02,  1.3265e-02,  4.6409e-03,\n",
      "        -1.4040e-02, -6.6241e-03, -1.3606e-03,  6.9426e-03, -1.1004e-02,\n",
      "        -1.2439e-04,  1.1609e-02,  4.3415e-03, -3.6102e-03, -1.1789e-02,\n",
      "         4.2551e-03,  6.9764e-03, -1.7455e-03, -4.0540e-03,  9.0521e-03,\n",
      "        -2.0836e-03, -1.4285e-02, -5.6321e-03,  2.6582e-03,  6.4855e-03,\n",
      "         1.0726e-02,  1.5330e-02,  9.8653e-03,  8.2393e-03,  4.4996e-03,\n",
      "         5.6165e-03, -4.6451e-03, -7.1095e-03, -6.4750e-03, -8.6075e-03,\n",
      "         4.5452e-04,  6.5312e-03, -1.0263e-02, -6.7003e-04, -1.6197e-03,\n",
      "         1.0102e-02, -6.1809e-03, -1.0236e-03,  6.2442e-03, -1.5336e-02,\n",
      "        -5.6287e-03,  5.6200e-03,  4.4990e-03,  5.2741e-03,  2.5595e-03,\n",
      "        -3.6304e-03,  7.1289e-03,  6.1462e-03,  1.1159e-02, -7.9797e-03,\n",
      "        -9.4064e-03,  3.8453e-03, -9.1986e-03, -3.8923e-03,  1.2911e-02,\n",
      "        -2.1626e-03, -9.5082e-03, -1.9567e-03, -1.2046e-02,  9.8913e-03,\n",
      "        -9.2472e-03,  7.2684e-03, -6.1756e-04,  1.4287e-02,  1.6558e-03,\n",
      "        -1.1020e-02, -1.1383e-02,  1.6448e-03, -4.6529e-04, -6.8660e-03,\n",
      "         1.4119e-02,  3.0378e-03, -9.1472e-03,  9.3372e-03, -5.0024e-03,\n",
      "        -3.2771e-03, -7.7335e-03, -1.2066e-02,  7.1169e-03, -9.3329e-03,\n",
      "         1.4851e-02,  7.8348e-03, -1.4966e-03,  4.4134e-03, -5.8947e-03,\n",
      "        -5.3861e-04, -6.8245e-03,  4.1507e-03, -3.5045e-03,  7.6089e-03,\n",
      "        -1.0977e-02,  2.0828e-03, -9.1501e-03, -9.9254e-03, -6.8449e-04,\n",
      "         1.3432e-02, -5.6302e-03,  2.3735e-03,  5.0828e-03,  4.8327e-03,\n",
      "         1.5943e-03,  7.8848e-03, -9.4094e-03, -7.5864e-03, -3.0142e-04,\n",
      "        -1.0028e-03, -1.0016e-02], device='cuda:0')\n",
      "bbox_pred shape:  torch.Size([512])\n",
      "cls_score:  tensor([ 0.2266, -0.9411, -0.0245, -0.6544,  0.3443, -3.8883, -1.1482, -3.8927,\n",
      "         0.2362, -0.9212, -0.0245, -0.6615,  0.3512, -3.8773, -1.1399, -3.8919,\n",
      "         0.2321, -0.9023, -0.0165, -0.6713,  0.3463, -3.8746, -1.1516, -3.8726,\n",
      "         0.2419, -0.9395,  0.0162, -0.6653,  0.3110, -3.8591, -1.1477, -3.8679,\n",
      "         0.2383, -0.9313, -0.0206, -0.6435,  0.3568, -3.8841, -1.1334, -3.8805,\n",
      "         0.2337, -0.9352, -0.0363, -0.6416,  0.3455, -3.8835, -1.1466, -3.8659,\n",
      "         0.2447, -0.9283, -0.0192, -0.6579,  0.3640, -3.8708, -1.1619, -3.8845,\n",
      "         0.2332, -0.9275, -0.0297, -0.6782,  0.3486, -3.8709, -1.1485, -3.8685,\n",
      "         3.6176, -3.6012, -3.6199, -3.6056, -3.6079, -3.5984, -3.6126, -3.5994,\n",
      "         3.6150, -3.6103, -3.6284, -3.6226, -3.6207, -3.6106, -3.6044, -3.6014,\n",
      "         3.6033, -3.6249, -3.6142, -3.6077, -3.6099, -3.6095, -3.6117, -3.6104,\n",
      "         3.6140, -3.6073, -3.6082, -3.6066, -3.6031, -3.6175, -3.6036, -3.6092,\n",
      "         3.6192, -3.6048, -3.6210, -3.6177, -3.6228, -3.6120, -3.6115, -3.6227,\n",
      "         3.6249, -3.6106, -3.6103, -3.6067, -3.6182, -3.6071, -3.6102, -3.6106,\n",
      "         3.6126, -3.6088, -3.6058, -3.6024, -3.6106, -3.6176, -3.6144, -3.6074,\n",
      "         3.6102, -3.6145, -3.6040, -3.6075, -3.6188, -3.6093, -3.6117, -3.6171],\n",
      "       device='cuda:0')\n",
      "cls_pred:  tensor(104, device='cuda:0')\n",
      "cls_prob:  tensor([3.7103e-03, 1.1543e-03, 2.8866e-03, 1.5374e-03, 4.1736e-03, 6.0581e-05,\n",
      "        9.3826e-04, 6.0312e-05, 3.7460e-03, 1.1774e-03, 2.8866e-03, 1.5266e-03,\n",
      "        4.2026e-03, 6.1249e-05, 9.4615e-04, 6.0361e-05, 3.7308e-03, 1.1999e-03,\n",
      "        2.9096e-03, 1.5117e-03, 4.1823e-03, 6.1414e-05, 9.3515e-04, 6.1536e-05,\n",
      "        3.7676e-03, 1.1560e-03, 3.0064e-03, 1.5207e-03, 4.0372e-03, 6.2376e-05,\n",
      "        9.3882e-04, 6.1828e-05, 3.7541e-03, 1.1656e-03, 2.8978e-03, 1.5543e-03,\n",
      "        4.2265e-03, 6.0834e-05, 9.5232e-04, 6.1057e-05, 3.7368e-03, 1.1610e-03,\n",
      "        2.8526e-03, 1.5572e-03, 4.1789e-03, 6.0874e-05, 9.3982e-04, 6.1952e-05,\n",
      "        3.7781e-03, 1.1690e-03, 2.9018e-03, 1.5321e-03, 4.2567e-03, 6.1649e-05,\n",
      "        9.2557e-04, 6.0809e-05, 3.7349e-03, 1.1700e-03, 2.8714e-03, 1.5013e-03,\n",
      "        4.1917e-03, 6.1646e-05, 9.3802e-04, 6.1793e-05, 1.1018e-01, 8.0726e-05,\n",
      "        7.9230e-05, 8.0370e-05, 8.0192e-05, 8.0953e-05, 7.9814e-05, 8.0877e-05,\n",
      "        1.0989e-01, 7.9995e-05, 7.8563e-05, 7.9022e-05, 7.9169e-05, 7.9973e-05,\n",
      "        8.0473e-05, 8.0715e-05, 1.0862e-01, 7.8838e-05, 7.9681e-05, 8.0207e-05,\n",
      "        8.0026e-05, 8.0063e-05, 7.9885e-05, 7.9987e-05, 1.0978e-01, 8.0240e-05,\n",
      "        8.0166e-05, 8.0296e-05, 8.0574e-05, 7.9424e-05, 8.0531e-05, 8.0083e-05,\n",
      "        1.1036e-01, 8.0434e-05, 7.9142e-05, 7.9408e-05, 7.9005e-05, 7.9861e-05,\n",
      "        7.9898e-05, 7.9009e-05, 1.1099e-01, 7.9969e-05, 7.9996e-05, 8.0286e-05,\n",
      "        7.9370e-05, 8.0253e-05, 8.0000e-05, 7.9972e-05, 1.0963e-01, 8.0116e-05,\n",
      "        8.0358e-05, 8.0634e-05, 7.9970e-05, 7.9416e-05, 7.9672e-05, 8.0232e-05,\n",
      "        1.0936e-01, 7.9658e-05, 8.0504e-05, 8.0217e-05, 7.9320e-05, 8.0079e-05,\n",
      "        7.9881e-05, 7.9457e-05], device='cuda:0')\n",
      "bbox_pred:  tensor([ 7.3791e-03, -2.4011e-03,  1.3976e-02,  2.0960e-04,  1.6160e-02,\n",
      "         1.1153e-01, -9.1434e-02,  1.0842e-01,  9.0946e-02,  1.0241e-01,\n",
      "         8.5254e-02,  8.1421e-02, -2.6683e-02,  1.0023e-02,  2.6724e-03,\n",
      "         1.0803e-01,  1.2965e-01,  9.8793e-03,  5.9301e-02,  1.1154e-01,\n",
      "        -7.8297e-03,  1.6963e-03, -4.0475e-03, -7.6407e-03, -2.1700e-02,\n",
      "         1.1385e-02,  3.9772e-02, -1.6125e-02,  1.8882e-02, -3.2655e-03,\n",
      "         1.0149e-02,  1.1120e-02,  2.9649e-03, -4.4118e-03, -6.3656e-03,\n",
      "        -4.2921e-03,  1.8777e-01,  1.7510e-01,  1.6450e-01,  1.7652e-01,\n",
      "         9.3084e-02,  9.1461e-02,  8.0378e-02,  9.4074e-02, -5.0657e-02,\n",
      "        -9.6185e-02,  1.3524e-01,  1.1884e-01,  6.8169e-02, -2.7839e-02,\n",
      "         6.4985e-02,  2.6559e-02, -8.3764e-03,  1.0118e-02, -2.9734e-03,\n",
      "         3.0913e-03, -1.0722e-01, -6.8856e-02,  3.7492e-02, -2.7504e-02,\n",
      "        -1.4946e-02,  3.3234e-03,  5.6467e-03, -6.9270e-03,  1.4581e-02,\n",
      "        -5.8814e-03, -4.3712e-03, -9.8572e-03,  1.6085e-01,  1.4539e-01,\n",
      "         1.8759e-01,  1.7156e-01,  9.3541e-02,  8.3283e-02,  7.0776e-02,\n",
      "         7.3360e-02,  6.1626e-02,  1.0069e-01,  3.7260e-02, -1.0701e-01,\n",
      "         6.3307e-02,  1.2724e-02,  7.2802e-02, -1.5978e-02, -5.9742e-03,\n",
      "         6.1627e-03, -4.7727e-03,  3.5348e-03,  1.2338e-01,  1.0148e-01,\n",
      "         9.6714e-02,  1.1434e-01, -7.7943e-03,  7.3422e-03, -7.9083e-03,\n",
      "        -1.4988e-03,  1.0130e-02, -6.1672e-03,  6.8191e-03, -8.6426e-03,\n",
      "         3.3407e-01, -2.1595e-02,  2.9783e-01, -6.5804e-02,  6.7860e-02,\n",
      "         5.1210e-02,  4.4241e-02,  4.8678e-02,  1.4015e-02,  3.7407e-02,\n",
      "        -1.6579e-01,  1.0825e-01,  9.3807e-02, -3.4814e-02, -1.8754e-02,\n",
      "         1.1978e-01, -1.4475e-03, -6.0606e-04, -9.2987e-03,  1.1659e-03,\n",
      "         1.1909e-01,  1.0240e-01, -1.4332e-01,  6.6111e-02, -7.4327e-03,\n",
      "        -1.1428e-02, -1.7122e-02,  9.2625e-03,  5.4403e-03,  4.0452e-03,\n",
      "        -1.3595e-02, -2.3175e-03, -5.1797e-02, -6.2943e-02,  9.9628e-02,\n",
      "         1.0328e-01,  7.2519e-02,  9.9938e-02,  7.8999e-02,  8.5314e-02,\n",
      "         8.0104e-02,  1.7866e-02,  6.7532e-04,  4.9766e-02,  8.1919e-04,\n",
      "         5.6813e-02, -8.6097e-02,  5.8933e-02, -3.3074e-03, -6.8341e-03,\n",
      "         1.2012e-02, -1.0504e-02, -5.0643e-02,  1.7683e-01,  6.4693e-02,\n",
      "         1.6631e-01,  1.6452e-02,  3.3090e-03, -1.2199e-02,  1.8207e-02,\n",
      "        -5.6914e-03,  1.0925e-02,  8.3845e-03, -1.0795e-03,  7.1534e-02,\n",
      "         1.6741e-01, -1.5259e-01, -1.5846e-01,  6.1608e-02, -3.0875e-02,\n",
      "         3.0839e-02,  3.6957e-02, -6.2047e-02, -5.2600e-02,  1.2911e-01,\n",
      "         3.2859e-03,  6.0083e-02,  1.0861e-01,  6.0940e-03,  4.9291e-02,\n",
      "         6.0657e-03, -4.1229e-03, -6.7625e-03,  1.4548e-02,  8.8775e-04,\n",
      "         7.8894e-02,  8.9081e-02, -8.8875e-02,  1.3859e-03, -7.7052e-03,\n",
      "        -3.0764e-03,  1.1183e-02,  8.4976e-04,  3.3825e-03, -1.4738e-03,\n",
      "        -1.6567e-03, -3.2425e-02,  5.5228e-02, -2.1924e-01,  3.7578e-02,\n",
      "         9.5701e-02,  1.0146e-01,  5.1242e-02,  8.9766e-02, -9.6524e-02,\n",
      "        -1.2872e-01,  5.5206e-02, -1.6315e-01,  1.7013e-01,  1.3614e-01,\n",
      "        -7.3672e-02,  5.8387e-02, -8.5561e-03, -6.1414e-03, -8.3352e-03,\n",
      "        -2.8189e-03,  1.6247e-01, -3.4584e-02,  1.0475e-01,  1.0029e-01,\n",
      "         4.7256e-04, -1.1725e-02,  6.6855e-03,  8.8609e-03, -4.0069e-03,\n",
      "         6.8252e-03, -1.2530e-02,  5.1749e-03,  1.7868e-01,  1.6542e-01,\n",
      "         1.5758e-01,  1.7726e-01,  9.8017e-02,  8.0852e-02,  5.7943e-02,\n",
      "         4.7979e-02,  8.6736e-03, -3.7952e-03, -7.7504e-04, -4.0186e-03,\n",
      "        -1.2217e-02,  1.6204e-01, -2.5991e-02,  6.0030e-02, -1.1626e-02,\n",
      "         8.2405e-03, -1.2313e-02, -1.3099e-02,  1.4297e-01,  2.0061e-01,\n",
      "        -7.8808e-02, -1.2166e-01,  8.9054e-03,  3.8468e-04, -1.8478e-02,\n",
      "        -1.0810e-02, -1.0536e-02,  1.1679e-02,  1.3188e-02, -3.1133e-03,\n",
      "        -2.1283e-04, -5.7671e-03,  4.1082e-03, -7.3414e-03,  8.0038e-03,\n",
      "         1.2644e-02, -8.6296e-04, -8.2934e-03, -5.5231e-03, -1.1930e-02,\n",
      "         1.1685e-02,  4.7822e-03, -6.8669e-03, -6.1232e-03, -6.4633e-03,\n",
      "        -2.1953e-03, -5.8594e-03, -1.0593e-02,  2.0995e-03, -1.6992e-03,\n",
      "         5.0157e-03,  2.0098e-03, -9.7214e-03, -5.6202e-03,  1.3751e-02,\n",
      "        -1.6565e-02,  1.0756e-02, -1.8307e-03, -1.4739e-03, -1.7066e-02,\n",
      "        -4.0792e-03, -2.3526e-03,  5.1484e-03, -4.4902e-03, -1.1902e-02,\n",
      "         8.7045e-03,  1.9351e-04,  1.4068e-02, -9.0868e-03,  8.0012e-03,\n",
      "         3.4764e-04,  2.5781e-03,  6.9297e-03,  1.0506e-02,  1.6467e-02,\n",
      "         1.0004e-02, -1.9556e-03,  3.9635e-03, -1.2019e-02,  9.9541e-03,\n",
      "        -1.3512e-02, -7.0980e-03,  2.4306e-04, -3.3814e-03,  1.2956e-02,\n",
      "        -1.6893e-02,  3.9262e-04,  1.2372e-03,  4.6156e-03,  1.3618e-03,\n",
      "         9.4699e-03, -1.2409e-02,  8.2781e-03,  4.6790e-03, -7.0800e-03,\n",
      "         2.7862e-03,  1.3173e-02,  1.0951e-02,  4.9395e-04,  3.7756e-03,\n",
      "        -4.0650e-03, -8.9938e-03,  3.4300e-03, -1.0452e-02, -9.2137e-03,\n",
      "         6.0563e-03, -9.0848e-05, -1.1740e-02,  4.1658e-03, -8.5947e-04,\n",
      "        -3.6863e-03,  1.0782e-02, -6.9890e-03, -4.1013e-03,  4.0311e-03,\n",
      "        -1.1244e-02, -7.1088e-04, -9.3849e-03,  6.9083e-03, -3.0614e-03,\n",
      "        -9.1752e-03,  8.1978e-03, -1.7683e-04,  1.2870e-02,  8.0927e-04,\n",
      "        -7.4593e-03,  2.1472e-03,  1.8100e-03, -9.6066e-04,  4.6536e-03,\n",
      "        -5.3908e-03, -5.4655e-03,  1.3436e-02,  5.5874e-03,  7.4594e-03,\n",
      "         3.5812e-03,  3.1545e-03, -6.7347e-04, -1.1840e-02, -1.5420e-02,\n",
      "         4.8883e-03, -1.8707e-02,  7.3770e-03, -6.4520e-03,  9.7065e-03,\n",
      "        -5.6966e-03,  4.5453e-03,  4.9615e-03, -1.1724e-02, -7.7274e-03,\n",
      "         2.9808e-03,  4.9405e-03, -1.8661e-03,  1.1340e-03,  1.5203e-03,\n",
      "        -1.6227e-03, -9.0010e-03,  8.7509e-03,  2.8344e-03,  8.4176e-03,\n",
      "         6.1894e-03,  5.3533e-03, -4.4510e-03, -2.2766e-03,  7.8920e-03,\n",
      "         4.7015e-03,  3.4466e-03,  1.0758e-02, -4.3436e-03, -1.3548e-03,\n",
      "         1.0826e-02, -9.3978e-03, -1.1277e-02,  1.3265e-02,  4.6409e-03,\n",
      "        -1.4040e-02, -6.6241e-03, -1.3606e-03,  6.9426e-03, -1.1004e-02,\n",
      "        -1.2439e-04,  1.1609e-02,  4.3415e-03, -3.6102e-03, -1.1789e-02,\n",
      "         4.2551e-03,  6.9764e-03, -1.7455e-03, -4.0540e-03,  9.0521e-03,\n",
      "        -2.0836e-03, -1.4285e-02, -5.6321e-03,  2.6582e-03,  6.4855e-03,\n",
      "         1.0726e-02,  1.5330e-02,  9.8653e-03,  8.2393e-03,  4.4996e-03,\n",
      "         5.6165e-03, -4.6451e-03, -7.1095e-03, -6.4750e-03, -8.6075e-03,\n",
      "         4.5452e-04,  6.5312e-03, -1.0263e-02, -6.7003e-04, -1.6197e-03,\n",
      "         1.0102e-02, -6.1809e-03, -1.0236e-03,  6.2442e-03, -1.5336e-02,\n",
      "        -5.6287e-03,  5.6200e-03,  4.4990e-03,  5.2741e-03,  2.5595e-03,\n",
      "        -3.6304e-03,  7.1289e-03,  6.1462e-03,  1.1159e-02, -7.9797e-03,\n",
      "        -9.4064e-03,  3.8453e-03, -9.1986e-03, -3.8923e-03,  1.2911e-02,\n",
      "        -2.1626e-03, -9.5082e-03, -1.9567e-03, -1.2046e-02,  9.8913e-03,\n",
      "        -9.2472e-03,  7.2684e-03, -6.1756e-04,  1.4287e-02,  1.6558e-03,\n",
      "        -1.1020e-02, -1.1383e-02,  1.6448e-03, -4.6529e-04, -6.8660e-03,\n",
      "         1.4119e-02,  3.0378e-03, -9.1472e-03,  9.3372e-03, -5.0024e-03,\n",
      "        -3.2771e-03, -7.7335e-03, -1.2066e-02,  7.1169e-03, -9.3329e-03,\n",
      "         1.4851e-02,  7.8348e-03, -1.4966e-03,  4.4134e-03, -5.8947e-03,\n",
      "        -5.3861e-04, -6.8245e-03,  4.1507e-03, -3.5045e-03,  7.6089e-03,\n",
      "        -1.0977e-02,  2.0828e-03, -9.1501e-03, -9.9254e-03, -6.8449e-04,\n",
      "         1.3432e-02, -5.6302e-03,  2.3735e-03,  5.0828e-03,  4.8327e-03,\n",
      "         1.5943e-03,  7.8848e-03, -9.4094e-03, -7.5864e-03, -3.0142e-04,\n",
      "        -1.0028e-03, -1.0016e-02], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3488\\2966056558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'test error  = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_error\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'percent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0meval_on_test_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3488\\2966056558.py\u001b[0m in \u001b[0;36meval_on_test_set\u001b[1;34m(test_loader)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m# error = utils.get_error( scores , minibatch_label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mrunning_error\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mnum_batches\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "def eval_on_test_set(test_loader):\n",
    "    # net = faster_R_CNN()\n",
    "    # net.to(device)\n",
    "    device = torch.device('cuda')\n",
    "    net = torch.load(\"model\")\n",
    "    net.to(device)\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    TEST_NMS = 0.3\n",
    "    thresh = 0.3\n",
    "\n",
    "    for data in test_loader:\n",
    "        with torch.no_grad():\n",
    "            batch_images, batch_bboxes = data[0], data[1]\n",
    "            batch_images = batch_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cls_score, cls_pred, cls_prob, bbox_pred = net(batch_images, batch_bboxes, train_flag=False)\n",
    "\n",
    "            n_anchors = int(bbox_pred.shape[0] / (num_of_class * 4))\n",
    "            scores = cls_score.view(n_anchors, -1).cpu().numpy()\n",
    "            boxes = bbox_pred.view(n_anchors, -1).cpu().numpy()\n",
    "            predictions = {}\n",
    "            # error = utils.get_error( scores , minibatch_label)\n",
    "            # skip j = 0, because it's the background class\n",
    "            for j in range(1, num_of_class):\n",
    "                inds = np.where(scores[:, j] > thresh)[0]\n",
    "                cls_scores = scores[inds, j]\n",
    "                cls_boxes = boxes[inds, j * 4:(j + 1) * 4]\n",
    "                cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n",
    "                .astype(np.float32, copy=False)\n",
    "                keep = nms(\n",
    "                    torch.from_numpy(cls_boxes), torch.from_numpy(cls_scores),\n",
    "                    TEST_NMS).numpy() if cls_dets.size > 0 else []\n",
    "                cls_dets = cls_dets[keep, :]\n",
    "                label = index_label_dict[j]\n",
    "                if label in predictions.keys():\n",
    "                    predictions[label].append(cls_dets)\n",
    "                else:\n",
    "                    predictions[label] = [cls_dets]\n",
    "            print(\"predictions: \", predictions)\n",
    "            print(\"batch_bbox: \", batch_bboxes)\n",
    "            # running_error += error.item()\n",
    "\n",
    "            num_batches+=1\n",
    "\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'test error  = ', total_error*100 ,'percent')\n",
    "\n",
    "eval_on_test_set(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb3f132b8f46ef83ff81b7067b37036dbb49ec9e6fa8b80a12ce6c1c87b906f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('deeplearn_course': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
