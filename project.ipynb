{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# What Really Counts: A Cash Recognization System\n",
    "\n",
    "---\n",
    "\n",
    "**Group Members**:\n",
    "- Ek Chin Hui (A0205578X)\n",
    "- Lee Hyung Woon (A0218475X)\n",
    "- Toh Hai Jie Joey (A0205141Y)\n",
    "\n",
    "---\n",
    "\n",
    "Our project, named **What Really Counts**, is a Cash Recognization System for the Visually Impaired in Singapore. In Singapore, the disabled community face many challenges in their daily lives, and this is especially so for those who are hampered by visual impairments. One such challenge they face is cash payment, as they need to identify the correct combinations of bills and coins. Hence, our aim was to contruct a system that can help them overcome these challenges by employing a deep learning-based Object Detection model using Convolutional Neural Networks (CNN) - in particular, the Faster R-CNN model. \n",
    "\n",
    "**What Really Counts** is an architecture that detects and analyzes given images of Singapore Currencies (bills and/or coins), and is primarily designed to assist the visually impaired in identifying the correct combinations of bills and coins. The model uses CNNs to perform image classification on the objects detected in a given input image, through which we can ascertain the exact number and type of bills / coins present in the image, allowing us to calculate and return the sum of the currency to the user.\n",
    "\n",
    "For this project, we will gather and pre-process our own dataset, and then move onto training and testing of our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries\n",
    "\n",
    "The following are modules that we used for this project..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "import os\r\n",
    "import xml.etree.ElementTree as ET\r\n",
    "import torchvision.transforms.functional as FT\r\n",
    "import json\r\n",
    "import time\r\n",
    "import utils\r\n",
    "import math\r\n",
    "import numpy as np\r\n",
    "import numpy.random as npr\r\n",
    "\r\n",
    "from torch import nn\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from torchvision import transforms\r\n",
    "from torchvision.ops import nms, RoIAlign\r\n",
    "from PIL import Image\r\n",
    "from glob import glob\r\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection\n",
    "\n",
    "For this project, we collected data by taking about 150 pictures of different combinations of bills and coins.\n",
    "\n",
    "As an initial proof of concept, we decided to focus on the 7 most common classes of currencies in Singapore: \\\\$10, \\\\$5, \\\\$2, \\\\$1, 50c, 20c, 10c.\n",
    "\n",
    "After we gathered the data, we manually labelled the bounding boxes and their respective classes using [LabelImg](https://github.com/tzutalin/labelImg), which produces an XML file for each image to store the bounding box and class information."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Set class labels\r\n",
    "coin_labels = ('10c', '20c', '50c', '$1', '$2', '$5', '$10')\r\n",
    "\r\n",
    "label_index_dict = {k:v+1 for v, k in enumerate(coin_labels)}\r\n",
    "label_index_dict['background'] = 0\r\n",
    "print(label_index_dict)\r\n",
    "\r\n",
    "index_label_dict = {v+1:k for v, k in enumerate(coin_labels)}\r\n",
    "index_label_dict[0] = 'background'\r\n",
    "print(index_label_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'10c': 1, '20c': 2, '50c': 3, '$1': 4, '$2': 5, '$5': 6, '$10': 7, 'background': 0}\n",
      "{1: '10c', 2: '20c', 3: '50c', 4: '$1', 5: '$2', 6: '$5', 7: '$10', 0: 'background'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we defined a function to parse the XML files into a Python dictionary:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def parse_annotation(annotation_path):\r\n",
    "    '''\r\n",
    "    Function to convert XML data of a single image into an object.\r\n",
    "    The object contains the Bbox parameters and the corresponding label for each Bbox.\r\n",
    "    '''\r\n",
    "\r\n",
    "    # Parse the XML file into a tree structure\r\n",
    "    tree = ET.parse(annotation_path)\r\n",
    "    root = tree.getroot()\r\n",
    "    height = float(root.find('size').find('height').text)\r\n",
    "    width = float(root.find('size').find('width').text)\r\n",
    "\r\n",
    "    # Set initial lists\r\n",
    "    boxes = list()\r\n",
    "    labels = list()\r\n",
    "\r\n",
    "    # Loop over each Bbox found in the XML file\r\n",
    "    for object in root.iter('object'):\r\n",
    "\r\n",
    "        # Convert Bbox co-ordinates\r\n",
    "        bbox = object.find('bndbox')\r\n",
    "        xmin = int(bbox.find('xmin').text) - 1\r\n",
    "        ymin = int(bbox.find('ymin').text) - 1\r\n",
    "        xmax = int(bbox.find('xmax').text) - 1\r\n",
    "        ymax = int(bbox.find('ymax').text) - 1\r\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\r\n",
    "\r\n",
    "        # Convert Bbox Label\r\n",
    "        label = object.find('name').text.lower().strip()\r\n",
    "        if label not in label_index_dict:\r\n",
    "            continue\r\n",
    "        labels.append(label_index_dict[label])\r\n",
    "\r\n",
    "    return {'boxes': boxes, 'labels': labels}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we defined another function that makes use of the earlier `parse_annotation` function to convert the XML files into JSON objects.\n",
    "\n",
    "Each bounding box is stored as [xmin, ymin, xmax, ymax]."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def xml_to_json(*files):\r\n",
    "    '''\r\n",
    "    Function to convert image and XML data into two separate JSON objects.\r\n",
    "    One object for images, and another object for Bbox co-ordinate values and labels.\r\n",
    "    '''\r\n",
    "    \r\n",
    "    # Initialise lists\r\n",
    "    images_list = [] \r\n",
    "    objects_list = []\r\n",
    "    files = [file for sublist in files for file in sublist]\r\n",
    "    \r\n",
    "    # Set up two JSON files to be written\r\n",
    "    images_file = open(\"TRAIN_images.json\", 'w')\r\n",
    "    objects_file = open(\"TRAIN_objects.json\", 'w')\r\n",
    "    \r\n",
    "    # Iterate through each XML-Image pair\r\n",
    "    for file in files:\r\n",
    "    \r\n",
    "        # Add each image file path into the images list\r\n",
    "        file_path = os.path.splitext(file)[0]   \r\n",
    "        images_list.append(file_path + \".jpg\")\r\n",
    "        \r\n",
    "        # Add each XML object into the objects list\r\n",
    "        xml_dict = parse_annotation(file)\r\n",
    "        objects_list.append(xml_dict)\r\n",
    "    \r\n",
    "    # Write each list into the corresponding JSON files\r\n",
    "    json.dump(images_list, images_file)\r\n",
    "    json.dump(objects_list, objects_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we convert our dataset from XML to JSON accordingly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "CH_FILES = glob(r'dataset/ch dataset/*.xml')\r\n",
    "xml_to_json(CH_FILES)\r\n",
    "\r\n",
    "# JY_FILES = glob(r'dataset/joey dataset/*.xml')\r\n",
    "# HW_FILES = glob(r'dataset/hw dataset/*.xml')\r\n",
    "# xml_to_json(CH_FILES, HW_FILES)\r\n",
    "# xml_to_json(HW_FILES)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before getting started with our network, we need to do the necessary data preparation and pre-processing...\n",
    "\n",
    "The function `normalise_and_resize` was partially adapted from the [PyTorch Tutorial to Object Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py).\n",
    "\n",
    "First, we declare a function to resize and normalise the given images and the bounding boxes. By default, we set it to dimensions `(320, 320)`, which is the standard input size that we will use for our network..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def normalise_and_resize(image, boxes, labels, dims = (320, 320)):\r\n",
    "    '''\r\n",
    "    Adapted from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\r\n",
    "    Resize the input images and the bounding boxes, and apply Normalisations.\r\n",
    "    \r\n",
    "    :param image: image, a PIL Image\r\n",
    "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\r\n",
    "    :param labels: labels of objects, a tensor of dimensions (n_objects)\r\n",
    "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\r\n",
    "    '''\r\n",
    "\r\n",
    "    # Mean and Standard deviation used for the base VGG from torchvision\r\n",
    "    # See: https://pytorch.org/docs/stable/torchvision/models.html\r\n",
    "    mean = [0.485, 0.456, 0.406]\r\n",
    "    std = [0.229, 0.224, 0.225]\r\n",
    "\r\n",
    "    # Resize image and convert the image to Torch tensor\r\n",
    "    new_image = FT.resize(image, dims)\r\n",
    "    new_image = FT.to_tensor(new_image)\r\n",
    "    \r\n",
    "    # Normalize the image by mean and standard deviation\r\n",
    "    new_image = FT.normalize(new_image, mean = mean, std = std)\r\n",
    "\r\n",
    "    # Resize Bounding boxes\r\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\r\n",
    "    new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\r\n",
    "    new_boxes = (boxes / old_dims) * new_dims\r\n",
    "    \r\n",
    "    return new_image, new_boxes, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we declare a class PascalVOCDataset (Dataset in Pascal VOC format) that we can use to create batches of data easily and efficiently..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class PascalVOCDataset(Dataset):\r\n",
    "    \"\"\"\r\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader.\r\n",
    "    This class is primarily used to create batches.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    def __init__(self, data_folder, split):\r\n",
    "        '''\r\n",
    "        :param data_folder: folder where data files are stored\r\n",
    "        :param split: split, one of 'TRAIN' or 'TEST'\r\n",
    "        '''\r\n",
    "\r\n",
    "        self.data_folder = data_folder\r\n",
    "        self.split = split.upper()\r\n",
    "        \r\n",
    "        # The values of split should only be either 'TRAIN' or 'TEST'\r\n",
    "        assert self.split in {'TRAIN', 'TEST'}\r\n",
    "\r\n",
    "        # Read from the JSON files (initially craeted from our XML files)\r\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\r\n",
    "            self.images = json.load(j)\r\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\r\n",
    "            self.objects = json.load(j)\r\n",
    "\r\n",
    "        # Number of images must match the number of objects containing the Bboxes for each image\r\n",
    "        assert len(self.images) == len(self.objects)\r\n",
    "\r\n",
    "    def __getitem__(self, i):\r\n",
    "        '''\r\n",
    "        ...\r\n",
    "        '''\r\n",
    "\r\n",
    "        # Read image\r\n",
    "        image = Image.open(self.images[i], mode='r')\r\n",
    "        image = image.convert('RGB')\r\n",
    "        image_tensor = transforms.ToTensor()(image)\r\n",
    "        image_tensor = transforms.Resize(size = (320, 320))(image_tensor)\r\n",
    "\r\n",
    "        # Read objects in this image (Bboxes, labels)\r\n",
    "        objects = self.objects[i]\r\n",
    "        box = torch.FloatTensor(objects['boxes'])\r\n",
    "        label = torch.LongTensor(objects['labels'])\r\n",
    "\r\n",
    "        # Apply normalisations and resizes\r\n",
    "        image, box, label = normalise_and_resize(image, box, label)\r\n",
    "        box_and_label = torch.cat([box, label.unsqueeze(1)], 1)\r\n",
    "        return image_tensor, box_and_label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.images)\r\n",
    "\r\n",
    "    def collate_fn(self, batch):\r\n",
    "        '''\r\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\r\n",
    "        This describes how to combine these tensors of different sizes using Python lists.\r\n",
    "        \r\n",
    "        :param batch: an iterable of N sets from __getitem__()\r\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\r\n",
    "        '''\r\n",
    "\r\n",
    "        images = list()\r\n",
    "        boxes = list()\r\n",
    "\r\n",
    "        for b in batch:\r\n",
    "            images.append(b[0])\r\n",
    "            boxes.append(b[1])\r\n",
    "\r\n",
    "        images = torch.stack(images, dim = 0)\r\n",
    "        return images, boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we load our training and test dataset using PyTorch's DataLoader, which allows us to iterate through the dataset easily. The original dataset was randomly split into the train and test set, with a ratio of 80:20. \n",
    "\n",
    "Additionally, since our dataset was relatively small, we chose to stick to a batch size of 1 to achieve better training stability and generalization performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dataset = PascalVOCDataset(\".\", \"TRAIN\")\r\n",
    "length = len(dataset)\r\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [math.floor(0.8*length), math.ceil(0.2*length)])\r\n",
    "train_dataloader = DataLoader(train_data, batch_size = 1, shuffle = True, collate_fn = dataset.collate_fn)\r\n",
    "test_dataloader = DataLoader(test_data, batch_size = 1, shuffle = True, collate_fn = dataset.collate_fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Visualisation\n",
    "\n",
    "To ensure that our Data Preparations are complete, we may use the `matplotlib` library to visualise our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.patches import Rectangle\r\n",
    "import math\r\n",
    "\r\n",
    "color_maps = ['r', 'g', 'b', 'y', 'm', 'w', 'k', 'c']\r\n",
    "\r\n",
    "# Visualise data\r\n",
    "def visualise_data(dataloader):\r\n",
    "    '''\r\n",
    "    ...\r\n",
    "    '''\r\n",
    "    \r\n",
    "    for data in dataloader:\r\n",
    "        for batch in range(1):\r\n",
    "            img = data[0][batch]\r\n",
    "            boxes = data[1][batch]\r\n",
    "    #         labels = data[2][batch].tolist()\r\n",
    "    #         named_labels = [index_label_dict[label] for label in labels]\r\n",
    "            plt.imshow(transforms.ToPILImage()(img))\r\n",
    "            ax = plt.gca()\r\n",
    "            labelled = set()\r\n",
    "            for i, box in enumerate(boxes):\r\n",
    "                w,h = box[2] - box[0], box[3] - box[1]\r\n",
    "                x,y = box[0].item(), box[1].item()\r\n",
    "                x = [x, x + w, x + w, x, x]\r\n",
    "                y = [y, y, y + h, y + h, y]\r\n",
    "                label = int(box[4].item())\r\n",
    "                if label not in labelled:\r\n",
    "                    plt.plot(x,y, color = color_maps[label], label = index_label_dict[label])\r\n",
    "                    labelled.add(label)\r\n",
    "                else:\r\n",
    "                    plt.plot(x,y, color = color_maps[label])\r\n",
    "                plt.legend(loc = 'best')\r\n",
    "            break\r\n",
    "        break\r\n",
    "\r\n",
    "def display_img_bbox(img, gt_bboxes, predictions=()):\r\n",
    "    '''\r\n",
    "    ...\r\n",
    "    '''\r\n",
    "    \r\n",
    "    plt.imshow(transforms.ToPILImage()(img))\r\n",
    "    gt_bbox_color = 'g'\r\n",
    "    pred_color = 'r'\r\n",
    "    fs = 10\r\n",
    "    # ax = plt.gca()\r\n",
    "    # labelled = set()\r\n",
    "\r\n",
    "    # plot gt_boxes\r\n",
    "    for bbox in gt_bboxes:\r\n",
    "        w,h = bbox[2] - bbox[0], bbox[3] - bbox[1]\r\n",
    "        x,y = bbox[0], bbox[1]\r\n",
    "        x = [x, x + w, x + w, x, x]\r\n",
    "        y = [y, y, y + h, y + h, y]\r\n",
    "        label_idx = bbox[4].item()\r\n",
    "        label = index_label_dict[label_idx]\r\n",
    "        plt.text(x[0], y[0], label, fontsize=fs)\r\n",
    "        plt.plot(x,y, color = gt_bbox_color, label = label)\r\n",
    "    \r\n",
    "    # plot predicted boxes\r\n",
    "    for bbox in predictions:\r\n",
    "        w,h = bbox[2] - bbox[0], bbox[3] - bbox[1]\r\n",
    "        x,y = bbox[0], bbox[1]\r\n",
    "        x = [x, x + w, x + w, x, x]\r\n",
    "        y = [y, y, y + h, y + h, y]\r\n",
    "        label = index_label_dict[bbox[4]]\r\n",
    "        plt.text(x[0], y[0], label, fontsize=fs)\r\n",
    "        plt.plot(x,y, color = pred_color, label = label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constructing the Model \n",
    "\n",
    "In order to simplify our model, we first declared a few utility functions.\n",
    "\n",
    "Firstly, we declare functions that will help us generate the anchors for our network. To do so efficiently, we need functions to help us switch the data format of the images (and the bounding boxes) from the Pascal VOC format to the YOLO format:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def pascal2yolo(anchor):\r\n",
    "    '''\r\n",
    "    Transforms anchor coordinates of the form [xmin, ymin, xmax, ymax] to [x_center, y_center, width, height]\r\n",
    "    '''\r\n",
    "    \r\n",
    "    width = anchor[2] - anchor[0] + 1\r\n",
    "    height = anchor[3] - anchor[1] + 1\r\n",
    "    x_ctr = anchor[0] + (width-1)/2 \r\n",
    "    y_ctr = anchor[1] + (height-1)/2\r\n",
    "    return width, height, x_ctr, y_ctr\r\n",
    "\r\n",
    "def yolo2pascal(width, height, x_ctr, y_ctr):\r\n",
    "    '''\r\n",
    "    Transforms anchor coordinates of the form [x_center, y_center, width, height] to [xmin, ymin, xmax, ymax]\r\n",
    "    '''\r\n",
    "    \r\n",
    "    width = width[:, np.newaxis]\r\n",
    "    height = height[:, np.newaxis]\r\n",
    "    anchors = np.hstack((x_ctr - 0.5 * (width - 1), y_ctr - 0.5 * (height - 1),\r\n",
    "                         x_ctr + 0.5 * (width - 1), y_ctr + 0.5 * (height - 1)))\r\n",
    "    return anchors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we declare utility functions for generating the anchors themselves:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def generate_ratio_anchors(anchor, ratios=(0.5,1,2)):\r\n",
    "    '''\r\n",
    "    Generate anchors for each width:height ratio\r\n",
    "    '''\r\n",
    "    \r\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor)\r\n",
    "    size = w*h\r\n",
    "    size_ratios = size / ratios\r\n",
    "    ws = np.round(np.sqrt(size_ratios))\r\n",
    "    hs = np.round(ws * ratios)\r\n",
    "    anchors = yolo2pascal(ws, hs, x_ctr, y_ctr)\r\n",
    "    return anchors\r\n",
    "\r\n",
    "def generate_scale_anchors(anchor, scales=np.array((8,16,32))):\r\n",
    "    '''\r\n",
    "    Generate anchors for each scale\r\n",
    "    '''\r\n",
    "    \r\n",
    "    w,h,x_ctr,y_ctr = pascal2yolo(anchor) \r\n",
    "    scaled_w = w * scales\r\n",
    "    scaled_h = h * scales\r\n",
    "    anchors = yolo2pascal(scaled_w, scaled_h, x_ctr, y_ctr)\r\n",
    "    return anchors\r\n",
    "\r\n",
    "def generate_anchors(height, width, aspect_ratio=np.array((0.5,1,2)), stride_length=16, scales=np.array((8,16,32))):\r\n",
    "    '''\r\n",
    "    Generate anchors of differing scale and aspect ratios\r\n",
    "    '''\r\n",
    "    \r\n",
    "    base_anchor = pascal2yolo([0,0,15,15]) # 16, 16, 7.5, 7.5\r\n",
    "    ratio_anchors = generate_ratio_anchors(base_anchor, ratios=aspect_ratio)\r\n",
    "    anchors = np.vstack([\r\n",
    "        generate_scale_anchors(ratio_anchors[i, :], scales)\r\n",
    "        for i in range(ratio_anchors.shape[0])\r\n",
    "    ])\r\n",
    "    A = anchors.shape[0]\r\n",
    "    shift_x = np.arange(0, width) * stride_length\r\n",
    "    shift_y = np.arange(0, height) * stride_length\r\n",
    "    \r\n",
    "    # Shift each ratio\r\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\r\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\r\n",
    "                        shift_y.ravel())).transpose()\r\n",
    "    K = shifts.shape[0]\r\n",
    "    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)) # H, W, C\r\n",
    "    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\r\n",
    "    length = np.int32(anchors.shape[0])\r\n",
    "\r\n",
    "    return anchors, length"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we declare a couple of utility functions to help us manipulate and deal with bounding boxes.\n",
    "\n",
    "BBox transformations using deltas & inverse ...\n",
    "\n",
    "Clip bboxes to image boundaries...\n",
    "\n",
    "Calculate bbox overlaps..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def bbox_transform(ex_rois, gt_rois):\r\n",
    "    '''\r\n",
    "    ...\r\n",
    "    '''\r\n",
    "    \r\n",
    "    ex_rois = ex_rois.to(device)\r\n",
    "    gt_rois = gt_rois.to(device)\r\n",
    "    \r\n",
    "    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\r\n",
    "    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\r\n",
    "    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\r\n",
    "    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\r\n",
    "\r\n",
    "    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\r\n",
    "    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\r\n",
    "    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\r\n",
    "    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\r\n",
    "\r\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\r\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\r\n",
    "    targets_dw = torch.log(gt_widths / ex_widths)\r\n",
    "    targets_dh = torch.log(gt_heights / ex_heights)\r\n",
    "\r\n",
    "    targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), 1)\r\n",
    "    return targets.cpu()\r\n",
    "\r\n",
    "def bbox_transform_inv(boxes, deltas):\r\n",
    "    '''\r\n",
    "    ...\r\n",
    "    '''\r\n",
    "\r\n",
    "    if len(boxes) == 0:\r\n",
    "        return deltas.detach() * 0\r\n",
    "    boxes = boxes.to(device)\r\n",
    "    widths = boxes[:, 2] - boxes[:, 0] + 1.0\r\n",
    "    heights = boxes[:, 3] - boxes[:, 1] + 1.0\r\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\r\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\r\n",
    "    dx = deltas[:, 0::4]\r\n",
    "    dy = deltas[:, 1::4]\r\n",
    "    dw = deltas[:, 2::4]\r\n",
    "    dh = deltas[:, 3::4]\r\n",
    "\r\n",
    "    pred_ctr_x = dx * widths.unsqueeze(1) + ctr_x.unsqueeze(1)\r\n",
    "    pred_ctr_y = dy * heights.unsqueeze(1) + ctr_y.unsqueeze(1)\r\n",
    "    pred_w = torch.exp(dw) * widths.unsqueeze(1)\r\n",
    "    pred_h = torch.exp(dh) * heights.unsqueeze(1)\r\n",
    "\r\n",
    "    pred_boxes = torch.cat([_.unsqueeze(2) for _ in [pred_ctr_x - 0.5 * pred_w,\r\n",
    "        pred_ctr_y - 0.5 * pred_h,\r\n",
    "        pred_ctr_x + 0.5 * pred_w,\r\n",
    "        pred_ctr_y + 0.5 * pred_h]], 2).view(len(boxes), -1)\r\n",
    "    return pred_boxes\r\n",
    "\r\n",
    "def clip_boxes(boxes, im_shape):\r\n",
    "    '''\r\n",
    "    Clip boxes to image boundaries\r\n",
    "    '''\r\n",
    "\r\n",
    "    if not hasattr(boxes, 'data'):\r\n",
    "        boxes_ = boxes.numpy()\r\n",
    "\r\n",
    "    boxes = boxes.view(boxes.size(0), -1, 4)\r\n",
    "    boxes = torch.stack([boxes[:,:,0].clamp(0, im_shape[1] - 1),\r\n",
    "        boxes[:,:,1].clamp(0, im_shape[0] - 1),\r\n",
    "        boxes[:,:,2].clamp(0, im_shape[1] - 1),\r\n",
    "        boxes[:,:,3].clamp(0, im_shape[0] - 1)], 2).view(boxes.size(0), -1)\r\n",
    "    return boxes\r\n",
    "\r\n",
    "def bbox_overlaps(boxes, query_boxes):\r\n",
    "    '''\r\n",
    "    Computes the overlapped area between boxes and query boxes (ground truth boxes)\r\n",
    "    using intersection-over-union\r\n",
    "    \r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    boxes: (N, 4) ndarray or tensor or variable\r\n",
    "    query_boxes: (K, 4) ndarray or tensor or variable\r\n",
    "    Returns\r\n",
    "    -------\r\n",
    "    overlaps: (N, K) overlap between boxes and query_boxes\r\n",
    "    '''\r\n",
    "    \r\n",
    "    # If input is ndarray, turn the overlaps back to ndarray when return\r\n",
    "    if isinstance(boxes, np.ndarray):\r\n",
    "        boxes = torch.from_numpy(boxes)\r\n",
    "        query_boxes = torch.from_numpy(query_boxes)\r\n",
    "        out_fn = lambda x: x.numpy()  \r\n",
    "    else:\r\n",
    "        out_fn = lambda x: x\r\n",
    "\r\n",
    "    boxes = boxes.to(device)\r\n",
    "    query_boxes = query_boxes.to(device)\r\n",
    "    box_areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\r\n",
    "    query_areas = (query_boxes[:, 2] - query_boxes[:, 0] + 1) * (query_boxes[:, 3] - query_boxes[:, 1] + 1)\r\n",
    "\r\n",
    "    iw = (torch.min(boxes[:, 2:3], query_boxes[:, 2:3].t()) - torch.max(boxes[:, 0:1], query_boxes[:, 0:1].t()) + 1).clamp(min=0)\r\n",
    "    ih = (torch.min(boxes[:, 3:4], query_boxes[:, 3:4].t()) - torch.max(boxes[:, 1:2], query_boxes[:, 1:2].t()) + 1).clamp(min=0)\r\n",
    "    ua = box_areas.view(-1, 1) + query_areas.view(1, -1) - iw * ih\r\n",
    "    overlaps = iw * ih / ua\r\n",
    "    return out_fn(overlaps.cpu())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unmap function for the anchor target layer..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def unmap(data, count, inds, fill=0):\r\n",
    "    '''\r\n",
    "    Unmap a subset of item (data) back to the original set of items (of size count)\r\n",
    "    '''\r\n",
    "    \r\n",
    "    if len(data.shape) == 1:\r\n",
    "        ret = np.empty((count, ), dtype=np.float32)\r\n",
    "        ret.fill(fill)\r\n",
    "        ret[inds] = data\r\n",
    "    else:\r\n",
    "        ret = np.empty((count, ) + data.shape[1:], dtype=np.float32)\r\n",
    "        ret.fill(fill)\r\n",
    "        ret[inds, :] = data\r\n",
    "    return ret"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def get_bbox_regression_labels(bbox_target_data, num_classes):\r\n",
    "    '''\r\n",
    "    Bounding-box regression targets (bbox_target_data) are stored in a\r\n",
    "    compact form N x (class, tx, ty, tw, th)\r\n",
    "    This function expands those targets into the 4-of-4*K representation used\r\n",
    "    by the network (i.e. only one class has non-zero targets).\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        bbox_target (ndarray): N x 4K blob of regression targets\r\n",
    "        bbox_inside_weights (ndarray): N x 4K blob of loss weights\r\n",
    "    '''\r\n",
    "\r\n",
    "    clss = bbox_target_data[:, 0]\r\n",
    "    bbox_targets = clss.new_zeros(clss.numel(), 4 * num_classes)\r\n",
    "    bbox_inside_weights = clss.new_zeros(bbox_targets.shape)\r\n",
    "    inds = (clss > 0).nonzero().view(-1)\r\n",
    "    if inds.numel() > 0:\r\n",
    "        clss = clss[inds].contiguous().view(-1, 1)\r\n",
    "        dim1_inds = inds.unsqueeze(1).expand(inds.size(0), 4)\r\n",
    "        dim2_inds = torch.cat([4 * clss, 4 * clss + 1, 4 * clss + 2, 4 * clss + 3], 1).long()\r\n",
    "        test = bbox_target_data[inds][:, 1:]\r\n",
    "        bbox_targets[dim1_inds, dim2_inds] = test\r\n",
    "        bbox_inside_weights[dim1_inds, dim2_inds] = bbox_targets.new(BBOX_INSIDE_WEIGHTS).view(-1, 4).expand_as(dim1_inds)\r\n",
    "\r\n",
    "    return bbox_targets, bbox_inside_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def fix_sample_regions(fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image):\r\n",
    "    '''\r\n",
    "    Function to ensure total number of foreground and background ROIs is constant\r\n",
    "    by randomly repeating background indices to make up for lesser foreground indices.\r\n",
    "    '''\r\n",
    "    \r\n",
    "    if fg_inds.numel() == 0 and bg_inds.numel() == 0:\r\n",
    "        to_replace = all_rois.size(0) < rois_per_image\r\n",
    "        bg_inds = torch.from_numpy(npr.choice(np.arange(0, all_rois.size(0)), size=int(rois_per_image), replace=to_replace)).long()\r\n",
    "        fg_rois_per_image = 0\r\n",
    "    elif fg_inds.numel() > 0 and bg_inds.numel() > 0:\r\n",
    "        fg_rois_per_image = min(fg_rois_per_image, fg_inds.numel())\r\n",
    "        fg_inds = fg_inds[torch.from_numpy(\r\n",
    "            npr.choice(\r\n",
    "                np.arange(0, fg_inds.numel()),\r\n",
    "                size=int(fg_rois_per_image),\r\n",
    "                replace=False)).long().to(gt_boxes.device)]\r\n",
    "        bg_rois_per_image = rois_per_image - fg_rois_per_image\r\n",
    "        to_replace = bg_inds.numel() < bg_rois_per_image\r\n",
    "        bg_inds = bg_inds[torch.from_numpy(\r\n",
    "            npr.choice(\r\n",
    "                np.arange(0, bg_inds.numel()),\r\n",
    "                size=int(bg_rois_per_image),\r\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\r\n",
    "    elif fg_inds.numel() > 0:\r\n",
    "        to_replace = fg_inds.numel() < rois_per_image\r\n",
    "        fg_inds = fg_inds[torch.from_numpy(\r\n",
    "            npr.choice(\r\n",
    "                np.arange(0, fg_inds.numel()),\r\n",
    "                size=int(rois_per_image),\r\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\r\n",
    "        fg_rois_per_image = rois_per_image\r\n",
    "    elif bg_inds.numel() > 0:\r\n",
    "        to_replace = bg_inds.numel() < rois_per_image\r\n",
    "        bg_inds = bg_inds[torch.from_numpy(\r\n",
    "            npr.choice(\r\n",
    "                np.arange(0, bg_inds.numel()),\r\n",
    "                size=int(rois_per_image),\r\n",
    "                replace=to_replace)).long().to(gt_boxes.device)]\r\n",
    "        fg_rois_per_image = 0\r\n",
    "\r\n",
    "    return fg_inds, bg_inds, gt_boxes, all_rois, rois_per_image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\r\n",
    "    \r\n",
    "    sigma_2 = sigma**2\r\n",
    "    box_diff = bbox_pred - bbox_targets\r\n",
    "    in_box_diff = bbox_inside_weights * box_diff\r\n",
    "    abs_in_box_diff = torch.abs(in_box_diff)\r\n",
    "    smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\r\n",
    "    \r\n",
    "    in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\r\n",
    "                  + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\r\n",
    "    out_loss_box = bbox_outside_weights * in_loss_box\r\n",
    "    loss_box = out_loss_box\r\n",
    "    \r\n",
    "    for i in sorted(dim, reverse=True):\r\n",
    "        loss_box = loss_box.sum(i)\r\n",
    "    loss_box = loss_box.mean()\r\n",
    "    \r\n",
    "    return loss_box"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Faster R-CNN Model\n",
    "\n",
    "First, we set the network constants as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Set network constants\r\n",
    "HIDDEN_DIM = 64\r\n",
    "NUM_OF_CLASS = 8\r\n",
    "BATCH_SIZE = 1\r\n",
    "\r\n",
    "# Thresholds\r\n",
    "# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\r\n",
    "# Overlap threshold for a ROI to be considered background (class = 0 if overlap in [LO, HI))\r\n",
    "FG_THRESH = 0.5 \r\n",
    "BG_THRESH_HI = 0.5\r\n",
    "BG_THRESH_LO = 0.1\r\n",
    "FG_FRACTION = 0.5\r\n",
    "\r\n",
    "PRE_NMS_TOPN = 12000\r\n",
    "POST_NMS_TOPN = 2000\r\n",
    "NMS_THRESH = 0.7\r\n",
    "\r\n",
    "POSITIVE_OVERLAP = 0.7\r\n",
    "NEGATIVE_OVERLAP = 0.3\r\n",
    "CLOBBER_POSITIVES = False\r\n",
    "POOLING_SIZE = 7\r\n",
    "RPN_BATCH_SIZE = 32\r\n",
    "RPN_POSITIVE_WEIGHT = -1.0\r\n",
    "\r\n",
    "BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\r\n",
    "BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\r\n",
    "BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\r\n",
    "\r\n",
    "ANCHOR_SCALES = (8, 16, 32)\r\n",
    "ANCHOR_RATIOS = (0.5, 1, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The flow of our faster RCNN model is as follows:\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 1. Head Network Layer\r\n",
    "\r\n",
    "The head network consists of 4 convolutional layers with max pooling and RELU activation between each layer. It serves as the \"backbone\" network and its purpose is to produce convolutional feature maps to be used in anchor generation and region proposal network.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. Anchor Generation Layer\r\n",
    "\r\n",
    "The anchor generation layer generates a fixed number of anchors. We first generated 9 anchors of differing scales (8, 16, 32) and aspect ratios (0.5, 1, 2) and then duplicated these anchors by translating them across uniformly spaced grid points spanning the input image.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. Region Proposal Network (RPN)\r\n",
    "\r\n",
    "The region proposal layer runs feature maps produced by the head network through a convolutional layer followed by RELU. Its purpose is to produce the background/foreground class scores and probabilities, and corresponding bounding box regression coefficients.\r\n",
    "\r\n",
    "Input: features from head network, bounding boxes, anchors generated\r\n",
    "        Output: rois\r\n",
    "        1. Proposal Layer\r\n",
    "        2. Anchor Target Layer\r\n",
    "        3. Compute RPN Loss\r\n",
    "        4. Proposal Target Layer\r\n",
    "        \r\n",
    "        Features -> class probabilities of anchors(fg or bg) and bbox coeff of anchors (to adjust them)\r\n",
    "\r\n",
    "\r\n",
    "### 3.1. Proposal Layer\r\n",
    "\r\n",
    "The proposal layer prunes the number of boxes and transforms the bounding boxes. The pruning is done by applying non-maximum suppression based on the foreground scores, while the transformation is done by applying the regression coefficients generated by the RPN to the corresponding anchor boxes. The purpose of this layer is to produce ROIs.\r\n",
    "\r\n",
    "Prunes no. of boxes using NMS based on fg scores and transforms bbox using regression coeff\r\n",
    "bbox_pred: BATCH_SIZE * h * w * (num_anchors*4)  \r\n",
    "\r\n",
    "### 3.2. Anchor Target Layer\r\n",
    "\r\n",
    "The anchor target layer selects promising anchors from the anchors generated by first computing the overlap between the anchors and the ground truth boxes. The anchors will then be selected based on certain overlap thresholds. These anchors will then be used to train the RPN. \r\n",
    "\r\n",
    "\r\n",
    "Parameters\r\n",
    "        rpn_cls_score: Class scores generated by the Region Proposal Network\r\n",
    "        gt_boxes: Ground Truth boxes\r\n",
    "        all_anchors: Anchor boxes generated by the anchor generation layer\r\n",
    "        \r\n",
    "        ### Fixed Parameters ###\r\n",
    "        im_info: Image dimensions\r\n",
    "        num_anchors: Number of different Anchor boxes used. By default, it is set to 9 here.\r\n",
    "\r\n",
    "        ### Additional information ###\r\n",
    "        POSITIVE_OVERLAP:       Threshold used to select if an anchor box is a good foreground box (Default: 0.7)\r\n",
    "\r\n",
    "        NEGATIVE_OVERLAP:       If the max overlap of a anchor from a ground truth box is lower than this thershold, it is marked as background. \r\n",
    "                                Boxes whose overlap is > than NEGATIVE_OVERLAP but < POSITIVE_OVERLAP are marked \r\n",
    "                                “don’t care”. (Default: 0.3)\r\n",
    "        \r\n",
    "        CLOBBER_POSITIVES:      If a particular anchor is satisfied by both the positive and the negative conditions,\r\n",
    "                                and if this value is set to False, then set the anchor to a negative example.\r\n",
    "                                Else, set the anchor to a positive example.\r\n",
    "        \r\n",
    "        RPN_BATCH_SIZE:                 Total number of background and foreground anchors. (Default: 256)\r\n",
    "        \r\n",
    "        FG_FRACTION:            Fraction of the batch size that is foreground anchors (Default: 0.5). \r\n",
    "                                If the number of foreground anchors found is larger than RPN_BATCH_SIZE * FG_FRACTION, \r\n",
    "                                the excess (indices are selected randomly) is marked “don’t care”.\r\n",
    "                            \r\n",
    "        RPN_POSITIVE_WEIGHT:    Using this value:\r\n",
    "                                Positive RPN examples are given a weight of RPN_POSITIVE_WEIGHT * 1 / num_of_positives\r\n",
    "                                Negative RPN examples are given a weight of (1 - RPN_POSITIVE_WEIGHT)\r\n",
    "                                Set to -1 by default, which will ensure uniform example weighting.\r\n",
    "\r\n",
    "\r\n",
    "### 3.3. Compute RPN Loss\r\n",
    "\r\n",
    "### 3.4. Proposal Target Layer\r\n",
    "\r\n",
    "The proposal target layer selects promising ROIs that are generated by first computing the max overlap between each ROI and all ground truth boxes. The ROIs will then be selected based on certain overlap thresholds.\r\n",
    "\r\n",
    "1. Calculate overlap between ROI and GT boxes\r\n",
    "        2. Select promising ROIs by comparing against threshold(s)\r\n",
    "        3. Compute bounding box target regression targets and get bounding box regression labels\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 4. ROI Pooling Layer\r\n",
    "\r\n",
    "The ROI pooling layer crops the features produced by the head network using the ROI Align method (interpolation).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 5. Region Classification Network (RCNN)\r\n",
    "\r\n",
    "The region classification layer passes the feature maps produced by the head network through a single convolutional layer and two fully connected (FC) layers. The first FC layer produces the class probability distribution for each region proposal and the second FC layer produces a set of class specific bounding box regressors.\r\n",
    "\r\n",
    "cls_score\r\n",
    "            Linear layer (fc7 channels, num_classes)\r\n",
    "            torch max\r\n",
    "            softmax\r\n",
    "        bbox_pred\r\n",
    "            Linear layer (fc7 channels, num_classes*4)\r\n",
    "\r\n",
    "### 5.1. Classification Layer\r\n",
    "\r\n",
    "### 5.2 Compute RCNN Loss\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 6. Compute Total Loss\r\n",
    "\r\n",
    "The total loss is the sum of the classification loss and bounding box regression loss in both the RPN and RCNN. The classification loss uses cross entropy loss to penalize incorrectly classified boxes while the regression loss uses a function of the distance between the true regression coefficients."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class faster_R_CNN(nn.Module):\r\n",
    "    '''\r\n",
    "    The main Faster R-CNN network used for this project.\r\n",
    "    '''\r\n",
    "    \r\n",
    "    def __init__(self):\r\n",
    "        super(faster_R_CNN, self).__init__()\r\n",
    "        \r\n",
    "        # Python dictionaries to contain the various values obtained during the training process\r\n",
    "        self._predictions = {}\r\n",
    "        self._losses = {}\r\n",
    "        self._anchor_targets = {}\r\n",
    "        self._proposal_targets = {}\r\n",
    "        \r\n",
    "        # Network constants\r\n",
    "        self.fc_channels = 256\r\n",
    "        self.net_conv_channels = 1024\r\n",
    "\r\n",
    "        # Set the number of anchors used for convolutions\r\n",
    "        self.num_of_anchors = len(ANCHOR_SCALES) * len(ANCHOR_RATIOS)  \r\n",
    "        \r\n",
    "        # Head Net: Generating a series of Feature maps from the input image\r\n",
    "        # Current Size: 3 x h x w\r\n",
    "        self.head_conv1 = nn.Conv2d(3,  HIDDEN_DIM,  kernel_size=4, stride=2, padding=1)\r\n",
    "        # Current Size: 64 x h/2 x w/2\r\n",
    "        self.head_batch_norm1 = nn.BatchNorm2d(HIDDEN_DIM)\r\n",
    "        # Current Size: 64 x h/2 x w/2 \r\n",
    "        self.head_relu1 = nn.ReLU()\r\n",
    "        # Current Size: 64 x h/2 x w/2\r\n",
    "        self.head_pool1 = nn.MaxPool2d([3,3], padding=1, stride=2)\r\n",
    "        # Current Size: 64 x h/4 x w/4\r\n",
    "        self.head_layer1 = nn.Conv2d(HIDDEN_DIM,  HIDDEN_DIM*4,  kernel_size=3, padding=1)\r\n",
    "        self.head_relu2 = nn.ReLU()\r\n",
    "        # Current Size: 256 x h/4 x w/4\r\n",
    "        self.head_layer2 = nn.Conv2d(HIDDEN_DIM*4,  HIDDEN_DIM*8,  kernel_size=3, padding=1)\r\n",
    "        self.head_pool2 = nn.MaxPool2d([3,3], padding=1, stride=2)\r\n",
    "        self.head_relu3 = nn.ReLU()\r\n",
    "        # Current Size: 512 x h/8 x w/8\r\n",
    "        self.head_layer3 = nn.Conv2d(HIDDEN_DIM*8,  HIDDEN_DIM*16,  kernel_size=3, padding=1)\r\n",
    "        self.head_pool3 = nn.MaxPool2d([3,3], padding=1, stride=2)\r\n",
    "        self.head_relu4 = nn.ReLU()\r\n",
    "        # Current Size: 1024 x h/16 x w/16\r\n",
    "\r\n",
    "        # Test\r\n",
    "        self.test_conv = nn.Conv2d(self.net_conv_channels, 256,  kernel_size=3, padding=1)\r\n",
    "        self.test_avgpool = nn.AvgPool2d([3,3], 1)\r\n",
    "\r\n",
    "        # FC layer\r\n",
    "        self.fc_layer1 = nn.Linear(6400, 1000)\r\n",
    "        self.fc_layer2 = nn.Linear(1000, self.fc_channels)\r\n",
    "\r\n",
    "        # Region Proposal Network\r\n",
    "        self.rpn_net = nn.Conv2d(self.net_conv_channels, 512 , kernel_size=3, padding=1)\r\n",
    "        self.rpn_cls_score_net = nn.Conv2d(512, self.num_of_anchors*2, [1,1])\r\n",
    "        self.rpn_bbox_pred_net = nn.Conv2d(512, self.num_of_anchors*4, [1,1])\r\n",
    "\r\n",
    "        # Classification Network\r\n",
    "        self.cls_score_net = nn.Linear(self.fc_channels, NUM_OF_CLASS)\r\n",
    "        self.bbox_pred_net = nn.Linear(self.fc_channels, NUM_OF_CLASS*4)\r\n",
    "\r\n",
    "    def head_net_layer(self):\r\n",
    "        '''\r\n",
    "        1. Head Network Layer\r\n",
    "        '''\r\n",
    "        \r\n",
    "        return nn.Sequential(\r\n",
    "            self.head_conv1,\r\n",
    "            self.head_batch_norm1,\r\n",
    "            self.head_relu1,\r\n",
    "            self.head_pool1,\r\n",
    "            self.head_layer1,\r\n",
    "            self.head_relu2,\r\n",
    "            self.head_layer2,\r\n",
    "            self.head_pool2,\r\n",
    "            self.head_relu3,\r\n",
    "            self.head_layer3,\r\n",
    "            self.head_pool3,\r\n",
    "            self.head_relu4\r\n",
    "        )\r\n",
    "    \r\n",
    "    def anchor_generation_layer(self, head_net_output):\r\n",
    "        '''\r\n",
    "        2. Anchor Generation Layer\r\n",
    "        '''\r\n",
    "        \r\n",
    "        anchors, length = generate_anchors(head_net_output.size(2), head_net_output.size(3))\r\n",
    "        anchors = torch.from_numpy(anchors)\r\n",
    "        return anchors\r\n",
    "    \r\n",
    "    def proposal_layer(self, cls_prob, bbox_pred, anchors, num_of_anchors):\r\n",
    "        '''\r\n",
    "        3.1. Proposal Layer\r\n",
    "        '''\r\n",
    "        \r\n",
    "        # Get the scores and bounding boxes\r\n",
    "        scores = cls_prob[:, :, :, num_of_anchors:]\r\n",
    "        rpn_bbox_pred = bbox_pred.view((-1, 4))\r\n",
    "        scores = scores.contiguous().view(-1, 1)\r\n",
    "        proposals = bbox_transform_inv(anchors, rpn_bbox_pred) # shift boxes based on prediction\r\n",
    "        proposals = clip_boxes(proposals, self._im_info[:2])\r\n",
    "\r\n",
    "        # NMS Selection\r\n",
    "        \r\n",
    "        # Pick the top region proposals\r\n",
    "        scores, order = scores.view(-1).sort(descending=True)\r\n",
    "        if PRE_NMS_TOPN > 0:\r\n",
    "            order = order[:PRE_NMS_TOPN]\r\n",
    "            scores = scores[:PRE_NMS_TOPN].view(-1, 1)\r\n",
    "        proposals = proposals[order.data, :]\r\n",
    "\r\n",
    "        # Non-maximal suppression\r\n",
    "        keep = nms(proposals, scores.squeeze(1), NMS_THRESH)\r\n",
    "\r\n",
    "        # Pick the top region proposals after NMS\r\n",
    "        if POST_NMS_TOPN > 0:\r\n",
    "            keep = keep[:POST_NMS_TOPN]\r\n",
    "        proposals = proposals[keep, :]\r\n",
    "        scores = scores[keep, ]\r\n",
    "\r\n",
    "        # Only support single image as input\r\n",
    "        batch_inds = proposals.new_zeros(proposals.size(0), 1)\r\n",
    "        blob = torch.cat((batch_inds, proposals), 1)\r\n",
    "        \r\n",
    "        return blob, scores\r\n",
    "\r\n",
    "    def anchor_target_layer(self, rpn_cls_score, gt_boxes, all_anchors, im_info=[320, 320, 1]):\r\n",
    "        '''\r\n",
    "        3.2. Anchor Target Layer\r\n",
    "        '''\r\n",
    "\r\n",
    "        # Map of shape (..., H, W)\r\n",
    "        height, width = rpn_cls_score.shape[1:3]\r\n",
    "\r\n",
    "        # Only keep anchors that are completely inside the image\r\n",
    "        inds_inside = np.where(\r\n",
    "            (all_anchors[:, 0] >= 0) &\r\n",
    "            (all_anchors[:, 1] >= 0) &\r\n",
    "            (all_anchors[:, 2] < im_info[1]) &  # Width\r\n",
    "            (all_anchors[:, 3] < im_info[0])  # Height\r\n",
    "        )[0]\r\n",
    "        anchors = all_anchors[inds_inside, :]\r\n",
    "\r\n",
    "        # Label: 1 is positive, 0 is negative, -1 is dont care\r\n",
    "        labels = np.empty((len(inds_inside)), dtype=np.float32)\r\n",
    "        labels.fill(-1)\r\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\r\n",
    "        \r\n",
    "        # Overlaps between the Anchors and the Ground Truth boxes\r\n",
    "        overlaps = bbox_overlaps(np.ascontiguousarray(anchors, dtype=np.float), np.ascontiguousarray(gt_boxes, dtype=np.float))\r\n",
    "        argmax_overlaps = overlaps.argmax(axis=1)\r\n",
    "        max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\r\n",
    "        gt_argmax_overlaps = overlaps.argmax(axis=0)\r\n",
    "        gt_max_overlaps = overlaps[gt_argmax_overlaps, np.arange(overlaps.shape[1])]\r\n",
    "        gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\r\n",
    "\r\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\r\n",
    "        # \"Positives clobber Negatives\"\r\n",
    "        if not CLOBBER_POSITIVES:\r\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\r\n",
    "\r\n",
    "        # Foreground label: for each Ground Truth box, anchor with highest overlap\r\n",
    "        labels[gt_argmax_overlaps] = 1\r\n",
    "\r\n",
    "        # Foreground label: above threshold IOU\r\n",
    "        labels[max_overlaps >= POSITIVE_OVERLAP] = 1\r\n",
    "\r\n",
    "        # Set anchors whose overlap < NEGATIVE_OVERLAP to a negative example\r\n",
    "        # \"Negatives clobber Positives\"\r\n",
    "        if CLOBBER_POSITIVES:\r\n",
    "            labels[max_overlaps < NEGATIVE_OVERLAP] = 0\r\n",
    "\r\n",
    "        # Subsample positive labels if we have too many\r\n",
    "        num_fg = int(FG_FRACTION * RPN_BATCH_SIZE)\r\n",
    "        fg_inds = np.where(labels == 1)[0]\r\n",
    "        if len(fg_inds) > num_fg:\r\n",
    "            disable_inds = npr.choice(fg_inds, size=(len(fg_inds) - num_fg), replace=False)\r\n",
    "            labels[disable_inds] = -1\r\n",
    "\r\n",
    "        # Subsample negative labels if we have too many\r\n",
    "        num_bg = RPN_BATCH_SIZE - np.sum(labels == 1)\r\n",
    "        bg_inds = np.where(labels == 0)[0]\r\n",
    "        if len(bg_inds) > num_bg:\r\n",
    "            disable_inds = npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)\r\n",
    "            labels[disable_inds] = -1\r\n",
    "\r\n",
    "        bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\r\n",
    "        labels = torch.from_numpy(labels)\r\n",
    "        \r\n",
    "        gt_rois_np = gt_boxes[argmax_overlaps, :].numpy()\r\n",
    "        bbox_targets = bbox_transform(anchors, torch.from_numpy(gt_rois_np[:, :4])).numpy()\r\n",
    "        \r\n",
    "        bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\r\n",
    "        # Only the positive ones have regression targets\r\n",
    "        bbox_inside_weights[labels == 1, :] = np.array((1.0, 1.0, 1.0, 1.0))\r\n",
    "\r\n",
    "        labels = labels.numpy()\r\n",
    "        bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\r\n",
    "        if RPN_POSITIVE_WEIGHT < 0:\r\n",
    "            # Uniform weighting of examples (given non-uniform sampling)\r\n",
    "            num_examples = np.sum(labels >= 0)\r\n",
    "            positive_weights = np.ones((1, 4)) * 1.0 / num_examples\r\n",
    "            negative_weights = np.ones((1, 4)) * 1.0 / num_examples\r\n",
    "        else:\r\n",
    "            assert ((RPN_POSITIVE_WEIGHT > 0) & (RPN_POSITIVE_WEIGHT < 1))\r\n",
    "            positive_weights = (RPN_POSITIVE_WEIGHT / np.sum(labels == 1))\r\n",
    "            negative_weights = ((1.0 - RPN_POSITIVE_WEIGHT) / np.sum(labels == 0))\r\n",
    "        bbox_outside_weights[labels == 1, :] = positive_weights\r\n",
    "        bbox_outside_weights[labels == 0, :] = negative_weights\r\n",
    "\r\n",
    "        # Map up to original set of anchors\r\n",
    "        total_anchors = all_anchors.shape[0]\r\n",
    "        labels = unmap(labels, total_anchors, inds_inside, fill=-1)\r\n",
    "\r\n",
    "        bbox_targets = unmap(bbox_targets, total_anchors, inds_inside, fill=0)\r\n",
    "        bbox_inside_weights = unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\r\n",
    "        bbox_outside_weights = unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\r\n",
    "\r\n",
    "        # Labels\r\n",
    "        labels = labels.reshape((1, height, width, self.num_of_anchors)).transpose(0, 3, 1, 2)\r\n",
    "        labels = labels.reshape((1, 1, self.num_of_anchors * height, width))\r\n",
    "        rpn_labels = labels\r\n",
    "        \r\n",
    "        # Bounding boxes\r\n",
    "        bbox_targets = bbox_targets.reshape((1, height, width, self.num_of_anchors * 4))\r\n",
    "        rpn_bbox_targets = bbox_targets\r\n",
    "        bbox_inside_weights = bbox_inside_weights.reshape((1, height, width, self.num_of_anchors * 4))\r\n",
    "        rpn_bbox_inside_weights = bbox_inside_weights\r\n",
    "        bbox_outside_weights = bbox_outside_weights.reshape((1, height, width, self.num_of_anchors * 4))\r\n",
    "        rpn_bbox_outside_weights = bbox_outside_weights\r\n",
    "\r\n",
    "        # Re-shape for future use\r\n",
    "        rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\r\n",
    "        rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\r\n",
    "        rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\r\n",
    "        rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\r\n",
    "        rpn_labels = rpn_labels.long()\r\n",
    "\r\n",
    "        # Data storing\r\n",
    "        self._anchor_targets['rpn_labels'] = rpn_labels\r\n",
    "        self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\r\n",
    "        self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\r\n",
    "        self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\r\n",
    "\r\n",
    "        return rpn_labels\r\n",
    "        \r\n",
    "    def proposal_target_layer(self, proposed_rois, proposed_roi_scores, gt_boxes):\r\n",
    "        '''\r\n",
    "        3.4. Proposal Target Layer\r\n",
    "        '''\r\n",
    "        \r\n",
    "        # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\r\n",
    "        gt_boxes = torch.stack(gt_boxes).squeeze()\r\n",
    "        num_images = 1\r\n",
    "        rois_per_image = RPN_BATCH_SIZE / num_images\r\n",
    "        fg_rois_per_image = int(round(FG_FRACTION * rois_per_image))\r\n",
    "\r\n",
    "        # Sample rois with classification labels and bounding box regression targets\r\n",
    "        # overlaps: (rois x gt_boxes)\r\n",
    "        overlaps = bbox_overlaps(proposed_rois[:, 1:5].data, gt_boxes[:, :4].data)\r\n",
    "        max_overlaps, gt_assignment = overlaps.max(1)\r\n",
    "        labels = gt_boxes[gt_assignment, [4]]\r\n",
    "\r\n",
    "        # Select foreground RoIs as those with >= FG_THRESH overlap\r\n",
    "        fg_inds = (max_overlaps >= FG_THRESH).nonzero().view(-1)\r\n",
    "        \r\n",
    "        # Guard against the case when an image has fewer than fg_rois_per_image\r\n",
    "        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\r\n",
    "        bg_inds = ((max_overlaps < BG_THRESH_HI) + (max_overlaps >= BG_THRESH_LO) == 2).nonzero().view(-1)\r\n",
    "\r\n",
    "        # Ensure a fixed number of regions are sampled (optional?)\r\n",
    "        fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image = fix_sample_regions(fg_inds, bg_inds, gt_boxes, proposed_rois, rois_per_image)\r\n",
    "        \r\n",
    "        # The indices that we're selecting (both fg and bg)\r\n",
    "        keep_inds = torch.cat([fg_inds, bg_inds], 0)\r\n",
    "\r\n",
    "        # Select sampled values from various arrays:\r\n",
    "        labels = labels[keep_inds].contiguous()\r\n",
    "\r\n",
    "        # Clamp labels for the background RoIs to 0\r\n",
    "        labels[int(fg_rois_per_image):] = 0\r\n",
    "        rois_final = proposed_rois[keep_inds].contiguous()\r\n",
    "        roi_scores_final = proposed_roi_scores[keep_inds].contiguous()\r\n",
    "        \r\n",
    "        # Compute bounding box target regression targets\r\n",
    "        ex_rois = rois_final[:, 1:5].data\r\n",
    "        gt_rois = gt_boxes[gt_assignment[keep_inds]][:, :4].data\r\n",
    "        targets = bbox_transform(ex_rois, gt_rois)\r\n",
    "        bbox_target_data = torch.cat([labels.data.unsqueeze(1), targets], 1)\r\n",
    "        bbox_targets, bbox_inside_weights = get_bbox_regression_labels(bbox_target_data, NUM_OF_CLASS)\r\n",
    "\r\n",
    "        # Reshape tensors\r\n",
    "        rois_final = rois_final.view(-1, 5)\r\n",
    "        roi_scores_final = roi_scores_final.view(-1)\r\n",
    "        labels = labels.view(-1, 1)\r\n",
    "        bbox_targets = bbox_targets.view(-1, NUM_OF_CLASS * 4)\r\n",
    "        bbox_inside_weights = bbox_inside_weights.view(-1, NUM_OF_CLASS * 4)\r\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\r\n",
    "        self._proposal_targets['rois'] = rois_final\r\n",
    "        self._proposal_targets['labels'] = labels.long()\r\n",
    "        self._proposal_targets['bbox_targets'] = bbox_targets\r\n",
    "        self._proposal_targets['bbox_inside_weights'] = bbox_inside_weights\r\n",
    "        self._proposal_targets['bbox_outside_weights'] = bbox_outside_weights\r\n",
    "\r\n",
    "        return rois_final, roi_scores_final\r\n",
    "\r\n",
    "    def region_proposal(self, net_conv, bb, anchors):\r\n",
    "        '''\r\n",
    "        3. Region Proposal Network (RPN)\r\n",
    "        '''\r\n",
    "                                       \r\n",
    "        rpn = F.relu(self.rpn_net(net_conv))\r\n",
    "        rpn_cls_score = self.rpn_cls_score_net(rpn)\r\n",
    "                                       \r\n",
    "        rpn_cls_score_reshape = rpn_cls_score.view(1, 2, -1, rpn_cls_score.size()[-1]) # batch * 2 * (num_anchors*h) * w\r\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, dim=1)\r\n",
    "\r\n",
    "        # Move channel to the last dimenstion, to fit the input of python functions\r\n",
    "        rpn_cls_prob = rpn_cls_prob_reshape.view_as(rpn_cls_score).permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\r\n",
    "        rpn_cls_score = rpn_cls_score.permute(0, 2, 3, 1) # batch * h * w * (num_anchors * 2)\r\n",
    "        rpn_cls_score_reshape = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous() # batch * (num_anchors*h) * w * 2\r\n",
    "        rpn_cls_pred = torch.max(rpn_cls_score_reshape.view(-1, 2), 1)[1]\r\n",
    "\r\n",
    "        rpn_bbox_pred = self.rpn_bbox_pred_net(rpn)\r\n",
    "        rpn_bbox_pred = rpn_bbox_pred.permute(0, 2, 3, 1).contiguous()  # batch * h * w * (num_anchors*4)                  \r\n",
    "\r\n",
    "        if self.mode == 'TRAIN':\r\n",
    "            rois, roi_scores = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, num_of_anchors=self.num_of_anchors)\r\n",
    "            rpn_labels = self.anchor_target_layer(rpn_cls_score, gt_boxes=bb, all_anchors=anchors)\r\n",
    "            rois, _ = self.proposal_target_layer(rois, roi_scores, gt_boxes=bb)\r\n",
    "        else:\r\n",
    "            rois, _ = self.proposal_layer(rpn_cls_prob, rpn_bbox_pred, anchors=anchors, num_of_anchors=self.num_of_anchors)\r\n",
    "        \r\n",
    "        self._predictions[\"rpn_cls_score\"] = rpn_cls_score\r\n",
    "        self._predictions[\"rpn_cls_score_reshape\"] = rpn_cls_score_reshape\r\n",
    "        self._predictions[\"rpn_cls_prob\"] = rpn_cls_prob\r\n",
    "        self._predictions[\"rpn_cls_pred\"] = rpn_cls_pred\r\n",
    "        self._predictions[\"rpn_bbox_pred\"] = rpn_bbox_pred\r\n",
    "        self._predictions[\"rois\"] = rois\r\n",
    "        \r\n",
    "        return rois\r\n",
    "\r\n",
    "    def roi_align_layer(self, bottom, rois):\r\n",
    "        '''\r\n",
    "        4. ROI Pooling Layer\r\n",
    "        '''\r\n",
    "        \r\n",
    "        return RoIAlign((POOLING_SIZE, POOLING_SIZE), 1.0 / 16.0, 0)(bottom, rois)\r\n",
    "    \r\n",
    "    def region_classification(self, fc7):\r\n",
    "        '''\r\n",
    "        5. Region Classification Network (RCNN)\r\n",
    "        '''\r\n",
    "        \r\n",
    "        self.cls_score_net.to(device)\r\n",
    "        self.bbox_pred_net.to(device)\r\n",
    "        cls_score = self.cls_score_net(fc7)\r\n",
    "        cls_score = cls_score.view(-1, NUM_OF_CLASS)\r\n",
    "        cls_pred = torch.max(cls_score, 1)[1]\r\n",
    "        cls_prob = F.softmax(cls_score, dim=1)\r\n",
    "        bbox_pred = self.bbox_pred_net(fc7)\r\n",
    "        \r\n",
    "        self._predictions[\"cls_score\"] = cls_score\r\n",
    "        self._predictions[\"cls_pred\"] = cls_pred\r\n",
    "        self._predictions[\"cls_prob\"] = cls_prob\r\n",
    "        self._predictions[\"bbox_pred\"] = bbox_pred\r\n",
    "\r\n",
    "        return cls_prob, bbox_pred\r\n",
    "    \r\n",
    "    def compute_rpn_loss(self):\r\n",
    "        '''\r\n",
    "        3.3. Compute RPN Loss\r\n",
    "        '''\r\n",
    "        \r\n",
    "        # RPN, class loss\r\n",
    "        rpn_cls_score = self._predictions['rpn_cls_score_reshape'].view(-1, 2).to(device)\r\n",
    "        rpn_label = self._anchor_targets['rpn_labels'].view(-1).to(device)\r\n",
    "        rpn_select = (rpn_label.data != -1).nonzero().view(-1)\r\n",
    "        rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\r\n",
    "        rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\r\n",
    "        rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\r\n",
    "\r\n",
    "        # RPN, bbox loss\r\n",
    "        rpn_bbox_pred = self._predictions['rpn_bbox_pred'].to(device)\r\n",
    "        rpn_bbox_targets = self._anchor_targets['rpn_bbox_targets'].to(device)\r\n",
    "        rpn_bbox_inside_weights = self._anchor_targets['rpn_bbox_inside_weights'].to(device)\r\n",
    "        rpn_bbox_outside_weights = self._anchor_targets['rpn_bbox_outside_weights'].to(device)\r\n",
    "        rpn_loss_box = smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights, sigma=3.0, dim=[1, 2, 3])\r\n",
    "\r\n",
    "        self._losses['rpn_cross_entropy'] = rpn_cross_entropy\r\n",
    "        self._losses['rpn_loss_box'] = rpn_loss_box\r\n",
    "        \r\n",
    "        return rpn_cross_entropy + rpn_loss_box\r\n",
    "\r\n",
    "    def compute_rcnn_loss(self):\r\n",
    "        '''\r\n",
    "        5.2. Compute RCNN Loss\r\n",
    "        '''\r\n",
    "        \r\n",
    "        # RCNN, class loss\r\n",
    "        cls_score = self._predictions[\"cls_score\"].to(device)\r\n",
    "        label = self._proposal_targets[\"labels\"].view(-1).to(device)\r\n",
    "        print(\"label: \", label)\r\n",
    "        cross_entropy = F.cross_entropy(cls_score.view(-1, NUM_OF_CLASS), label)\r\n",
    "\r\n",
    "        # RCNN, bbox loss\r\n",
    "        bbox_pred = self._predictions['bbox_pred'].to(device)\r\n",
    "        print(\"bbox_pred: \", bbox_pred.size())\r\n",
    "        bbox_pred = bbox_pred.view(RPN_BATCH_SIZE, -1)\r\n",
    "        bbox_targets = self._proposal_targets['bbox_targets'].to(device)\r\n",
    "        bbox_inside_weights = self._proposal_targets['bbox_inside_weights'].to(device)\r\n",
    "        bbox_outside_weights = self._proposal_targets['bbox_outside_weights'].to(device)\r\n",
    "        loss_box = smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\r\n",
    "\r\n",
    "        self._losses['cross_entropy'] = cross_entropy # highest loss\r\n",
    "        self._losses['loss_box'] = loss_box\r\n",
    "        \r\n",
    "        return cross_entropy + loss_box\r\n",
    "    \r\n",
    "    def compute_total_loss(self):\r\n",
    "        '''\r\n",
    "        6. Compute Total Loss\r\n",
    "        '''\r\n",
    "        \r\n",
    "        rpn_loss = self.compute_rpn_loss()\r\n",
    "        rcnn_loss = self.compute_rcnn_loss()\r\n",
    "        total_loss = rpn_loss + rcnn_loss\r\n",
    "        self._losses['total_loss'] = total_loss\r\n",
    "        return self._losses['cross_entropy'], self._losses['loss_box'], self._losses['rpn_cross_entropy'], self._losses['rpn_loss_box'], total_loss\r\n",
    "\r\n",
    "    def test_layer(self):\r\n",
    "        return nn.Sequential(\r\n",
    "            self.test_conv,\r\n",
    "            self.test_avgpool,\r\n",
    "        )\r\n",
    "\r\n",
    "    def fc7(self):\r\n",
    "        return nn.Sequential(\r\n",
    "            self.fc_layer1,\r\n",
    "            self.fc_layer2,\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, x, bb, im_info=[320, 320, 1], train_flag=True):\r\n",
    "        if train_flag:\r\n",
    "            self.mode = \"TRAIN\"\r\n",
    "        else:\r\n",
    "            self.mode = \"TEST\"\r\n",
    "        # Store image information\r\n",
    "        self._im_info = im_info\r\n",
    "                                       \r\n",
    "        # Pass the image through the Backbone ConvNet to generate the series of Feature maps\r\n",
    "        head_net = self.head_net_layer()\r\n",
    "        output = head_net(x)\r\n",
    "        anchors = self.anchor_generation_layer(output)\r\n",
    "        rois = self.region_proposal(output, bb, anchors) # [RPN_BATCH_SIZE, 5]\r\n",
    "        pool5 = self.roi_align_layer(output, rois) # [RPN_BATCH_SIZE, 1024, 7, 7]\r\n",
    "        pool5 = self.test_layer()(pool5)\r\n",
    "\r\n",
    "        pool5 = pool5.view(pool5.size()[0], -1)\r\n",
    "\r\n",
    "        fc7 = self.fc7()\r\n",
    "        # fc7.to(device)\r\n",
    "        fc7_out = fc7(pool5) # [RPN_BATCH_SIZE, RPN_BATCH_SIZE, 5, 5]\r\n",
    "        \r\n",
    "        # fc7_out = fc7_out.view(-1) # [RPN_BATCH_SIZE * 5 * 5]\r\n",
    "        cls_prob, bbox_pred = self.region_classification(fc7_out)\r\n",
    "\r\n",
    "        if self.mode == 'TEST':\r\n",
    "            # stds = bbox_pred.data.new((0.1, 0.1, 0.2, 0.2)).repeat(NUM_OF_CLASS).expand_as(bbox_pred)\r\n",
    "            # means = bbox_pred.data.new((0.0, 0.0, 0.0, 0.0)).repeat(NUM_OF_CLASS).expand_as(bbox_pred)\r\n",
    "            # self._predictions[\"bbox_pred\"] = bbox_pred.mul(stds).add(means)\r\n",
    "            return self._predictions[\"cls_score\"], self._predictions[\"cls_pred\"], self._predictions[\"cls_prob\"], self._predictions[\"bbox_pred\"], self._predictions[\"rois\"]\r\n",
    "        else:\r\n",
    "            loss = self.compute_total_loss()\r\n",
    "            return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "To train the model, we used an initial learning rate of 0.01 and divided it by 1.5 every 10 epochs. We trained for a total of 20 epochs with a batch size of 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "device = torch.device('cuda')\r\n",
    "# device = torch.device('cpu')\r\n",
    "\r\n",
    "net = faster_R_CNN()\r\n",
    "net.to(device)\r\n",
    "\r\n",
    "print(net)\r\n",
    "print(utils.display_num_param(net))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "faster_R_CNN(\n",
      "  (head_conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (head_batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head_relu1): ReLU()\n",
      "  (head_pool1): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_layer1): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_relu2): ReLU()\n",
      "  (head_layer2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool2): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_relu3): ReLU()\n",
      "  (head_layer3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (head_pool3): MaxPool2d(kernel_size=[3, 3], stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (head_relu4): ReLU()\n",
      "  (test_conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (test_avgpool): AvgPool2d(kernel_size=[3, 3], stride=1, padding=0)\n",
      "  (fc_layer1): Linear(in_features=6400, out_features=1000, bias=True)\n",
      "  (fc_layer2): Linear(in_features=1000, out_features=256, bias=True)\n",
      "  (rpn_net): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rpn_cls_score_net): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rpn_bbox_pred_net): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (cls_score_net): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (bbox_pred_net): Linear(in_features=256, out_features=32, bias=True)\n",
      ")\n",
      "There are 19824646 (19.82 million) parameters in this neural network\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Set initial learning rate and Optimizer\r\n",
    "lr = 0.01\r\n",
    "EPOCHS = 50\r\n",
    "\r\n",
    "# Set training variables\r\n",
    "running_loss = 0 \r\n",
    "num_batches = 0\r\n",
    "start = time.time()\r\n",
    "\r\n",
    "# Training process\r\n",
    "for epoch in range(EPOCHS):\r\n",
    "    # learning rate strategy : divide the learning rate by 1.5 every 10 epochs\r\n",
    "    if epoch%10==0 and epoch>=10: \r\n",
    "        lr = lr / 1.5\r\n",
    "    total_ce_loss = 0\r\n",
    "    total_box_loss = 0\r\n",
    "    total_r_ce_loss = 0\r\n",
    "    total_r_box_loss = 0\r\n",
    "    sum_total_loss = 0\r\n",
    "    single_batch = 0\r\n",
    "\r\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\r\n",
    "\r\n",
    "    for data in train_dataloader:\r\n",
    "        batch_images, batch_bboxes = data[0], data[1]\r\n",
    "        batch_images = batch_images.to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        ce_loss, box_loss, r_ce_loss, r_box_loss, loss = net(batch_images, batch_bboxes)\r\n",
    "        total_ce_loss += ce_loss.detach().item()\r\n",
    "        total_box_loss += box_loss.detach().item()\r\n",
    "        total_r_ce_loss += r_ce_loss.detach().item()\r\n",
    "        total_r_box_loss += r_box_loss.detach().item()\r\n",
    "        sum_total_loss += loss.detach().item()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        running_loss += loss.detach().item()\r\n",
    "        num_batches += 1\r\n",
    "        single_batch += 1\r\n",
    "\r\n",
    "    # AVERAGE STATS THEN DISPLAY\r\n",
    "    total_loss = running_loss/num_batches\r\n",
    "    elapsed = (time.time()-start)/60\r\n",
    "    \r\n",
    "    print(f\"Percentage loss: \\nRCNN: ce: {total_ce_loss/single_batch} \\t box: {total_box_loss/single_batch}\\nRPN: ce: {total_r_ce_loss/single_batch} \\t box: {total_r_box_loss/single_batch}\\n\")\r\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', lr  ,'\\t loss=', total_loss )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\chinhui\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "C:\\Users\\chinhui\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Percentage loss: \n",
      "RCNN: ce: 4913.594808556636 \t box: 5056.981219460878\n",
      "RPN: ce: 52.13199465796352 \t box: 187.13641227851622\n",
      "\n",
      "epoch= 0 \t time= 0.7495535055796305 min \t lr= 0.01 \t loss= 10209.844489192963\n",
      "Percentage loss: \n",
      "RCNN: ce: 29.9969308813413 \t box: 31.3935625167874\n",
      "RPN: ce: 0.46495279545585316 \t box: 0.14422840108551707\n",
      "\n",
      "epoch= 1 \t time= 1.5012277205785116 min \t lr= 0.01 \t loss= 5135.922081578275\n",
      "Percentage loss: \n",
      "RCNN: ce: 5.667083083589872 \t box: 4.248743847571314\n",
      "RPN: ce: 0.461823000262181 \t box: 0.1377281963514785\n",
      "\n",
      "epoch= 2 \t time= 2.2431270917256674 min \t lr= 0.01 \t loss= 3427.4531804611283\n",
      "Percentage loss: \n",
      "RCNN: ce: 4.307673404117425 \t box: 3.1457569405126073\n",
      "RPN: ce: 0.4682384941726923 \t box: 0.1355116273742169\n",
      "\n",
      "epoch= 3 \t time= 2.983328413963318 min \t lr= 0.01 \t loss= 2572.604180457691\n",
      "Percentage loss: \n",
      "RCNN: ce: 4.673337864875793 \t box: 3.556673611141741\n",
      "RPN: ce: 0.46130146719515325 \t box: 0.1335680584423244\n",
      "\n",
      "epoch= 4 \t time= 3.720759399731954 min \t lr= 0.01 \t loss= 2059.8483205787343\n",
      "Percentage loss: \n",
      "RCNN: ce: 3.4113639960686366 \t box: 3.2265846976389487\n",
      "RPN: ce: 0.45994599846502143 \t box: 0.1374832650180906\n",
      "\n",
      "epoch= 5 \t time= 4.456695413589477 min \t lr= 0.01 \t loss= 1717.746163474189\n",
      "Percentage loss: \n",
      "RCNN: ce: 3.4630582243204118 \t box: 3.1859195980553827\n",
      "RPN: ce: 0.4637293636798859 \t box: 0.13367536241809527\n",
      "\n",
      "epoch= 6 \t time= 5.194446531931559 min \t lr= 0.01 \t loss= 1473.3890519198917\n",
      "Percentage loss: \n",
      "RCNN: ce: 3.584076376756032 \t box: 2.6688480572775006\n",
      "RPN: ce: 0.46007419352730117 \t box: 0.13211098701382676\n",
      "\n",
      "epoch= 7 \t time= 5.934612373510997 min \t lr= 0.01 \t loss= 1290.0710591316224\n",
      "Percentage loss: \n",
      "RCNN: ce: 3.7776467035214107 \t box: 2.8107113212347032\n",
      "RPN: ce: 0.46321747414767744 \t box: 0.1349217512567217\n",
      "\n",
      "epoch= 8 \t time= 6.674914717674255 min \t lr= 0.01 \t loss= 1147.528330036225\n",
      "Percentage loss: \n",
      "RCNN: ce: 4.587637174129486 \t box: 5.954280148819089\n",
      "RPN: ce: 0.47321457229554653 \t box: 0.1331244393872718\n",
      "\n",
      "epoch= 9 \t time= 7.4473606189092 min \t lr= 0.01 \t loss= 1033.8903226566315\n",
      "Percentage loss: \n",
      "RCNN: ce: 2.2968264361222586 \t box: 1.8849111216142773\n",
      "RPN: ce: 0.47012779203553995 \t box: 0.13240707262884827\n",
      "\n",
      "epoch= 10 \t time= 8.204376594225566 min \t lr= 0.006666666666666667 \t loss= 940.3352271797079\n",
      "Percentage loss: \n",
      "RCNN: ce: 2.1511447980999945 \t box: 1.490107908534507\n",
      "RPN: ce: 0.45819649994373324 \t box: 0.1326732800419753\n",
      "\n",
      "epoch= 11 \t time= 8.97375565369924 min \t lr= 0.006666666666666667 \t loss= 862.3266351232098\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.8365069975455601 \t box: 0.6199718988500535\n",
      "RPN: ce: 0.4599336157242457 \t box: 0.1325115255235384\n",
      "\n",
      "epoch= 12 \t time= 9.72701700925827 min \t lr= 0.006666666666666667 \t loss= 796.2283496542619\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.8582895288864771 \t box: 0.5924788216594606\n",
      "RPN: ce: 0.4639516151199738 \t box: 0.13139349315315485\n",
      "\n",
      "epoch= 13 \t time= 10.484371141592662 min \t lr= 0.006666666666666667 \t loss= 739.57247564005\n",
      "Percentage loss: \n",
      "RCNN: ce: 2.1015023678541183 \t box: 0.872782816675802\n",
      "RPN: ce: 0.4611071130881707 \t box: 0.13390603695685666\n",
      "\n",
      "epoch= 14 \t time= 11.251891295115152 min \t lr= 0.006666666666666667 \t loss= 690.5055971500609\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.8377774159113567 \t box: 0.7064761778339743\n",
      "RPN: ce: 0.4706340190023184 \t box: 0.13029746694955974\n",
      "\n",
      "epoch= 15 \t time= 12.00576255718867 min \t lr= 0.006666666666666667 \t loss= 647.5455713953202\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.8876199831565221 \t box: 0.6813829725918671\n",
      "RPN: ce: 0.4675725191831589 \t box: 0.1300566967887183\n",
      "\n",
      "epoch= 16 \t time= 12.754967820644378 min \t lr= 0.006666666666666667 \t loss= 609.6409279113307\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.8214667131503424 \t box: 0.5558019396848977\n",
      "RPN: ce: 0.4665585204958916 \t box: 0.13249030336737633\n",
      "\n",
      "epoch= 17 \t time= 13.488900287946064 min \t lr= 0.006666666666666667 \t loss= 575.9373384428796\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.9395514180262883 \t box: 0.9765593054083487\n",
      "RPN: ce: 0.4549457300454378 \t box: 0.13281405625554424\n",
      "\n",
      "epoch= 18 \t time= 14.22644335826238 min \t lr= 0.006666666666666667 \t loss= 545.8092611831008\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.812155325214068 \t box: 0.756625952385366\n",
      "RPN: ce: 0.4548352291186651 \t box: 0.13006612579338253\n",
      "\n",
      "epoch= 19 \t time= 14.96737907330195 min \t lr= 0.006666666666666667 \t loss= 518.6764822547634\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6038194427887598 \t box: 0.2061773817675809\n",
      "RPN: ce: 0.47068747592469057 \t box: 0.1290578282593439\n",
      "\n",
      "epoch= 20 \t time= 15.714675728480021 min \t lr= 0.0044444444444444444 \t loss= 494.09235177286087\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6461736808220546 \t box: 0.21607632022351025\n",
      "RPN: ce: 0.46759920778373876 \t box: 0.13135179575377454\n",
      "\n",
      "epoch= 21 \t time= 16.478158847490946 min \t lr= 0.0044444444444444444 \t loss= 471.74548128391757\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6369369288285573 \t box: 0.24509308993195494\n",
      "RPN: ce: 0.4666552506387234 \t box: 0.12949424363517512\n",
      "\n",
      "epoch= 22 \t time= 17.23486401240031 min \t lr= 0.0044444444444444444 \t loss= 451.3425551196803\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6273317396640778 \t box: 0.18230817511988182\n",
      "RPN: ce: 0.4605439019699891 \t box: 0.12976819310958188\n",
      "\n",
      "epoch= 23 \t time= 17.998672076066335 min \t lr= 0.0044444444444444444 \t loss= 432.636613322753\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.649987550576528 \t box: 0.1784590594470501\n",
      "RPN: ce: 0.4584112020830313 \t box: 0.13139352222594122\n",
      "\n",
      "epoch= 24 \t time= 18.756396325429282 min \t lr= 0.0044444444444444444 \t loss= 415.42787884358563\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6425491482019425 \t box: 0.16865615765564143\n",
      "RPN: ce: 0.4716510115812222 \t box: 0.13040578808480252\n",
      "\n",
      "epoch= 25 \t time= 19.522719581921894 min \t lr= 0.0044444444444444444 \t loss= 399.54270127644907\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6355957070986429 \t box: 0.18520369639930626\n",
      "RPN: ce: 0.460178059960405 \t box: 0.1332951757280777\n",
      "\n",
      "epoch= 26 \t time= 20.288790289560954 min \t lr= 0.0044444444444444444 \t loss= 384.83424095677003\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6366559853156408 \t box: 0.17555281662692626\n",
      "RPN: ce: 0.4632325900097688 \t box: 0.13151489830731103\n",
      "\n",
      "epoch= 27 \t time= 21.046711305777233 min \t lr= 0.0044444444444444444 \t loss= 371.176123647037\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.656614464521408 \t box: 0.29411512586909033\n",
      "RPN: ce: 0.47052550117174785 \t box: 0.13040443180749814\n",
      "\n",
      "epoch= 28 \t time= 21.81060916185379 min \t lr= 0.0044444444444444444 \t loss= 358.4649352290507\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.6483072449763616 \t box: 0.1911215517980357\n",
      "RPN: ce: 0.4565752708663543 \t box: 0.13100115835356216\n",
      "\n",
      "epoch= 29 \t time= 22.578104503949483 min \t lr= 0.0044444444444444444 \t loss= 346.59700422919457\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5986826529105505 \t box: 0.10218421493967374\n",
      "RPN: ce: 0.45762083791196345 \t box: 0.13055177450490493\n",
      "\n",
      "epoch= 30 \t time= 23.341409456729888 min \t lr= 0.002962962962962963 \t loss= 335.49029568924055\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5909937610228857 \t box: 0.12206529990459482\n",
      "RPN: ce: 0.464880246296525 \t box: 0.13079006976137558\n",
      "\n",
      "epoch= 31 \t time= 24.10480898618698 min \t lr= 0.002962962962962963 \t loss= 325.0783717420573\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5927362104256948 \t box: 0.08334915093146264\n",
      "RPN: ce: 0.45905803168813386 \t box: 0.13098022782554228\n",
      "\n",
      "epoch= 32 \t time= 24.867683351039886 min \t lr= 0.002962962962962963 \t loss= 315.29618240489503\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.606955220301946 \t box: 0.09169908634697398\n",
      "RPN: ce: 0.4654500586291154 \t box: 0.13063847596446673\n",
      "\n",
      "epoch= 33 \t time= 25.6250861287117 min \t lr= 0.002962962962962963 \t loss= 306.09025771191307\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.589253090818723 \t box: 0.06884238303949436\n",
      "RPN: ce: 0.4607267217089733 \t box: 0.13115277336910366\n",
      "\n",
      "epoch= 34 \t time= 26.385980383555093 min \t lr= 0.002962962962962963 \t loss= 297.40910677665755\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5744816929101944 \t box: 0.059092942718416454\n",
      "RPN: ce: 0.4681337141742309 \t box: 0.1307717676196868\n",
      "\n",
      "epoch= 35 \t time= 27.087326912085214 min \t lr= 0.002962962962962963 \t loss= 289.2097560361856\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5819677193959554 \t box: 0.06804964262992144\n",
      "RPN: ce: 0.45927864002684754 \t box: 0.1307231954531744\n",
      "\n",
      "epoch= 36 \t time= 27.763957206408183 min \t lr= 0.002962962962962963 \t loss= 281.45381720243273\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5784890880187352 \t box: 0.06750137209892274\n",
      "RPN: ce: 0.4659875864783923 \t box: 0.1300721480200688\n",
      "\n",
      "epoch= 37 \t time= 28.442806661128998 min \t lr= 0.002962962962962963 \t loss= 274.1061391234921\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5725110173225403 \t box: 0.10483343172818423\n",
      "RPN: ce: 0.4585076253861189 \t box: 0.12927495684319487\n",
      "\n",
      "epoch= 38 \t time= 29.127059797445934 min \t lr= 0.002962962962962963 \t loss= 267.1358567620954\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.595355467001597 \t box: 0.10293071689084173\n",
      "RPN: ce: 0.45904949953158697 \t box: 0.1304247252875939\n",
      "\n",
      "epoch= 39 \t time= 29.806143828233083 min \t lr= 0.002962962962962963 \t loss= 260.5146543533107\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5780398666858673 \t box: 0.055595136666670444\n",
      "RPN: ce: 0.4703300273666779 \t box: 0.1305936372761304\n",
      "\n",
      "epoch= 40 \t time= 30.484158670902254 min \t lr= 0.0019753086419753087 \t loss= 254.21513982422468\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5689149846633275 \t box: 0.0589802920507888\n",
      "RPN: ce: 0.46729651677111783 \t box: 0.1293820145074278\n",
      "\n",
      "epoch= 41 \t time= 31.169741213321686 min \t lr= 0.0019753086419753087 \t loss= 248.21536444295492\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5646504978338878 \t box: 0.04937385384303828\n",
      "RPN: ce: 0.47030354626476767 \t box: 0.12950037536987413\n",
      "\n",
      "epoch= 42 \t time= 31.85043652455012 min \t lr= 0.0019753086419753087 \t loss= 242.49439848562082\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5813388814528784 \t box: 0.050259735978518925\n",
      "RPN: ce: 0.46490934615333873 \t box: 0.12761663137935103\n",
      "\n",
      "epoch= 43 \t time= 32.527491211891174 min \t lr= 0.0019753086419753087 \t loss= 237.0337104428221\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5649779677391051 \t box: 0.051455805699030556\n",
      "RPN: ce: 0.4610822156071663 \t box: 0.12957581051935751\n",
      "\n",
      "epoch= 44 \t time= 33.215697387854256 min \t lr= 0.0019753086419753087 \t loss= 231.81534113954615\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5818941424290338 \t box: 0.056786714633926746\n",
      "RPN: ce: 0.4746037303159634 \t box: 0.1299047663807869\n",
      "\n",
      "epoch= 45 \t time= 33.89543908437093 min \t lr= 0.0019753086419753087 \t loss= 226.82464218778887\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.571382255355517 \t box: 0.044209218421019617\n",
      "RPN: ce: 0.4696615577985843 \t box: 0.12984838883082073\n",
      "\n",
      "epoch= 46 \t time= 34.56940937439601 min \t lr= 0.0019753086419753087 \t loss= 222.04571578828157\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5632528255383173 \t box: 0.05093191199315091\n",
      "RPN: ce: 0.4621755318095287 \t box: 0.12770643981639296\n",
      "\n",
      "epoch= 47 \t time= 35.25352107286453 min \t lr= 0.0019753086419753087 \t loss= 217.46568143256007\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.563161666194598 \t box: 0.04969726192454497\n",
      "RPN: ce: 0.45952705182135106 \t box: 0.12727123328174153\n",
      "\n",
      "epoch= 48 \t time= 35.935195513566335 min \t lr= 0.0019753086419753087 \t loss= 213.07249726484827\n",
      "Percentage loss: \n",
      "RCNN: ce: 1.5760360062122345 \t box: 0.0562430899279813\n",
      "RPN: ce: 0.459343267728885 \t box: 0.1281189596125235\n",
      "\n",
      "epoch= 49 \t time= 36.61923001607259 min \t lr= 0.0019753086419753087 \t loss= 208.85544214592377\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "## Save model\r\n",
    "\r\n",
    "torch.save(net, \"model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "Our trained Faster RCNN model was evaluated based on the mean average precision (mAP) method taught in lecture. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def sample_classification(gt_boxes, pred_boxes, POSITIVE_OVERLAP):\r\n",
    "    '''\r\n",
    "    Calculates the sample classifications for a given set of predicted boxes.\r\n",
    "        \r\n",
    "    gt_boxes: n x 5 \r\n",
    "        n: number of ground truth boxes labelled on the image)\r\n",
    "    \r\n",
    "    pred_boxes: n_rois x 5\r\n",
    "        n_rois: number of rois filtered through NMS    \r\n",
    "\r\n",
    "    '''\r\n",
    "    \r\n",
    "    # Convert the boxes and predictions into Numpy arrays\r\n",
    "    gt_boxes = gt_boxes.cpu().numpy()\r\n",
    "    pred_boxes = pred_boxes.cpu().numpy()\r\n",
    "\r\n",
    "    tp = 0\r\n",
    "    fp = 0\r\n",
    "    tn = 0\r\n",
    "    fn = 0\r\n",
    "    \r\n",
    "    for pred_box in pred_boxes:\r\n",
    "        \r\n",
    "        # Obtain the class label for the current predicted box, between 0.0 ~ 7.0\r\n",
    "        roi_cls_pred = pred_box[4].astype(np.int32)\r\n",
    "        \r\n",
    "        for gt_box in gt_boxes:\r\n",
    "            \r\n",
    "            pred_box_no_label = torch.unsqueeze(torch.from_numpy(pred_box[:4]), 0)\r\n",
    "            gt_box_no_label = torch.unsqueeze(torch.from_numpy(gt_box[:4]), 0)\r\n",
    "            overlap = bbox_overlaps(pred_box_no_label, gt_box_no_label)\r\n",
    "            \r\n",
    "            # If the class label exists in the list of ground truth labels, it is a positive\r\n",
    "            if gt_box[4].astype(np.int32) == roi_cls_pred:                \r\n",
    "                \r\n",
    "                # If the overlap is greater than the positive overlap threshold, true positive\r\n",
    "                if overlap >= POSITIVE_OVERLAP:\r\n",
    "                    tp += 1\r\n",
    "                # If not, false positive\r\n",
    "                else:\r\n",
    "                    fp += 1\r\n",
    "            \r\n",
    "            # If the class label does not exist in the list of ground truth labels, it is a negative\r\n",
    "            else:\r\n",
    "                \r\n",
    "                # If the overlap is greater than the positive overlap threshold, false negative\r\n",
    "                if overlap >= POSITIVE_OVERLAP:\r\n",
    "                    fn += 1\r\n",
    "                # If not, true negative\r\n",
    "                else:\r\n",
    "                    tn += 1\r\n",
    "    \r\n",
    "    return tp, fp, tn, fn\r\n",
    "\r\n",
    "def precision_recall(tp, fp, tn, fn):\r\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\r\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\r\n",
    "    return precision, recall\r\n",
    "    \r\n",
    "def mean_average_precision(precisions, recalls):\r\n",
    "    '''\r\n",
    "    Calculates the mean Average Precision (mAP) score.\r\n",
    "    '''\r\n",
    "    \r\n",
    "    precisions = np.array(precisions)\r\n",
    "    recalls = np.array(recalls)\r\n",
    "    mAP = np.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\r\n",
    "    return mAP\r\n",
    "\r\n",
    "def filter_predictions(scores, pred_boxes, TEST_NMS_THRESH, PROB_THRESH):\r\n",
    "    \r\n",
    "    predictions = {index_label_dict[i]: [] for i in range(NUM_OF_CLASS)}\r\n",
    "    filtered_pred_boxes = torch.Tensor([])\r\n",
    "\r\n",
    "    for j in range(1, NUM_OF_CLASS):\r\n",
    "        \r\n",
    "        # Get the label for the current iteration\r\n",
    "        label = index_label_dict[j]\r\n",
    "        \r\n",
    "        # Get the co-ordinates corresponding to the labels only\r\n",
    "        inds = np.where(scores[:, j] > PROB_THRESH)[0]\r\n",
    "        cls_scores = scores[inds, j].cpu() # n x 1\r\n",
    "        cls_boxes = pred_boxes[inds, j * 4:(j + 1) * 4].cpu() # n x 4\r\n",
    "        cls_labels = torch.Tensor([j]).repeat(cls_boxes.size()[0], 1) # n x 1 (n-times repeated tensor of j)\r\n",
    "        cls_pred_boxes = torch.cat((cls_boxes, cls_labels), 1) # n x 5\r\n",
    "        cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32, copy=False)        \r\n",
    "\r\n",
    "        # Use NMS to filter predicted boxes through the NMS Threshold\r\n",
    "        keep = nms(cls_boxes, cls_scores, TEST_NMS_THRESH).numpy() if cls_dets.size > 0 else []\r\n",
    "\r\n",
    "        # Update predictions\r\n",
    "        cls_dets = cls_dets[keep, :]\r\n",
    "        predictions[label] = cls_dets\r\n",
    "        \r\n",
    "        # Update filtered predicted boxes\r\n",
    "        cls_pred_boxes = cls_pred_boxes[keep, :]\r\n",
    "        filtered_pred_boxes = torch.cat((filtered_pred_boxes, cls_pred_boxes), 0)\r\n",
    "        \r\n",
    "    return predictions, filtered_pred_boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def eval_on_test_set(test_loader):\r\n",
    "    \r\n",
    "    TEST_NMS_THRESH = 0.3\r\n",
    "    PROB_THRESH = 0.12\r\n",
    "    TEST_POSITIVE_OVERLAP = 0.2\r\n",
    "\r\n",
    "    running_error = 0\r\n",
    "    num_batches = 0\r\n",
    "    optimizer = torch.optim.Adam(net.parameters())\r\n",
    "    \r\n",
    "    # Classification values\r\n",
    "    precisions = []\r\n",
    "    recalls = []\r\n",
    "    \r\n",
    "    tp_total = 0\r\n",
    "    fp_total = 0\r\n",
    "    tn_total = 0\r\n",
    "    fn_total = 0\r\n",
    "\r\n",
    "    for data in test_loader:\r\n",
    "        with torch.no_grad():\r\n",
    "            batch_images, batch_bboxes = data[0], data[1]\r\n",
    "            batch_images = batch_images.to(device)\r\n",
    "            optimizer.zero_grad()\r\n",
    "            cls_score, cls_pred, cls_prob, bbox_pred, rois = net(batch_images, batch_bboxes, train_flag=False)\r\n",
    "\r\n",
    "            boxes = rois[:, 1:5]\r\n",
    "            batch_bboxes = batch_bboxes[0]\r\n",
    "            num_of_anchors = int(bbox_pred.shape[0] / (NUM_OF_CLASS * 4))\r\n",
    "            scores = cls_prob.cpu()\r\n",
    "            box_deltas = bbox_pred\r\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas)\r\n",
    "            pred_boxes = clip_boxes(pred_boxes, [320,320])\r\n",
    "                        \r\n",
    "            # Filter the Ground truth boxes and Predicted boxes\r\n",
    "            _, filtered_pred_boxes = filter_predictions(scores, pred_boxes, TEST_NMS_THRESH, PROB_THRESH)\r\n",
    "            \r\n",
    "            # Calculate the mAP Score\r\n",
    "            tp, fp, tn, fn = sample_classification(batch_bboxes, filtered_pred_boxes, TEST_POSITIVE_OVERLAP)\r\n",
    "            tp_total = tp_total + tp\r\n",
    "            fp_total = fp_total + fp\r\n",
    "            tn_total = tn_total + tn\r\n",
    "            fn_total = fn_total + fn\r\n",
    "            precision, recall = precision_recall(tp, fp, tn, fn)\r\n",
    "            precisions.append(precision)\r\n",
    "            recalls.append(recall)\r\n",
    "            \r\n",
    "            num_batches+=1\r\n",
    "            # print(\"TP:\", tp_total, \"\\nFP:\", fp_total, \"\\nTN:\", tn_total, \"\\nFP:\", fn_total, \"\\n\")\r\n",
    "            \r\n",
    "    total_error = running_error / num_batches\r\n",
    "    mAP = mean_average_precision(precisions, recalls)\r\n",
    "    print( 'test error  = ', total_error*100 ,'percent\\nMean Average Precision (MAP) Score = ', mAP)\r\n",
    "    \r\n",
    "device = torch.device('cuda')\r\n",
    "net = torch.load(\"model\")\r\n",
    "net.to(device)\r\n",
    "eval_on_test_set(test_dataloader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test error  =  0.0 percent\n",
      "Mean Average Precision (MAP) Score =  0.013068094871052824\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from PIL import Image\r\n",
    "\r\n",
    "def display_predictions(img, gt_bbox, max_per_image=5, limit_by_objs=True):\r\n",
    "    img = img.to(device)\r\n",
    "    with torch.no_grad():\r\n",
    "        cls_score, cls_pred, cls_prob, bbox_pred, rois = net(img, None, train_flag=False)\r\n",
    "        boxes = rois[:, 1:5]\r\n",
    "        scores = cls_prob\r\n",
    "        box_deltas = bbox_pred\r\n",
    "        n_objects = len(gt_bbox[0])\r\n",
    "        print(n_objects)\r\n",
    "        # print(\"boxes: \", boxes.size())\r\n",
    "        # print(\"box_deltas: \", box_deltas.size())\r\n",
    "        boxes = bbox_transform_inv(boxes, box_deltas)\r\n",
    "        boxes = clip_boxes(boxes, [320,320])\r\n",
    "        predictions, _ = filter_predictions(scores.cpu(), boxes.cpu(), 0.4, 0)\r\n",
    "        # print(\"predictions: \", predictions)\r\n",
    "        img = img.view(-1, 320, 320)\r\n",
    "        predictions = {label: bboxes[:1] for label, bboxes in predictions.items()}\r\n",
    "        predicted_boxes = []\r\n",
    "        for label, bboxes in predictions.items():\r\n",
    "            for bbox in bboxes:\r\n",
    "                bbox_list = bbox.tolist()\r\n",
    "                bbox_list.insert(4, label_index_dict[label])\r\n",
    "                predicted_boxes.append(bbox_list)\r\n",
    "        top_n_predicted_boxes = sorted(predicted_boxes, key=lambda x: x[4])[:n_objects]\r\n",
    "        print(\"top n: \", top_n_predicted_boxes)\r\n",
    "        display_img_bbox(img, gt_bbox[0], top_n_predicted_boxes)\r\n",
    "\r\n",
    "net = torch.load('model')\r\n",
    "device = torch.device('cuda')\r\n",
    "net.to(device)\r\n",
    "img, bbox = iter(test_dataloader).next()\r\n",
    "display_predictions(img, bbox)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15640\\2470328080.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mdisplay_img_bbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_bbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_n_predicted_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb3f132b8f46ef83ff81b7067b37036dbb49ec9e6fa8b80a12ce6c1c87b906f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}